{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Flags' from partially initialized module 'experiments.metric' (most likely due to a circular import) (c:\\KU\\ATNLP\\Scan_Experiments\\experiments\\metric.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\KU\\ATNLP\\Scan_Experiments\\experiments\\metric.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum, auto\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentType\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge_names\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlag\u001b[39;00m(Enum):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Metric Type\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KU\\ATNLP\\Scan_Experiments\\experiments\\experiment.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_type\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentType\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperiment_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExperimentConfig\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model_batchwise\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_result\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationResult\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_result_container\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationResultContainer\n",
      "File \u001b[1;32mc:\\KU\\ATNLP\\Scan_Experiments\\experiments\\evaluation.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flags, Flag\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_batch\u001b[39m(model, src_sequence, vocab, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Flags' from partially initialized module 'experiments.metric' (most likely due to a circular import) (c:\\KU\\ATNLP\\Scan_Experiments\\experiments\\metric.py)"
     ]
    }
   ],
   "source": [
    "from experiments.metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 'a'): 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{tuple([1,'a']):2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', 'A'],\n",
       "       ['1', 'A'],\n",
       "       ['2', 'A'],\n",
       "       ['0', 'B'],\n",
       "       ['1', 'B'],\n",
       "       ['2', 'B']], dtype='<U11')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "grid = np.meshgrid([0,1,2],[\"A\",\"B\"])\n",
    "\n",
    "grid = [np.ravel(x) for x in grid]\n",
    "grid = np.array(grid).T\n",
    "grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DistrFlags, _LevelFlags, _MetricFlags, _PredictionFlags]\n",
      "(5, 48)\n",
      "(4, 48)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([InLen, InLen, InLen, InLen, InLen, InLen, InLen, InLen, OutLen,\n",
       "       OutLen, OutLen, OutLen, OutLen, OutLen, OutLen, OutLen, Avrg, Avrg,\n",
       "       Avrg, Avrg, Avrg, Avrg, Avrg, Avrg, InLen, InLen, InLen, InLen,\n",
       "       InLen, InLen, InLen, InLen, OutLen, OutLen, OutLen, OutLen, OutLen,\n",
       "       OutLen, OutLen, OutLen, Avrg, Avrg, Avrg, Avrg, Avrg, Avrg, Avrg,\n",
       "       Avrg], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_groups = Flags.All\n",
    "print(f_groups)\n",
    "f_group_numb = len(f_groups)\n",
    "f_grouped = [f.All for f in f_groups]\n",
    "\n",
    "metrics_numb = np.prod([len(f) for f in f_grouped])\n",
    "lookup = np.empty((f_group_numb+1,metrics_numb), dtype=object)\n",
    "print(lookup.shape)\n",
    "\n",
    "grid = np.meshgrid(*f_grouped)\n",
    "grid = [np.ravel(x) for x in grid]\n",
    "grid = np.array(grid)\n",
    "print(grid.shape)\n",
    "grid = np.array(grid)\n",
    "indxs = np.arange(metrics_numb)\n",
    "grid = np.vstack([grid,indxs])\n",
    "grid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([_DistrFlags, _LevelFlags, _MetricFlags, _PredictionFlags])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flags.group_flags(Flags.All).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'A')\n",
      "(2, 'B')\n"
     ]
    }
   ],
   "source": [
    "d = {1:\"A\",2:\"B\"}\n",
    "for i in d.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{_DistrFlags: [Avrg], _LevelFlags: [SL], _MetricFlags: [CORRECT, TOTAL, ACC, ERR], _PredictionFlags: [NO_ORACLE]}\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False  True False  True False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([Avrg, SL, ERR, NO_ORACLE, 47], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup = grid\n",
    "from experiments.metric import MetricTemplate\n",
    "def find_index(cls, template: MetricTemplate):\n",
    "        f_grouped = Flags.group_flags(template.flags)\n",
    "        [f_grouped.update({k:k.All}) if len(v) == 0 else None for k,v in f_grouped.items()]\n",
    "        print(f_grouped)\n",
    "\n",
    "        indxs = np.full(lookup.shape[1], True)\n",
    "        for i,flags_in_group in enumerate(f_grouped.values()):\n",
    "            hits = np.isin(lookup[i],flags_in_group, assume_unique=True)\n",
    "            indxs = indxs & hits\n",
    "        print(indxs)\n",
    "        return lookup[-1][indxs]\n",
    "\n",
    "temp = MetricTemplate(None,\n",
    "                      [Flags.PredictionFlags.NO_ORACLE,\n",
    "                       Flags.DistrFlags.Avrg,\n",
    "                       Flags.LevelFlags.SL,\n",
    "\n",
    "                       ])\n",
    "indxs = find_index(None,temp)\n",
    "\n",
    "grid[:,indxs[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert dictionary update sequence element #0 to a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m l \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert dictionary update sequence element #0 to a sequence"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "  X\n",
      "    a : 1\n",
      "    b : 2\n",
      "  Y\n",
      "    a : 3\n",
      "    b : 4\n",
      "P\n",
      "  X\n",
      "    a : 1\n",
      "    b : 2\n",
      "  Y\n",
      "    a : 3\n",
      "    b : 4\n",
      "-----------------\n",
      "a\n",
      "  X\n",
      "    Q : 1\n",
      "    P : 1\n",
      "  Y\n",
      "    Q : 3\n",
      "    P : 3\n",
      "b\n",
      "  X\n",
      "    Q : 2\n",
      "    P : 2\n",
      "  Y\n",
      "    Q : 4\n",
      "    P : 4\n"
     ]
    }
   ],
   "source": [
    "A = {'a':1,'b':2}\n",
    "B = {'a':3,'b':4}\n",
    "C = {\"X\":A,\"Y\":B}\n",
    "D = {\"X\":A,\"Y\":B}\n",
    "E = {\"Q\":C,\"P\":D}\n",
    "\n",
    "\n",
    "# change to\n",
    "# A = {'Q':1,'P':2}\n",
    "# B = {'Q':3,'P':4}\n",
    "# C = {\"X\":A,\"Y\":B}\n",
    "# D = {\"X\":A,\"Y\":B}\n",
    "# E = {\"a\":C,\"b\":D}\n",
    "\n",
    "def flatten_dict(d, parent_key=(None), sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        #new_key = parent_key + sep + k if parent_key else k\n",
    "        if parent_key == None:\n",
    "            new_key = (k)\n",
    "        else:\n",
    "            new_key = tuple( list(parent_key) + [k] )\n",
    "\n",
    "        \n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "flatten_dict(E)\n",
    "\n",
    "def unflatten_dict(d):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        keys = list(k)\n",
    "        sub_items = items\n",
    "        for key in keys[:-1]:\n",
    "            sub_items = sub_items.setdefault(key, {})\n",
    "        sub_items[keys[-1]] = v\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "def switch_dict_lvls(d, new_indxs):\n",
    "    flat = flatten_dict(d)\n",
    "    new_flat = {}\n",
    "    for k,v in flat.items():\n",
    "        ks = list(k)\n",
    "        ks = [ks[i] for i in new_indxs]\n",
    "        ks = tuple(ks)\n",
    "        new_flat[ks] = v\n",
    "    return unflatten_dict(new_flat)\n",
    "\n",
    "\n",
    "def nice_dict_print(d, indent=0):\n",
    "    for k,v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            print(\" \"*indent + k)\n",
    "            nice_dict_print(v, indent+2)\n",
    "        else:\n",
    "            print(\" \"*indent + k + \" : \" + str(v))\n",
    "            \n",
    "# nice_dict_print(E)\n",
    "# print(\"-----------------\")\n",
    "# nice_dict_print(switch_dict_lvls(E,[2,1,0]))\n",
    "\n",
    "\n",
    "nice_dict_print(E)\n",
    "flatten_dict(E)\n",
    "print(\"-----------------\")\n",
    "nice_dict_print(switch_dict_lvls(E,[2,1,0]))\n",
    "\n",
    "\n",
    "# u = [1,2,3,4,5]\n",
    "# O = [1,2,5,4,3]\n",
    "\n",
    "# U = [O.index(x) for x in u]\n",
    "# U\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 48)\n"
     ]
    }
   ],
   "source": [
    "from experiments.evaluation_result import *\n",
    "from experiments.evaluation import _dummy_eval_result\n",
    "\n",
    "\n",
    "dummy = _dummy_eval_result()\n",
    "\n",
    "res = EvaluationResult(dummy)\n",
    "\n",
    "\n",
    "#res.get_data(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mixed MIX CORRECT  TL  Avrg  ORACLE  : 0,\n",
       " Mixed MIX CORRECT  TL  Avrg  NO_ORACLE  : 0,\n",
       " Mixed MIX CORRECT  SL  Avrg  ORACLE  : 0,\n",
       " Mixed MIX CORRECT  SL  Avrg  NO_ORACLE  : 0,\n",
       " Mixed MIX CORRECT  TL  InLen  ORACLE  : [0 0 0 0 0 0 0 0 0 0 0],\n",
       " Mixed MIX CORRECT  TL  InLen  NO_ORACLE  : [0 0 0 0 0 0 0 0 0 0 0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experiments.metric import *\n",
    "met = MetricTemplate(None,[Flag.CORRECT, Flag.TL])\n",
    "\n",
    "d = res.get_data(met)\n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
