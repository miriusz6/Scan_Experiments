{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models for all Experiments\n",
    "##### numbers of epoches are based on analysis of k-fold results\n",
    "##### all hyperparams are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "#from bart import SimpleBart\n",
    "from experiments.experiment_type import ExperimentType\n",
    "from dataset.scan_dataset import ScanDatasetHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BartModel, BartForConditionalGeneration\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id = 1, decoder_start_token_id = 2):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "class SimpleBart(nn.Module):\n",
    "    def __init__(self, out_vocab_size, input_length = 120):\n",
    "        super().__init__()\n",
    "        #self.bart = AutoAdapterModel.from_pretrained('facebook/bart-base')\n",
    "        #self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.bart = BartModel.from_pretrained('facebook/bart-base')\n",
    "        #self.up = nn.Linear(768, vocab_size)\n",
    "        #self.post = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "        self.up = nn.Linear(768, out_vocab_size)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def adaptable(self):\n",
    "        return self.bart\n",
    "\n",
    "    #def forward(self, in_ids, in_mask, tgt_ids, tgt_mask):\n",
    "    def forward(self, kwargs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the model.\n",
    "        NOT autoregressive\n",
    "        Args:\n",
    "            in_ids (torch.Tensor): Input IDs for the encoder.\n",
    "            in_mask (torch.Tensor): Attention mask for the encoder input.\n",
    "            tgt_ids (torch.Tensor): Input IDs for the decoder.\n",
    "            tgt_mask (torch.Tensor): Attention mask for the decoder input.\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model after passing through the encoder and decoder.\n",
    "        \"\"\"\n",
    "\n",
    "        #x = self.bart( **kwargs).logits\n",
    "\n",
    "        x = self.bart( **kwargs).last_hidden_state\n",
    "\n",
    "        x = self.up(x)\n",
    "        #x = self.post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR, BATCH_SIZE\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "batch_size = 32\n",
    "lr = 0.001#0.0001\n",
    "w_decay = 0.00001\n",
    "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA\n",
    "e_type = ExperimentType.E_1_1\n",
    "data_paths = e_type.get_data_paths()\n",
    "\n",
    "train_dataset = ScanDatasetHF(data_paths[\"train\"], tokenizer)\n",
    "test_dataset = ScanDatasetHF(data_paths[\"test\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "\n",
    "output_vocab = torch.concatenate([train_dataset.output_vocab,test_dataset.output_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.bias size: 384\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.bias size: 384\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "up.weight size: 33792\n",
      "up.bias size: 44\n",
      "prcnt of trainable: 0.008782271887640903\n",
      "All in millions: 140.644844\n",
      "Trainable in millions: 1.224428\n"
     ]
    }
   ],
   "source": [
    "#MODEL INITIALIZATION\n",
    "import adapters\n",
    "\n",
    "model = SimpleBart(out_vocab_size = output_vocab.size(0))\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "for name, param in model.bart.named_parameters():\n",
    "    if \"bart\"in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adapters.init(model.adaptable)\n",
    "\n",
    "from adapters import SeqBnInvConfig\n",
    "config = SeqBnInvConfig()\n",
    "model.adaptable.add_adapter(\"lang_adapter\", config=config)\n",
    "model.adaptable.set_active_adapters(\"lang_adapter\")\n",
    "model.adaptable.train_adapter(\"lang_adapter\")\n",
    "\n",
    "# from adapters import PrefixTuningConfig\n",
    "# config = PrefixTuningConfig(flat=False, prefix_length=30)\n",
    "# model.adaptable.add_adapter(\"prefix_tuning\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"prefix_tuning\")\n",
    "# model.adaptable.train_adapter(\"prefix_tuning\")\n",
    "\n",
    "\n",
    "\n",
    "# from adapters import IA3Config\n",
    "# config = IA3Config()\n",
    "# model.adaptable.add_adapter(\"ia3_adapter\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"ia3_adapter\")\n",
    "# model.adaptable.train_adapter(\"ia3_adapter\")\n",
    "\n",
    "#model.load_state_dict(torch.load(\"tbag/bart_1_1_5k.pth\"))\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "all_cnt = 0\n",
    "trainable_cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_cnt += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_cnt += param.numel()\n",
    "        print(name, 'size:', param.numel())\n",
    "\n",
    "#model.bart.lm_head.weight.requires_grad = True\n",
    "\n",
    "print('prcnt of trainable:', trainable_cnt/(all_cnt-trainable_cnt))\n",
    "print(\"All in millions:\", all_cnt/1000000)\n",
    "print(\"Trainable in millions:\", trainable_cnt/1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from math import e\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"name\": e_type.name,\n",
    "    \"tokenizer\" : tokenizer,\n",
    "    \"model\" : model,\n",
    "    \"evaluator\" : None,\n",
    "    \"optimizer\" : None,\n",
    "    \"grad_clip\" : 5.0,\n",
    "    \"lr\" : lr,\n",
    "    \"scheduler\" : None,\n",
    "    \"criterion\" : criterion,\n",
    "    \"train_dataset\" : train_dataset,\n",
    "    \"test_dataset\" : test_dataset,\n",
    "    \"train_loader\" : train_loader,\n",
    "    \"test_loader\" : test_loader,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"max_steps\" : 35 * 1000,#0,#//batch_size,\n",
    "    \"max_epochs\" : None,\n",
    "    \"evaluation_interval\" : 50,\n",
    "    \"model_save_interval\" : 50,\n",
    "    \"detailed_logging\" : True,\n",
    "    \"use_tensorboard\" : False,\n",
    "    \"tensorboard_dir\" : None,\n",
    "    \"model_save_dir\" : None,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "from os import name\n",
    "from turtle import mode\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from math import ceil\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def _train(config):\n",
    "    name = config[\"name\"]\n",
    "    tokenizer = config[\"tokenizer\"]\n",
    "    model = config[\"model\"]\n",
    "    evaluator = config[\"evaluator\"]\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    scheduler = config[\"scheduler\"]\n",
    "    grad_clip = config[\"grad_clip\"]\n",
    "    lr = config[\"lr\"]\n",
    "    criterion = config[\"criterion\"]\n",
    "    train_dataset = config[\"train_dataset\"]\n",
    "    test_dataset = config[\"test_dataset\"]\n",
    "    train_loader = config[\"train_loader\"]\n",
    "    test_loader = config[\"test_loader\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    max_steps = config[\"max_steps\"]\n",
    "    epochs = config[\"max_epochs\"]\n",
    "    evaluation_interval = config[\"evaluation_interval\"]\n",
    "    model_save_interval = config[\"model_save_interval\"]\n",
    "    detailed_logging = config[\"detailed_logging\"]\n",
    "    use_tensorboard = config[\"use_tensorboard\"]\n",
    "    tensorboard_dir = config[\"tensorboard_dir\"]\n",
    "    model_save_dir = config[\"model_save_dir\"]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=w_decay,\n",
    "        )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "    r_indx = name.find(\"_rep\")\n",
    "    org_name_base,org_rep_info = name[:r_indx], name[r_indx:]\n",
    "\n",
    "\n",
    "    name = org_name_base + org_rep_info\n",
    "\n",
    "    if use_tensorboard:\n",
    "        writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    # if model_weights_path is not None:\n",
    "    #     _load_weights()\n",
    "\n",
    "    grad_clip = grad_clip\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    max_epoch = epochs if epochs is not None else ceil(max_steps/len(train_dataset))\n",
    "    max_steps = max_steps if max_steps is not None else (max_epoch*len(train_loader)*batch_size)+1\n",
    "    max_batches = max_steps//batch_size\n",
    "    batch_num = 0\n",
    "    step_num = 0\n",
    "    batch_step = 0\n",
    "\n",
    "    eval_interval = evaluation_interval\n",
    "    model_sv_interval = model_save_interval\n",
    "\n",
    "    \n",
    "\n",
    "    if detailed_logging:\n",
    "        print(\"Training started for experiment: \", name)\n",
    "\n",
    "    \n",
    "    t ,_,_,_,_,_ = train_dataset[0]\n",
    "    \n",
    "    tgt_empty = torch.full((batch_size,t.shape[0]), tokenizer.pad_token_id, device='cuda')\n",
    "    tgt_empty_msks = torch.full(tgt_empty.shape, 0, device='cuda')\n",
    "    tgt_empty_msks[:,0] = 1\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    epoch_progress = tqdm(range(max_epoch), desc=\"EPOCH\")\n",
    "    for epoch in epoch_progress:\n",
    "        if max_steps is not None and step_num >= max_steps:\n",
    "                print(\"Early Stopping: Max steps reached\")\n",
    "                break\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        #batch_bar = tqdm(train_loader, desc=\"BATCH\")\n",
    "        #for batch in batch_bar:\n",
    "        batch_step = 0\n",
    "        for batch in train_loader:\n",
    "            if max_steps is not None and step_num >= max_steps:\n",
    "                break\n",
    "\n",
    "            inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to('cuda')\n",
    "            inputs_msks = inputs_msks.to('cuda')\n",
    "            dec_in = dec_in.to('cuda')\n",
    "            dec_in_msks = dec_in_msks.to('cuda')\n",
    "            dec_in_msks[:,0] = 1\n",
    "            targets = targets.to('cuda')\n",
    "            targets_msks = targets_msks.to('cuda')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            #out = model(inputs,inputs_msks,dec_in, dec_in_msks)\n",
    "\n",
    "            if True: #batch_num%2: #step_num < max_steps//2:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(dec_in), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : shift_tokens_right(dec_in_msks), #WRONG!!!!\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            else:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(tgt_empty), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : tgt_empty_msks, # None, #targets_msks,\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            out = model(m)\n",
    "\n",
    "\n",
    "            loss = criterion(out.permute(0, 2, 1), targets)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_num += 1\n",
    "            batch_step += 1\n",
    "            step_num += batch_size\n",
    "\n",
    "            if detailed_logging or batch_num == len(train_loader):\n",
    "                print(f\"Epoch {epoch+1}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num)} Step: {step_num}/{max_steps}\" )\n",
    "\n",
    "            \n",
    "\n",
    "        epoch += 1\n",
    "        if detailed_logging or epoch == max_epoch:\n",
    "            print(f\"Epoch {epoch}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num + 1)}\")\n",
    "\n",
    "        # EVALUATION \n",
    "        if epoch % eval_interval == 0 or epoch == max_epoch or max_steps is not None and step_num >= max_steps:\n",
    "            pass\n",
    "            # model.eval()\n",
    "            # with torch.no_grad():\n",
    "            #     total_loss = 0\n",
    "            #     for batch in test_loader:\n",
    "            #         inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            #         inputs = inputs.to('cuda')\n",
    "            #         inputs_msks = inputs_msks.to('cuda')\n",
    "            #         targets = targets.to('cuda')\n",
    "            #         targets_msks = targets_msks.to('cuda')\n",
    "            #         dec_in = dec_in.to('cuda')\n",
    "            #         dec_in_msks = dec_in_msks.to('cuda')\n",
    "\n",
    "            #         #out = model(inputs,inputs_msks,None, None)\n",
    "            #         m = {\n",
    "            #             \"input_ids\": inputs,\n",
    "            #             \"attention_mask\" : inputs_msks,\n",
    "            #             \"decoder_input_ids\" : None,\n",
    "            #             \"decoder_attention_mask\" : None,\n",
    "            #             \"head_mask\" : None,\n",
    "            #             \"decoder_head_mask\" : None,\n",
    "            #             \"cross_attn_head_mask\" : None,\n",
    "            #             \"encoder_outputs\" : None,\n",
    "            #             \"past_key_values\" : None,\n",
    "            #             \"inputs_embeds\" : None,\n",
    "            #             \"decoder_inputs_embeds\" : None,\n",
    "            #             \"use_cache\" : None,\n",
    "            #             \"output_attentions\" : None,\n",
    "            #             \"output_hidden_states\" : None,\n",
    "            #             \"return_dict\" : None,\n",
    "            #         }\n",
    "            #         out = model(m)\n",
    "            #         loss = criterion(out.permute(0, 2, 1), targets)\n",
    "            #         total_loss += loss.item()\n",
    "            #     print(f\"Epoch {epoch}/{max_epoch} Validation Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "        #     result = evaluate_model_batchwise(model,test_dataset, test_loader, test_dataset.vocab, device)\n",
    "        #     #result = EvaluationResult(result, name+fold_info+f\"_epoch_{epoch}\", e_type)\n",
    "        #     result = EvaluationResult(result, name+f\"_epoch_{epoch}\", e_type)\n",
    "        #     if detailed_logging or epoch == max_epoch:\n",
    "        #         result.print()\n",
    "        #     result_container.append_results(result)\n",
    "        \n",
    "        if epoch % model_sv_interval == 0 or epoch == max_epoch and not model_save_dir is None:\n",
    "            buff = name\n",
    "            name = org_name_base + f\"_epoch_{epoch}\" + org_rep_info\n",
    "            torch.save(model.state_dict(),model_save_dir+'/'+name)\n",
    "            name = buff\n",
    "\n",
    "\n",
    "        if use_tensorboard:\n",
    "            writer.add_scalar(tag = 'TrainLoss',\n",
    "                                scalar_value = total_loss,\n",
    "                                global_step = epoch)\n",
    "        \n",
    "    if use_tensorboard:\n",
    "        writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for experiment:  E_1_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067781a6598644e3898be19e5e1b7c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCH:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Batch 1/1093 Trining Loss: 15.234495162963867 Step: 32/35000\n",
      "Epoch 1/3 Batch 2/1093 Trining Loss: 11.94149923324585 Step: 64/35000\n",
      "Epoch 1/3 Batch 3/1093 Trining Loss: 9.960360685984293 Step: 96/35000\n",
      "Epoch 1/3 Batch 4/1093 Trining Loss: 8.456329107284546 Step: 128/35000\n",
      "Epoch 1/3 Batch 5/1093 Trining Loss: 7.4622985363006595 Step: 160/35000\n",
      "Epoch 1/3 Batch 6/1093 Trining Loss: 6.720834930737813 Step: 192/35000\n",
      "Epoch 1/3 Batch 7/1093 Trining Loss: 6.179171221596854 Step: 224/35000\n",
      "Epoch 1/3 Batch 8/1093 Trining Loss: 5.72399377822876 Step: 256/35000\n",
      "Epoch 1/3 Batch 9/1093 Trining Loss: 5.3582380877600775 Step: 288/35000\n",
      "Epoch 1/3 Batch 10/1093 Trining Loss: 5.037980270385742 Step: 320/35000\n",
      "Epoch 1/3 Batch 11/1093 Trining Loss: 4.76899918642911 Step: 352/35000\n",
      "Epoch 1/3 Batch 12/1093 Trining Loss: 4.51357834537824 Step: 384/35000\n",
      "Epoch 1/3 Batch 13/1093 Trining Loss: 4.275640790279095 Step: 416/35000\n",
      "Epoch 1/3 Batch 14/1093 Trining Loss: 4.067623198032379 Step: 448/35000\n",
      "Epoch 1/3 Batch 15/1093 Trining Loss: 3.8952664454778034 Step: 480/35000\n",
      "Epoch 1/3 Batch 16/1093 Trining Loss: 3.7231025472283363 Step: 512/35000\n",
      "Epoch 1/3 Batch 17/1093 Trining Loss: 3.562014004763435 Step: 544/35000\n",
      "Epoch 1/3 Batch 18/1093 Trining Loss: 3.4128167165650263 Step: 576/35000\n",
      "Epoch 1/3 Batch 19/1093 Trining Loss: 3.2788397230600057 Step: 608/35000\n",
      "Epoch 1/3 Batch 20/1093 Trining Loss: 3.1517775148153304 Step: 640/35000\n",
      "Epoch 1/3 Batch 21/1093 Trining Loss: 3.037158815633683 Step: 672/35000\n",
      "Epoch 1/3 Batch 22/1093 Trining Loss: 2.928566336631775 Step: 704/35000\n",
      "Epoch 1/3 Batch 23/1093 Trining Loss: 2.829316014828889 Step: 736/35000\n",
      "Epoch 1/3 Batch 24/1093 Trining Loss: 2.735260494053364 Step: 768/35000\n",
      "Epoch 1/3 Batch 25/1093 Trining Loss: 2.6449483501911164 Step: 800/35000\n",
      "Epoch 1/3 Batch 26/1093 Trining Loss: 2.5624057937126894 Step: 832/35000\n",
      "Epoch 1/3 Batch 27/1093 Trining Loss: 2.485828342261138 Step: 864/35000\n",
      "Epoch 1/3 Batch 28/1093 Trining Loss: 2.413673752120563 Step: 896/35000\n",
      "Epoch 1/3 Batch 29/1093 Trining Loss: 2.3465588627190423 Step: 928/35000\n",
      "Epoch 1/3 Batch 30/1093 Trining Loss: 2.282221714655558 Step: 960/35000\n",
      "Epoch 1/3 Batch 31/1093 Trining Loss: 2.2224312616932775 Step: 992/35000\n",
      "Epoch 1/3 Batch 32/1093 Trining Loss: 2.165944521315396 Step: 1024/35000\n",
      "Epoch 1/3 Batch 33/1093 Trining Loss: 2.110698570807775 Step: 1056/35000\n",
      "Epoch 1/3 Batch 34/1093 Trining Loss: 2.061487342504894 Step: 1088/35000\n",
      "Epoch 1/3 Batch 35/1093 Trining Loss: 2.0128877554621014 Step: 1120/35000\n",
      "Epoch 1/3 Batch 36/1093 Trining Loss: 1.9681027051475313 Step: 1152/35000\n",
      "Epoch 1/3 Batch 37/1093 Trining Loss: 1.9242529603275094 Step: 1184/35000\n",
      "Epoch 1/3 Batch 38/1093 Trining Loss: 1.882253386472401 Step: 1216/35000\n",
      "Epoch 1/3 Batch 39/1093 Trining Loss: 1.8418568219894018 Step: 1248/35000\n",
      "Epoch 1/3 Batch 40/1093 Trining Loss: 1.8044679388403893 Step: 1280/35000\n",
      "Epoch 1/3 Batch 41/1093 Trining Loss: 1.7692584686162995 Step: 1312/35000\n",
      "Epoch 1/3 Batch 42/1093 Trining Loss: 1.7360830817903792 Step: 1344/35000\n",
      "Epoch 1/3 Batch 43/1093 Trining Loss: 1.702977175629416 Step: 1376/35000\n",
      "Epoch 1/3 Batch 44/1093 Trining Loss: 1.6716786853291772 Step: 1408/35000\n",
      "Epoch 1/3 Batch 45/1093 Trining Loss: 1.641779836681154 Step: 1440/35000\n",
      "Epoch 1/3 Batch 46/1093 Trining Loss: 1.6134935850682466 Step: 1472/35000\n",
      "Epoch 1/3 Batch 47/1093 Trining Loss: 1.5862407081938805 Step: 1504/35000\n",
      "Epoch 1/3 Batch 48/1093 Trining Loss: 1.5593746826052666 Step: 1536/35000\n",
      "Epoch 1/3 Batch 49/1093 Trining Loss: 1.5341732660118415 Step: 1568/35000\n",
      "Epoch 1/3 Batch 50/1093 Trining Loss: 1.5093531841039658 Step: 1600/35000\n",
      "Epoch 1/3 Batch 51/1093 Trining Loss: 1.4859075698198057 Step: 1632/35000\n",
      "Epoch 1/3 Batch 52/1093 Trining Loss: 1.4632003639753048 Step: 1664/35000\n",
      "Epoch 1/3 Batch 53/1093 Trining Loss: 1.44084625199156 Step: 1696/35000\n",
      "Epoch 1/3 Batch 54/1093 Trining Loss: 1.4197646881695147 Step: 1728/35000\n",
      "Epoch 1/3 Batch 55/1093 Trining Loss: 1.398960904641585 Step: 1760/35000\n",
      "Epoch 1/3 Batch 56/1093 Trining Loss: 1.3793980681470461 Step: 1792/35000\n",
      "Epoch 1/3 Batch 57/1093 Trining Loss: 1.3603853395110683 Step: 1824/35000\n",
      "Epoch 1/3 Batch 58/1093 Trining Loss: 1.341551558724765 Step: 1856/35000\n",
      "Epoch 1/3 Batch 59/1093 Trining Loss: 1.3236781232437844 Step: 1888/35000\n",
      "Epoch 1/3 Batch 60/1093 Trining Loss: 1.305889612933 Step: 1920/35000\n",
      "Epoch 1/3 Batch 61/1093 Trining Loss: 1.2889657592187163 Step: 1952/35000\n",
      "Epoch 1/3 Batch 62/1093 Trining Loss: 1.2726165464808863 Step: 1984/35000\n",
      "Epoch 1/3 Batch 63/1093 Trining Loss: 1.2563070472743776 Step: 2016/35000\n",
      "Epoch 1/3 Batch 64/1093 Trining Loss: 1.2405479589942843 Step: 2048/35000\n",
      "Epoch 1/3 Batch 65/1093 Trining Loss: 1.2254259533607044 Step: 2080/35000\n",
      "Epoch 1/3 Batch 66/1093 Trining Loss: 1.2113068295699176 Step: 2112/35000\n",
      "Epoch 1/3 Batch 67/1093 Trining Loss: 1.196884546929331 Step: 2144/35000\n",
      "Epoch 1/3 Batch 68/1093 Trining Loss: 1.183116892023998 Step: 2176/35000\n",
      "Epoch 1/3 Batch 69/1093 Trining Loss: 1.1697376748357995 Step: 2208/35000\n",
      "Epoch 1/3 Batch 70/1093 Trining Loss: 1.1565905117562838 Step: 2240/35000\n",
      "Epoch 1/3 Batch 71/1093 Trining Loss: 1.1441440231783289 Step: 2272/35000\n",
      "Epoch 1/3 Batch 72/1093 Trining Loss: 1.1317654009908438 Step: 2304/35000\n",
      "Epoch 1/3 Batch 73/1093 Trining Loss: 1.1198123842477798 Step: 2336/35000\n",
      "Epoch 1/3 Batch 74/1093 Trining Loss: 1.1076940869962848 Step: 2368/35000\n",
      "Epoch 1/3 Batch 75/1093 Trining Loss: 1.0959881180524826 Step: 2400/35000\n",
      "Epoch 1/3 Batch 76/1093 Trining Loss: 1.0848321828402971 Step: 2432/35000\n",
      "Epoch 1/3 Batch 77/1093 Trining Loss: 1.0736041856663567 Step: 2464/35000\n",
      "Epoch 1/3 Batch 78/1093 Trining Loss: 1.0626655423488371 Step: 2496/35000\n",
      "Epoch 1/3 Batch 79/1093 Trining Loss: 1.0522429852546016 Step: 2528/35000\n",
      "Epoch 1/3 Batch 80/1093 Trining Loss: 1.042218980193138 Step: 2560/35000\n",
      "Epoch 1/3 Batch 81/1093 Trining Loss: 1.032356060213513 Step: 2592/35000\n",
      "Epoch 1/3 Batch 82/1093 Trining Loss: 1.0229895223931569 Step: 2624/35000\n",
      "Epoch 1/3 Batch 83/1093 Trining Loss: 1.0131010768643345 Step: 2656/35000\n",
      "Epoch 1/3 Batch 84/1093 Trining Loss: 1.0039549113384314 Step: 2688/35000\n",
      "Epoch 1/3 Batch 85/1093 Trining Loss: 0.9949678250971963 Step: 2720/35000\n",
      "Epoch 1/3 Batch 86/1093 Trining Loss: 0.9860410556890243 Step: 2752/35000\n",
      "Epoch 1/3 Batch 87/1093 Trining Loss: 0.9775853658887161 Step: 2784/35000\n",
      "Epoch 1/3 Batch 88/1093 Trining Loss: 0.9688752324066379 Step: 2816/35000\n",
      "Epoch 1/3 Batch 89/1093 Trining Loss: 0.9605251055085258 Step: 2848/35000\n",
      "Epoch 1/3 Batch 90/1093 Trining Loss: 0.9524238131112522 Step: 2880/35000\n",
      "Epoch 1/3 Batch 91/1093 Trining Loss: 0.9444411826002729 Step: 2912/35000\n",
      "Epoch 1/3 Batch 92/1093 Trining Loss: 0.9365978699339472 Step: 2944/35000\n",
      "Epoch 1/3 Batch 93/1093 Trining Loss: 0.9286656363676953 Step: 2976/35000\n",
      "Epoch 1/3 Batch 94/1093 Trining Loss: 0.9214163363931027 Step: 3008/35000\n",
      "Epoch 1/3 Batch 95/1093 Trining Loss: 0.9141468771194157 Step: 3040/35000\n",
      "Epoch 1/3 Batch 96/1093 Trining Loss: 0.9069034107960761 Step: 3072/35000\n",
      "Epoch 1/3 Batch 97/1093 Trining Loss: 0.8998208382387751 Step: 3104/35000\n",
      "Epoch 1/3 Batch 98/1093 Trining Loss: 0.8925911703584145 Step: 3136/35000\n",
      "Epoch 1/3 Batch 99/1093 Trining Loss: 0.8857451824828831 Step: 3168/35000\n",
      "Epoch 1/3 Batch 100/1093 Trining Loss: 0.8788234315812588 Step: 3200/35000\n",
      "Epoch 1/3 Batch 101/1093 Trining Loss: 0.8721887031109026 Step: 3232/35000\n",
      "Epoch 1/3 Batch 102/1093 Trining Loss: 0.8657165251526178 Step: 3264/35000\n",
      "Epoch 1/3 Batch 103/1093 Trining Loss: 0.8594137897479881 Step: 3296/35000\n",
      "Epoch 1/3 Batch 104/1093 Trining Loss: 0.8529827349747603 Step: 3328/35000\n",
      "Epoch 1/3 Batch 105/1093 Trining Loss: 0.846991491317749 Step: 3360/35000\n",
      "Epoch 1/3 Batch 106/1093 Trining Loss: 0.840934729379303 Step: 3392/35000\n",
      "Epoch 1/3 Batch 107/1093 Trining Loss: 0.8350483361527185 Step: 3424/35000\n",
      "Epoch 1/3 Batch 108/1093 Trining Loss: 0.8292845204748489 Step: 3456/35000\n",
      "Epoch 1/3 Batch 109/1093 Trining Loss: 0.8236144059294954 Step: 3488/35000\n",
      "Epoch 1/3 Batch 110/1093 Trining Loss: 0.817948830127716 Step: 3520/35000\n",
      "Epoch 1/3 Batch 111/1093 Trining Loss: 0.8120593062123737 Step: 3552/35000\n",
      "Epoch 1/3 Batch 112/1093 Trining Loss: 0.8067856686455863 Step: 3584/35000\n",
      "Epoch 1/3 Batch 113/1093 Trining Loss: 0.8011988484490234 Step: 3616/35000\n",
      "Epoch 1/3 Batch 114/1093 Trining Loss: 0.7958993269947537 Step: 3648/35000\n",
      "Epoch 1/3 Batch 115/1093 Trining Loss: 0.7907648225193439 Step: 3680/35000\n",
      "Epoch 1/3 Batch 116/1093 Trining Loss: 0.7856158154277966 Step: 3712/35000\n",
      "Epoch 1/3 Batch 117/1093 Trining Loss: 0.7806311023031545 Step: 3744/35000\n",
      "Epoch 1/3 Batch 118/1093 Trining Loss: 0.7756908208889476 Step: 3776/35000\n",
      "Epoch 1/3 Batch 119/1093 Trining Loss: 0.7709738828805315 Step: 3808/35000\n",
      "Epoch 1/3 Batch 120/1093 Trining Loss: 0.766048493112127 Step: 3840/35000\n",
      "Epoch 1/3 Batch 121/1093 Trining Loss: 0.7613107091631771 Step: 3872/35000\n",
      "Epoch 1/3 Batch 122/1093 Trining Loss: 0.7565346628916069 Step: 3904/35000\n",
      "Epoch 1/3 Batch 123/1093 Trining Loss: 0.7518982390562693 Step: 3936/35000\n",
      "Epoch 1/3 Batch 124/1093 Trining Loss: 0.7473547905683517 Step: 3968/35000\n",
      "Epoch 1/3 Batch 125/1093 Trining Loss: 0.7428187594413758 Step: 4000/35000\n",
      "Epoch 1/3 Batch 126/1093 Trining Loss: 0.7383599685771125 Step: 4032/35000\n",
      "Epoch 1/3 Batch 127/1093 Trining Loss: 0.7337718608341818 Step: 4064/35000\n",
      "Epoch 1/3 Batch 128/1093 Trining Loss: 0.7294114276301116 Step: 4096/35000\n",
      "Epoch 1/3 Batch 129/1093 Trining Loss: 0.7253380308086558 Step: 4128/35000\n",
      "Epoch 1/3 Batch 130/1093 Trining Loss: 0.7213025665053955 Step: 4160/35000\n",
      "Epoch 1/3 Batch 131/1093 Trining Loss: 0.7170738129670383 Step: 4192/35000\n",
      "Epoch 1/3 Batch 132/1093 Trining Loss: 0.712948461938085 Step: 4224/35000\n",
      "Epoch 1/3 Batch 133/1093 Trining Loss: 0.7090718779796944 Step: 4256/35000\n",
      "Epoch 1/3 Batch 134/1093 Trining Loss: 0.705196375277505 Step: 4288/35000\n",
      "Epoch 1/3 Batch 135/1093 Trining Loss: 0.701276942314925 Step: 4320/35000\n",
      "Epoch 1/3 Batch 136/1093 Trining Loss: 0.6973720302476603 Step: 4352/35000\n",
      "Epoch 1/3 Batch 137/1093 Trining Loss: 0.6935705451199609 Step: 4384/35000\n",
      "Epoch 1/3 Batch 138/1093 Trining Loss: 0.6897502347179081 Step: 4416/35000\n",
      "Epoch 1/3 Batch 139/1093 Trining Loss: 0.6860021956318574 Step: 4448/35000\n",
      "Epoch 1/3 Batch 140/1093 Trining Loss: 0.6822533094457217 Step: 4480/35000\n",
      "Epoch 1/3 Batch 141/1093 Trining Loss: 0.6786954670299029 Step: 4512/35000\n",
      "Epoch 1/3 Batch 142/1093 Trining Loss: 0.6751226602305829 Step: 4544/35000\n",
      "Epoch 1/3 Batch 143/1093 Trining Loss: 0.6714576849570641 Step: 4576/35000\n",
      "Epoch 1/3 Batch 144/1093 Trining Loss: 0.6680553800736865 Step: 4608/35000\n",
      "Epoch 1/3 Batch 145/1093 Trining Loss: 0.664678089372043 Step: 4640/35000\n",
      "Epoch 1/3 Batch 146/1093 Trining Loss: 0.661513842336119 Step: 4672/35000\n",
      "Epoch 1/3 Batch 147/1093 Trining Loss: 0.6581797658586178 Step: 4704/35000\n",
      "Epoch 1/3 Batch 148/1093 Trining Loss: 0.6550885576855492 Step: 4736/35000\n",
      "Epoch 1/3 Batch 149/1093 Trining Loss: 0.6517016065600735 Step: 4768/35000\n",
      "Epoch 1/3 Batch 150/1093 Trining Loss: 0.6485020008683204 Step: 4800/35000\n",
      "Epoch 1/3 Batch 151/1093 Trining Loss: 0.6455945835405628 Step: 4832/35000\n",
      "Epoch 1/3 Batch 152/1093 Trining Loss: 0.6424597085110451 Step: 4864/35000\n",
      "Epoch 1/3 Batch 153/1093 Trining Loss: 0.6393857107442968 Step: 4896/35000\n",
      "Epoch 1/3 Batch 154/1093 Trining Loss: 0.6365764703262936 Step: 4928/35000\n",
      "Epoch 1/3 Batch 155/1093 Trining Loss: 0.6336490360959883 Step: 4960/35000\n",
      "Epoch 1/3 Batch 156/1093 Trining Loss: 0.6307773302571896 Step: 4992/35000\n",
      "Epoch 1/3 Batch 157/1093 Trining Loss: 0.6280015404247175 Step: 5024/35000\n",
      "Epoch 1/3 Batch 158/1093 Trining Loss: 0.6252326576770106 Step: 5056/35000\n",
      "Epoch 1/3 Batch 159/1093 Trining Loss: 0.6224183956594587 Step: 5088/35000\n",
      "Epoch 1/3 Batch 160/1093 Trining Loss: 0.6197898588143289 Step: 5120/35000\n",
      "Epoch 1/3 Batch 161/1093 Trining Loss: 0.6170539012791948 Step: 5152/35000\n",
      "Epoch 1/3 Batch 162/1093 Trining Loss: 0.6142730277813511 Step: 5184/35000\n",
      "Epoch 1/3 Batch 163/1093 Trining Loss: 0.611638983151664 Step: 5216/35000\n",
      "Epoch 1/3 Batch 164/1093 Trining Loss: 0.6087987812735686 Step: 5248/35000\n",
      "Epoch 1/3 Batch 165/1093 Trining Loss: 0.6061924790794199 Step: 5280/35000\n",
      "Epoch 1/3 Batch 166/1093 Trining Loss: 0.6034471855824253 Step: 5312/35000\n",
      "Epoch 1/3 Batch 167/1093 Trining Loss: 0.6008535463831381 Step: 5344/35000\n",
      "Epoch 1/3 Batch 168/1093 Trining Loss: 0.5983058818216834 Step: 5376/35000\n",
      "Epoch 1/3 Batch 169/1093 Trining Loss: 0.5957838400933869 Step: 5408/35000\n",
      "Epoch 1/3 Batch 170/1093 Trining Loss: 0.5931091532111168 Step: 5440/35000\n",
      "Epoch 1/3 Batch 171/1093 Trining Loss: 0.5906564833825094 Step: 5472/35000\n",
      "Epoch 1/3 Batch 172/1093 Trining Loss: 0.5881071983035221 Step: 5504/35000\n",
      "Epoch 1/3 Batch 173/1093 Trining Loss: 0.585672516784916 Step: 5536/35000\n",
      "Epoch 1/3 Batch 174/1093 Trining Loss: 0.5832359296665794 Step: 5568/35000\n",
      "Epoch 1/3 Batch 175/1093 Trining Loss: 0.5808961497034345 Step: 5600/35000\n",
      "Epoch 1/3 Batch 176/1093 Trining Loss: 0.5785892796279355 Step: 5632/35000\n",
      "Epoch 1/3 Batch 177/1093 Trining Loss: 0.5763327562707966 Step: 5664/35000\n",
      "Epoch 1/3 Batch 178/1093 Trining Loss: 0.5739227315683043 Step: 5696/35000\n",
      "Epoch 1/3 Batch 179/1093 Trining Loss: 0.5716246765252598 Step: 5728/35000\n",
      "Epoch 1/3 Batch 180/1093 Trining Loss: 0.5693096231255267 Step: 5760/35000\n",
      "Epoch 1/3 Batch 181/1093 Trining Loss: 0.5670186074563811 Step: 5792/35000\n",
      "Epoch 1/3 Batch 182/1093 Trining Loss: 0.5645597382233694 Step: 5824/35000\n",
      "Epoch 1/3 Batch 183/1093 Trining Loss: 0.562348006852989 Step: 5856/35000\n",
      "Epoch 1/3 Batch 184/1093 Trining Loss: 0.5602070910289235 Step: 5888/35000\n",
      "Epoch 1/3 Batch 185/1093 Trining Loss: 0.5579572011490126 Step: 5920/35000\n",
      "Epoch 1/3 Batch 186/1093 Trining Loss: 0.5557643000477104 Step: 5952/35000\n",
      "Epoch 1/3 Batch 187/1093 Trining Loss: 0.5535401823048923 Step: 5984/35000\n",
      "Epoch 1/3 Batch 188/1093 Trining Loss: 0.5514292765329493 Step: 6016/35000\n",
      "Epoch 1/3 Batch 189/1093 Trining Loss: 0.5492431015249283 Step: 6048/35000\n",
      "Epoch 1/3 Batch 190/1093 Trining Loss: 0.5472531462970532 Step: 6080/35000\n",
      "Epoch 1/3 Batch 191/1093 Trining Loss: 0.5451257951284578 Step: 6112/35000\n",
      "Epoch 1/3 Batch 192/1093 Trining Loss: 0.5431161833306154 Step: 6144/35000\n",
      "Epoch 1/3 Batch 193/1093 Trining Loss: 0.5410651547612304 Step: 6176/35000\n",
      "Epoch 1/3 Batch 194/1093 Trining Loss: 0.5389756094088259 Step: 6208/35000\n",
      "Epoch 1/3 Batch 195/1093 Trining Loss: 0.5370098933959618 Step: 6240/35000\n",
      "Epoch 1/3 Batch 196/1093 Trining Loss: 0.5350171001437976 Step: 6272/35000\n",
      "Epoch 1/3 Batch 197/1093 Trining Loss: 0.5331154616199775 Step: 6304/35000\n",
      "Epoch 1/3 Batch 198/1093 Trining Loss: 0.5311438943250011 Step: 6336/35000\n",
      "Epoch 1/3 Batch 199/1093 Trining Loss: 0.529240832855953 Step: 6368/35000\n",
      "Epoch 1/3 Batch 200/1093 Trining Loss: 0.5272721844166517 Step: 6400/35000\n",
      "Epoch 1/3 Batch 201/1093 Trining Loss: 0.5252720937503511 Step: 6432/35000\n",
      "Epoch 1/3 Batch 202/1093 Trining Loss: 0.523309876496839 Step: 6464/35000\n",
      "Epoch 1/3 Batch 203/1093 Trining Loss: 0.5214160606632092 Step: 6496/35000\n",
      "Epoch 1/3 Batch 204/1093 Trining Loss: 0.519648041649192 Step: 6528/35000\n",
      "Epoch 1/3 Batch 205/1093 Trining Loss: 0.5178546804480436 Step: 6560/35000\n",
      "Epoch 1/3 Batch 206/1093 Trining Loss: 0.5160323867520082 Step: 6592/35000\n",
      "Epoch 1/3 Batch 207/1093 Trining Loss: 0.5141813013576655 Step: 6624/35000\n",
      "Epoch 1/3 Batch 208/1093 Trining Loss: 0.512389264212778 Step: 6656/35000\n",
      "Epoch 1/3 Batch 209/1093 Trining Loss: 0.5106712179748636 Step: 6688/35000\n",
      "Epoch 1/3 Batch 210/1093 Trining Loss: 0.5088538969556491 Step: 6720/35000\n",
      "Epoch 1/3 Batch 211/1093 Trining Loss: 0.5071252297733633 Step: 6752/35000\n",
      "Epoch 1/3 Batch 212/1093 Trining Loss: 0.5054602369947253 Step: 6784/35000\n",
      "Epoch 1/3 Batch 213/1093 Trining Loss: 0.5036219004104395 Step: 6816/35000\n",
      "Epoch 1/3 Batch 214/1093 Trining Loss: 0.5018919691373812 Step: 6848/35000\n",
      "Epoch 1/3 Batch 215/1093 Trining Loss: 0.5001536980964417 Step: 6880/35000\n",
      "Epoch 1/3 Batch 216/1093 Trining Loss: 0.4985522917230372 Step: 6912/35000\n",
      "Epoch 1/3 Batch 217/1093 Trining Loss: 0.49683072881489854 Step: 6944/35000\n",
      "Epoch 1/3 Batch 218/1093 Trining Loss: 0.49515969545469374 Step: 6976/35000\n",
      "Epoch 1/3 Batch 219/1093 Trining Loss: 0.49345030458686556 Step: 7008/35000\n",
      "Epoch 1/3 Batch 220/1093 Trining Loss: 0.49199732057750223 Step: 7040/35000\n",
      "Epoch 1/3 Batch 221/1093 Trining Loss: 0.49030560976509596 Step: 7072/35000\n",
      "Epoch 1/3 Batch 222/1093 Trining Loss: 0.48871206928480854 Step: 7104/35000\n",
      "Epoch 1/3 Batch 223/1093 Trining Loss: 0.4871256626774912 Step: 7136/35000\n",
      "Epoch 1/3 Batch 224/1093 Trining Loss: 0.4855147370004228 Step: 7168/35000\n",
      "Epoch 1/3 Batch 225/1093 Trining Loss: 0.4840329257647196 Step: 7200/35000\n",
      "Epoch 1/3 Batch 226/1093 Trining Loss: 0.4824211531178614 Step: 7232/35000\n",
      "Epoch 1/3 Batch 227/1093 Trining Loss: 0.48083256165361615 Step: 7264/35000\n",
      "Epoch 1/3 Batch 228/1093 Trining Loss: 0.47922812549299315 Step: 7296/35000\n",
      "Epoch 1/3 Batch 229/1093 Trining Loss: 0.47767586615668634 Step: 7328/35000\n",
      "Epoch 1/3 Batch 230/1093 Trining Loss: 0.4761727910326875 Step: 7360/35000\n",
      "Epoch 1/3 Batch 231/1093 Trining Loss: 0.47472673996444387 Step: 7392/35000\n",
      "Epoch 1/3 Batch 232/1093 Trining Loss: 0.4732437551920784 Step: 7424/35000\n",
      "Epoch 1/3 Batch 233/1093 Trining Loss: 0.4717184992371199 Step: 7456/35000\n",
      "Epoch 1/3 Batch 234/1093 Trining Loss: 0.4701521351424038 Step: 7488/35000\n",
      "Epoch 1/3 Batch 235/1093 Trining Loss: 0.46866053663035656 Step: 7520/35000\n",
      "Epoch 1/3 Batch 236/1093 Trining Loss: 0.46711490830501257 Step: 7552/35000\n",
      "Epoch 1/3 Batch 237/1093 Trining Loss: 0.4656444350375405 Step: 7584/35000\n",
      "Epoch 1/3 Batch 238/1093 Trining Loss: 0.46418364418028785 Step: 7616/35000\n",
      "Epoch 1/3 Batch 239/1093 Trining Loss: 0.46283101633378154 Step: 7648/35000\n",
      "Epoch 1/3 Batch 240/1093 Trining Loss: 0.4614386564431091 Step: 7680/35000\n",
      "Epoch 1/3 Batch 241/1093 Trining Loss: 0.4600595694916377 Step: 7712/35000\n",
      "Epoch 1/3 Batch 242/1093 Trining Loss: 0.4586516278218632 Step: 7744/35000\n",
      "Epoch 1/3 Batch 243/1093 Trining Loss: 0.45720247574794437 Step: 7776/35000\n",
      "Epoch 1/3 Batch 244/1093 Trining Loss: 0.4558025564937318 Step: 7808/35000\n",
      "Epoch 1/3 Batch 245/1093 Trining Loss: 0.4544289362673857 Step: 7840/35000\n",
      "Epoch 1/3 Batch 246/1093 Trining Loss: 0.4530881605436647 Step: 7872/35000\n",
      "Epoch 1/3 Batch 247/1093 Trining Loss: 0.4516434341910397 Step: 7904/35000\n",
      "Epoch 1/3 Batch 248/1093 Trining Loss: 0.45026074866614035 Step: 7936/35000\n",
      "Epoch 1/3 Batch 249/1093 Trining Loss: 0.44900388967799376 Step: 7968/35000\n",
      "Epoch 1/3 Batch 250/1093 Trining Loss: 0.4475952925980091 Step: 8000/35000\n",
      "Epoch 1/3 Batch 251/1093 Trining Loss: 0.4462631765708505 Step: 8032/35000\n",
      "Epoch 1/3 Batch 252/1093 Trining Loss: 0.4450063589546416 Step: 8064/35000\n",
      "Epoch 1/3 Batch 253/1093 Trining Loss: 0.4437237293293825 Step: 8096/35000\n",
      "Epoch 1/3 Batch 254/1093 Trining Loss: 0.4424304596375762 Step: 8128/35000\n",
      "Epoch 1/3 Batch 255/1093 Trining Loss: 0.44114396943181167 Step: 8160/35000\n",
      "Epoch 1/3 Batch 256/1093 Trining Loss: 0.43990625816513784 Step: 8192/35000\n",
      "Epoch 1/3 Batch 257/1093 Trining Loss: 0.4386241304387378 Step: 8224/35000\n",
      "Epoch 1/3 Batch 258/1093 Trining Loss: 0.4373317366942417 Step: 8256/35000\n",
      "Epoch 1/3 Batch 259/1093 Trining Loss: 0.4360712846906489 Step: 8288/35000\n",
      "Epoch 1/3 Batch 260/1093 Trining Loss: 0.43480806259008553 Step: 8320/35000\n",
      "Epoch 1/3 Batch 261/1093 Trining Loss: 0.4335686636667599 Step: 8352/35000\n",
      "Epoch 1/3 Batch 262/1093 Trining Loss: 0.43223246619219086 Step: 8384/35000\n",
      "Epoch 1/3 Batch 263/1093 Trining Loss: 0.4310706044784517 Step: 8416/35000\n",
      "Epoch 1/3 Batch 264/1093 Trining Loss: 0.42989332790514734 Step: 8448/35000\n",
      "Epoch 1/3 Batch 265/1093 Trining Loss: 0.4286536068005382 Step: 8480/35000\n",
      "Epoch 1/3 Batch 266/1093 Trining Loss: 0.42742205391588967 Step: 8512/35000\n",
      "Epoch 1/3 Batch 267/1093 Trining Loss: 0.4261457137903024 Step: 8544/35000\n",
      "Epoch 1/3 Batch 268/1093 Trining Loss: 0.4249648130429325 Step: 8576/35000\n",
      "Epoch 1/3 Batch 269/1093 Trining Loss: 0.42375904429690103 Step: 8608/35000\n",
      "Epoch 1/3 Batch 270/1093 Trining Loss: 0.4225633870672297 Step: 8640/35000\n",
      "Epoch 1/3 Batch 271/1093 Trining Loss: 0.4214420298808615 Step: 8672/35000\n",
      "Epoch 1/3 Batch 272/1093 Trining Loss: 0.42023858584134893 Step: 8704/35000\n",
      "Epoch 1/3 Batch 273/1093 Trining Loss: 0.4190695463191895 Step: 8736/35000\n",
      "Epoch 1/3 Batch 274/1093 Trining Loss: 0.41792194490885215 Step: 8768/35000\n",
      "Epoch 1/3 Batch 275/1093 Trining Loss: 0.4167145366560329 Step: 8800/35000\n",
      "Epoch 1/3 Batch 276/1093 Trining Loss: 0.4156208961554196 Step: 8832/35000\n",
      "Epoch 1/3 Batch 277/1093 Trining Loss: 0.41440053383688635 Step: 8864/35000\n",
      "Epoch 1/3 Batch 278/1093 Trining Loss: 0.4132858907147277 Step: 8896/35000\n",
      "Epoch 1/3 Batch 279/1093 Trining Loss: 0.4121480760928978 Step: 8928/35000\n",
      "Epoch 1/3 Batch 280/1093 Trining Loss: 0.41102770135871003 Step: 8960/35000\n",
      "Epoch 1/3 Batch 281/1093 Trining Loss: 0.409938002210707 Step: 8992/35000\n",
      "Epoch 1/3 Batch 282/1093 Trining Loss: 0.40890802979363616 Step: 9024/35000\n",
      "Epoch 1/3 Batch 283/1093 Trining Loss: 0.40780617412535125 Step: 9056/35000\n",
      "Epoch 1/3 Batch 284/1093 Trining Loss: 0.4067364895165386 Step: 9088/35000\n",
      "Epoch 1/3 Batch 285/1093 Trining Loss: 0.4057454482243772 Step: 9120/35000\n",
      "Epoch 1/3 Batch 286/1093 Trining Loss: 0.40464207988518935 Step: 9152/35000\n",
      "Epoch 1/3 Batch 287/1093 Trining Loss: 0.4035870297341397 Step: 9184/35000\n",
      "Epoch 1/3 Batch 288/1093 Trining Loss: 0.4025347928982228 Step: 9216/35000\n",
      "Epoch 1/3 Batch 289/1093 Trining Loss: 0.4014742700802001 Step: 9248/35000\n",
      "Epoch 1/3 Batch 290/1093 Trining Loss: 0.4004321304374728 Step: 9280/35000\n",
      "Epoch 1/3 Batch 291/1093 Trining Loss: 0.39938478581479325 Step: 9312/35000\n",
      "Epoch 1/3 Batch 292/1093 Trining Loss: 0.3983599471617235 Step: 9344/35000\n",
      "Epoch 1/3 Batch 293/1093 Trining Loss: 0.39728106082808035 Step: 9376/35000\n",
      "Epoch 1/3 Batch 294/1093 Trining Loss: 0.39625595264187474 Step: 9408/35000\n",
      "Epoch 1/3 Batch 295/1093 Trining Loss: 0.39527368497545434 Step: 9440/35000\n",
      "Epoch 1/3 Batch 296/1093 Trining Loss: 0.39429150139157837 Step: 9472/35000\n",
      "Epoch 1/3 Batch 297/1093 Trining Loss: 0.39323286742273006 Step: 9504/35000\n",
      "Epoch 1/3 Batch 298/1093 Trining Loss: 0.39231274244469283 Step: 9536/35000\n",
      "Epoch 1/3 Batch 299/1093 Trining Loss: 0.39136316736904675 Step: 9568/35000\n",
      "Epoch 1/3 Batch 300/1093 Trining Loss: 0.3904046917210023 Step: 9600/35000\n",
      "Epoch 1/3 Batch 301/1093 Trining Loss: 0.3894555952115312 Step: 9632/35000\n",
      "Epoch 1/3 Batch 302/1093 Trining Loss: 0.3884463965991475 Step: 9664/35000\n",
      "Epoch 1/3 Batch 303/1093 Trining Loss: 0.3875129435322072 Step: 9696/35000\n",
      "Epoch 1/3 Batch 304/1093 Trining Loss: 0.386538879333162 Step: 9728/35000\n",
      "Epoch 1/3 Batch 305/1093 Trining Loss: 0.3855445558418993 Step: 9760/35000\n",
      "Epoch 1/3 Batch 306/1093 Trining Loss: 0.3845673401906989 Step: 9792/35000\n",
      "Epoch 1/3 Batch 307/1093 Trining Loss: 0.38355558722337607 Step: 9824/35000\n",
      "Epoch 1/3 Batch 308/1093 Trining Loss: 0.3826073256938101 Step: 9856/35000\n",
      "Epoch 1/3 Batch 309/1093 Trining Loss: 0.3816436497572942 Step: 9888/35000\n",
      "Epoch 1/3 Batch 310/1093 Trining Loss: 0.3806755635286531 Step: 9920/35000\n",
      "Epoch 1/3 Batch 311/1093 Trining Loss: 0.3797045656722458 Step: 9952/35000\n",
      "Epoch 1/3 Batch 312/1093 Trining Loss: 0.3787682127589599 Step: 9984/35000\n",
      "Epoch 1/3 Batch 313/1093 Trining Loss: 0.377777790894714 Step: 10016/35000\n",
      "Epoch 1/3 Batch 314/1093 Trining Loss: 0.37686487626592824 Step: 10048/35000\n",
      "Epoch 1/3 Batch 315/1093 Trining Loss: 0.37590513742632337 Step: 10080/35000\n",
      "Epoch 1/3 Batch 316/1093 Trining Loss: 0.37496935236680357 Step: 10112/35000\n",
      "Epoch 1/3 Batch 317/1093 Trining Loss: 0.3740628577463258 Step: 10144/35000\n",
      "Epoch 1/3 Batch 318/1093 Trining Loss: 0.3731318878583938 Step: 10176/35000\n",
      "Epoch 1/3 Batch 319/1093 Trining Loss: 0.3722256864640033 Step: 10208/35000\n",
      "Epoch 1/3 Batch 320/1093 Trining Loss: 0.3712858340004459 Step: 10240/35000\n",
      "Epoch 1/3 Batch 321/1093 Trining Loss: 0.3703738928676766 Step: 10272/35000\n",
      "Epoch 1/3 Batch 322/1093 Trining Loss: 0.36945655335735833 Step: 10304/35000\n",
      "Epoch 1/3 Batch 323/1093 Trining Loss: 0.3685429373485016 Step: 10336/35000\n",
      "Epoch 1/3 Batch 324/1093 Trining Loss: 0.3676716272523742 Step: 10368/35000\n",
      "Epoch 1/3 Batch 325/1093 Trining Loss: 0.36677236476769814 Step: 10400/35000\n",
      "Epoch 1/3 Batch 326/1093 Trining Loss: 0.3658697207464031 Step: 10432/35000\n",
      "Epoch 1/3 Batch 327/1093 Trining Loss: 0.3649774113106071 Step: 10464/35000\n",
      "Epoch 1/3 Batch 328/1093 Trining Loss: 0.36408348763134424 Step: 10496/35000\n",
      "Epoch 1/3 Batch 329/1093 Trining Loss: 0.3632105155239352 Step: 10528/35000\n",
      "Epoch 1/3 Batch 330/1093 Trining Loss: 0.36228676682168787 Step: 10560/35000\n",
      "Epoch 1/3 Batch 331/1093 Trining Loss: 0.3614102868515199 Step: 10592/35000\n",
      "Epoch 1/3 Batch 332/1093 Trining Loss: 0.3604875999132553 Step: 10624/35000\n",
      "Epoch 1/3 Batch 333/1093 Trining Loss: 0.35966219688142026 Step: 10656/35000\n",
      "Epoch 1/3 Batch 334/1093 Trining Loss: 0.3588028634468952 Step: 10688/35000\n",
      "Epoch 1/3 Batch 335/1093 Trining Loss: 0.35792139399407513 Step: 10720/35000\n",
      "Epoch 1/3 Batch 336/1093 Trining Loss: 0.35713069624311866 Step: 10752/35000\n",
      "Epoch 1/3 Batch 337/1093 Trining Loss: 0.3562759619084593 Step: 10784/35000\n",
      "Epoch 1/3 Batch 338/1093 Trining Loss: 0.3554898255984106 Step: 10816/35000\n",
      "Epoch 1/3 Batch 339/1093 Trining Loss: 0.3546555173476185 Step: 10848/35000\n",
      "Epoch 1/3 Batch 340/1093 Trining Loss: 0.35378475288915284 Step: 10880/35000\n",
      "Epoch 1/3 Batch 341/1093 Trining Loss: 0.35295711183521755 Step: 10912/35000\n",
      "Epoch 1/3 Batch 342/1093 Trining Loss: 0.35213421162065356 Step: 10944/35000\n",
      "Epoch 1/3 Batch 343/1093 Trining Loss: 0.3512763007497822 Step: 10976/35000\n",
      "Epoch 1/3 Batch 344/1093 Trining Loss: 0.3504391779451696 Step: 11008/35000\n",
      "Epoch 1/3 Batch 345/1093 Trining Loss: 0.3495795610482278 Step: 11040/35000\n",
      "Epoch 1/3 Batch 346/1093 Trining Loss: 0.3487590464488792 Step: 11072/35000\n",
      "Epoch 1/3 Batch 347/1093 Trining Loss: 0.34797355194471413 Step: 11104/35000\n",
      "Epoch 1/3 Batch 348/1093 Trining Loss: 0.3471470644868825 Step: 11136/35000\n",
      "Epoch 1/3 Batch 349/1093 Trining Loss: 0.34632348115515915 Step: 11168/35000\n",
      "Epoch 1/3 Batch 350/1093 Trining Loss: 0.34552399735365597 Step: 11200/35000\n",
      "Epoch 1/3 Batch 351/1093 Trining Loss: 0.34472941021379244 Step: 11232/35000\n",
      "Epoch 1/3 Batch 352/1093 Trining Loss: 0.3439166409344497 Step: 11264/35000\n",
      "Epoch 1/3 Batch 353/1093 Trining Loss: 0.3431016628342884 Step: 11296/35000\n",
      "Epoch 1/3 Batch 354/1093 Trining Loss: 0.34231932632793477 Step: 11328/35000\n",
      "Epoch 1/3 Batch 355/1093 Trining Loss: 0.3415210683685793 Step: 11360/35000\n",
      "Epoch 1/3 Batch 356/1093 Trining Loss: 0.34074604162799843 Step: 11392/35000\n",
      "Epoch 1/3 Batch 357/1093 Trining Loss: 0.3399487397416967 Step: 11424/35000\n",
      "Epoch 1/3 Batch 358/1093 Trining Loss: 0.3391828873928366 Step: 11456/35000\n",
      "Epoch 1/3 Batch 359/1093 Trining Loss: 0.3384399952413644 Step: 11488/35000\n",
      "Epoch 1/3 Batch 360/1093 Trining Loss: 0.33765777728209895 Step: 11520/35000\n",
      "Epoch 1/3 Batch 361/1093 Trining Loss: 0.33685995272033104 Step: 11552/35000\n",
      "Epoch 1/3 Batch 362/1093 Trining Loss: 0.3361221157240769 Step: 11584/35000\n",
      "Epoch 1/3 Batch 363/1093 Trining Loss: 0.3353701302520007 Step: 11616/35000\n",
      "Epoch 1/3 Batch 364/1093 Trining Loss: 0.3346636758625999 Step: 11648/35000\n",
      "Epoch 1/3 Batch 365/1093 Trining Loss: 0.3339259109982889 Step: 11680/35000\n",
      "Epoch 1/3 Batch 366/1093 Trining Loss: 0.3331784669798417 Step: 11712/35000\n",
      "Epoch 1/3 Batch 367/1093 Trining Loss: 0.33242433941453614 Step: 11744/35000\n",
      "Epoch 1/3 Batch 368/1093 Trining Loss: 0.3316956980295641 Step: 11776/35000\n",
      "Epoch 1/3 Batch 369/1093 Trining Loss: 0.330958497849341 Step: 11808/35000\n",
      "Epoch 1/3 Batch 370/1093 Trining Loss: 0.3301975922910748 Step: 11840/35000\n",
      "Epoch 1/3 Batch 371/1093 Trining Loss: 0.3294755218003317 Step: 11872/35000\n",
      "Epoch 1/3 Batch 372/1093 Trining Loss: 0.3287536346363605 Step: 11904/35000\n",
      "Epoch 1/3 Batch 373/1093 Trining Loss: 0.3280235969888461 Step: 11936/35000\n",
      "Epoch 1/3 Batch 374/1093 Trining Loss: 0.32731828877433417 Step: 11968/35000\n",
      "Epoch 1/3 Batch 375/1093 Trining Loss: 0.3266114910443624 Step: 12000/35000\n",
      "Epoch 1/3 Batch 376/1093 Trining Loss: 0.3258936098043589 Step: 12032/35000\n",
      "Epoch 1/3 Batch 377/1093 Trining Loss: 0.32517782201501355 Step: 12064/35000\n",
      "Epoch 1/3 Batch 378/1093 Trining Loss: 0.3244708302140078 Step: 12096/35000\n",
      "Epoch 1/3 Batch 379/1093 Trining Loss: 0.3237837955532571 Step: 12128/35000\n",
      "Epoch 1/3 Batch 380/1093 Trining Loss: 0.32311592071660256 Step: 12160/35000\n",
      "Epoch 1/3 Batch 381/1093 Trining Loss: 0.3224353445816071 Step: 12192/35000\n",
      "Epoch 1/3 Batch 382/1093 Trining Loss: 0.3217752023809711 Step: 12224/35000\n",
      "Epoch 1/3 Batch 383/1093 Trining Loss: 0.32110895423641717 Step: 12256/35000\n",
      "Epoch 1/3 Batch 384/1093 Trining Loss: 0.32042220152410056 Step: 12288/35000\n",
      "Epoch 1/3 Batch 385/1093 Trining Loss: 0.31973474770784377 Step: 12320/35000\n",
      "Epoch 1/3 Batch 386/1093 Trining Loss: 0.3190838536102846 Step: 12352/35000\n",
      "Epoch 1/3 Batch 387/1093 Trining Loss: 0.31841755441801495 Step: 12384/35000\n",
      "Epoch 1/3 Batch 388/1093 Trining Loss: 0.3177862657694933 Step: 12416/35000\n",
      "Epoch 1/3 Batch 389/1093 Trining Loss: 0.3171426275299723 Step: 12448/35000\n",
      "Epoch 1/3 Batch 390/1093 Trining Loss: 0.31652944969634217 Step: 12480/35000\n",
      "Epoch 1/3 Batch 391/1093 Trining Loss: 0.3158770920542043 Step: 12512/35000\n",
      "Epoch 1/3 Batch 392/1093 Trining Loss: 0.3152409605776929 Step: 12544/35000\n",
      "Epoch 1/3 Batch 393/1093 Trining Loss: 0.3145759296022905 Step: 12576/35000\n",
      "Epoch 1/3 Batch 394/1093 Trining Loss: 0.31393528555704253 Step: 12608/35000\n",
      "Epoch 1/3 Batch 395/1093 Trining Loss: 0.3132987959664079 Step: 12640/35000\n",
      "Epoch 1/3 Batch 396/1093 Trining Loss: 0.31266423247077246 Step: 12672/35000\n",
      "Epoch 1/3 Batch 397/1093 Trining Loss: 0.3120308838578255 Step: 12704/35000\n",
      "Epoch 1/3 Batch 398/1093 Trining Loss: 0.31141446191986005 Step: 12736/35000\n",
      "Epoch 1/3 Batch 399/1093 Trining Loss: 0.3107753545979509 Step: 12768/35000\n",
      "Epoch 1/3 Batch 400/1093 Trining Loss: 0.3101373367290944 Step: 12800/35000\n",
      "Epoch 1/3 Batch 401/1093 Trining Loss: 0.30950074941105676 Step: 12832/35000\n",
      "Epoch 1/3 Batch 402/1093 Trining Loss: 0.30885856697771386 Step: 12864/35000\n",
      "Epoch 1/3 Batch 403/1093 Trining Loss: 0.3082257695588521 Step: 12896/35000\n",
      "Epoch 1/3 Batch 404/1093 Trining Loss: 0.30757713777999773 Step: 12928/35000\n",
      "Epoch 1/3 Batch 405/1093 Trining Loss: 0.306960993959212 Step: 12960/35000\n",
      "Epoch 1/3 Batch 406/1093 Trining Loss: 0.3063446699912325 Step: 12992/35000\n",
      "Epoch 1/3 Batch 407/1093 Trining Loss: 0.30571066291604637 Step: 13024/35000\n",
      "Epoch 1/3 Batch 408/1093 Trining Loss: 0.3051076896634756 Step: 13056/35000\n",
      "Epoch 1/3 Batch 409/1093 Trining Loss: 0.304494170758937 Step: 13088/35000\n",
      "Epoch 1/3 Batch 410/1093 Trining Loss: 0.30391082237588196 Step: 13120/35000\n",
      "Epoch 1/3 Batch 411/1093 Trining Loss: 0.303283715063203 Step: 13152/35000\n",
      "Epoch 1/3 Batch 412/1093 Trining Loss: 0.30270459884506407 Step: 13184/35000\n",
      "Epoch 1/3 Batch 413/1093 Trining Loss: 0.3020851453608375 Step: 13216/35000\n",
      "Epoch 1/3 Batch 414/1093 Trining Loss: 0.301502858599027 Step: 13248/35000\n",
      "Epoch 1/3 Batch 415/1093 Trining Loss: 0.30096629134502756 Step: 13280/35000\n",
      "Epoch 1/3 Batch 416/1093 Trining Loss: 0.3003590018219816 Step: 13312/35000\n",
      "Epoch 1/3 Batch 417/1093 Trining Loss: 0.29976921860012506 Step: 13344/35000\n",
      "Epoch 1/3 Batch 418/1093 Trining Loss: 0.2991959507171617 Step: 13376/35000\n",
      "Epoch 1/3 Batch 419/1093 Trining Loss: 0.2985995681267365 Step: 13408/35000\n",
      "Epoch 1/3 Batch 420/1093 Trining Loss: 0.2980625771163475 Step: 13440/35000\n",
      "Epoch 1/3 Batch 421/1093 Trining Loss: 0.29746008089206 Step: 13472/35000\n",
      "Epoch 1/3 Batch 422/1093 Trining Loss: 0.29691482997406715 Step: 13504/35000\n",
      "Epoch 1/3 Batch 423/1093 Trining Loss: 0.2963580716374918 Step: 13536/35000\n",
      "Epoch 1/3 Batch 424/1093 Trining Loss: 0.2957812074466415 Step: 13568/35000\n",
      "Epoch 1/3 Batch 425/1093 Trining Loss: 0.2951909275265301 Step: 13600/35000\n",
      "Epoch 1/3 Batch 426/1093 Trining Loss: 0.29464010819967645 Step: 13632/35000\n",
      "Epoch 1/3 Batch 427/1093 Trining Loss: 0.2940655132227815 Step: 13664/35000\n",
      "Epoch 1/3 Batch 428/1093 Trining Loss: 0.29354933696302854 Step: 13696/35000\n",
      "Epoch 1/3 Batch 429/1093 Trining Loss: 0.29298751796042166 Step: 13728/35000\n",
      "Epoch 1/3 Batch 430/1093 Trining Loss: 0.2924460458980743 Step: 13760/35000\n",
      "Epoch 1/3 Batch 431/1093 Trining Loss: 0.29188450171748337 Step: 13792/35000\n",
      "Epoch 1/3 Batch 432/1093 Trining Loss: 0.29137481983613084 Step: 13824/35000\n",
      "Epoch 1/3 Batch 433/1093 Trining Loss: 0.29084327428087625 Step: 13856/35000\n",
      "Epoch 1/3 Batch 434/1093 Trining Loss: 0.2903271023922252 Step: 13888/35000\n",
      "Epoch 1/3 Batch 435/1093 Trining Loss: 0.28983616006785423 Step: 13920/35000\n",
      "Epoch 1/3 Batch 436/1093 Trining Loss: 0.2893345435278131 Step: 13952/35000\n",
      "Epoch 1/3 Batch 437/1093 Trining Loss: 0.28882524817841004 Step: 13984/35000\n",
      "Epoch 1/3 Batch 438/1093 Trining Loss: 0.2883368334711687 Step: 14016/35000\n",
      "Epoch 1/3 Batch 439/1093 Trining Loss: 0.2878282639391058 Step: 14048/35000\n",
      "Epoch 1/3 Batch 440/1093 Trining Loss: 0.2873557641424916 Step: 14080/35000\n",
      "Epoch 1/3 Batch 441/1093 Trining Loss: 0.2868489537582376 Step: 14112/35000\n",
      "Epoch 1/3 Batch 442/1093 Trining Loss: 0.28631249746956705 Step: 14144/35000\n",
      "Epoch 1/3 Batch 443/1093 Trining Loss: 0.28582867479653984 Step: 14176/35000\n",
      "Epoch 1/3 Batch 444/1093 Trining Loss: 0.2853211554035813 Step: 14208/35000\n",
      "Epoch 1/3 Batch 445/1093 Trining Loss: 0.28483032650324736 Step: 14240/35000\n",
      "Epoch 1/3 Batch 446/1093 Trining Loss: 0.28433305209694687 Step: 14272/35000\n",
      "Epoch 1/3 Batch 447/1093 Trining Loss: 0.28383260655323134 Step: 14304/35000\n",
      "Epoch 1/3 Batch 448/1093 Trining Loss: 0.2833369084816825 Step: 14336/35000\n",
      "Epoch 1/3 Batch 449/1093 Trining Loss: 0.2828838032460823 Step: 14368/35000\n",
      "Epoch 1/3 Batch 450/1093 Trining Loss: 0.2824165542672078 Step: 14400/35000\n",
      "Epoch 1/3 Batch 451/1093 Trining Loss: 0.2819832626424581 Step: 14432/35000\n",
      "Epoch 1/3 Batch 452/1093 Trining Loss: 0.2814916790817427 Step: 14464/35000\n",
      "Epoch 1/3 Batch 453/1093 Trining Loss: 0.28103332254402424 Step: 14496/35000\n",
      "Epoch 1/3 Batch 454/1093 Trining Loss: 0.280599925962445 Step: 14528/35000\n",
      "Epoch 1/3 Batch 455/1093 Trining Loss: 0.2801359135207239 Step: 14560/35000\n",
      "Epoch 1/3 Batch 456/1093 Trining Loss: 0.2796780819792235 Step: 14592/35000\n",
      "Epoch 1/3 Batch 457/1093 Trining Loss: 0.27921316810196806 Step: 14624/35000\n",
      "Epoch 1/3 Batch 458/1093 Trining Loss: 0.27872003976044174 Step: 14656/35000\n",
      "Epoch 1/3 Batch 459/1093 Trining Loss: 0.2782284801416018 Step: 14688/35000\n",
      "Epoch 1/3 Batch 460/1093 Trining Loss: 0.2777847281049775 Step: 14720/35000\n",
      "Epoch 1/3 Batch 461/1093 Trining Loss: 0.27733152497907265 Step: 14752/35000\n",
      "Epoch 1/3 Batch 462/1093 Trining Loss: 0.2768850224697358 Step: 14784/35000\n",
      "Epoch 1/3 Batch 463/1093 Trining Loss: 0.27643689932402216 Step: 14816/35000\n",
      "Epoch 1/3 Batch 464/1093 Trining Loss: 0.2759542239203664 Step: 14848/35000\n",
      "Epoch 1/3 Batch 465/1093 Trining Loss: 0.2754788847379787 Step: 14880/35000\n",
      "Epoch 1/3 Batch 466/1093 Trining Loss: 0.27505394058549865 Step: 14912/35000\n",
      "Epoch 1/3 Batch 467/1093 Trining Loss: 0.2745859531378593 Step: 14944/35000\n",
      "Epoch 1/3 Batch 468/1093 Trining Loss: 0.2741372511586827 Step: 14976/35000\n",
      "Epoch 1/3 Batch 469/1093 Trining Loss: 0.2736706954758686 Step: 15008/35000\n",
      "Epoch 1/3 Batch 470/1093 Trining Loss: 0.27319320961833 Step: 15040/35000\n",
      "Epoch 1/3 Batch 471/1093 Trining Loss: 0.2727890414227346 Step: 15072/35000\n",
      "Epoch 1/3 Batch 472/1093 Trining Loss: 0.2723420349076011 Step: 15104/35000\n",
      "Epoch 1/3 Batch 473/1093 Trining Loss: 0.2718809852435649 Step: 15136/35000\n",
      "Epoch 1/3 Batch 474/1093 Trining Loss: 0.27142986198612157 Step: 15168/35000\n",
      "Epoch 1/3 Batch 475/1093 Trining Loss: 0.27097279241210537 Step: 15200/35000\n",
      "Epoch 1/3 Batch 476/1093 Trining Loss: 0.27051712092043473 Step: 15232/35000\n",
      "Epoch 1/3 Batch 477/1093 Trining Loss: 0.2700697291618998 Step: 15264/35000\n",
      "Epoch 1/3 Batch 478/1093 Trining Loss: 0.26960498945608297 Step: 15296/35000\n",
      "Epoch 1/3 Batch 479/1093 Trining Loss: 0.26915915223277936 Step: 15328/35000\n",
      "Epoch 1/3 Batch 480/1093 Trining Loss: 0.2686812107994532 Step: 15360/35000\n",
      "Epoch 1/3 Batch 481/1093 Trining Loss: 0.2682394329109955 Step: 15392/35000\n",
      "Epoch 1/3 Batch 482/1093 Trining Loss: 0.2677825366173913 Step: 15424/35000\n",
      "Epoch 1/3 Batch 483/1093 Trining Loss: 0.26735325059689596 Step: 15456/35000\n",
      "Epoch 1/3 Batch 484/1093 Trining Loss: 0.2668923826518753 Step: 15488/35000\n",
      "Epoch 1/3 Batch 485/1093 Trining Loss: 0.2664603547413939 Step: 15520/35000\n",
      "Epoch 1/3 Batch 486/1093 Trining Loss: 0.2660262753857384 Step: 15552/35000\n",
      "Epoch 1/3 Batch 487/1093 Trining Loss: 0.2655891419237277 Step: 15584/35000\n",
      "Epoch 1/3 Batch 488/1093 Trining Loss: 0.26511753321487885 Step: 15616/35000\n",
      "Epoch 1/3 Batch 489/1093 Trining Loss: 0.264668234111646 Step: 15648/35000\n",
      "Epoch 1/3 Batch 490/1093 Trining Loss: 0.2642536902077952 Step: 15680/35000\n",
      "Epoch 1/3 Batch 491/1093 Trining Loss: 0.2638205454481838 Step: 15712/35000\n",
      "Epoch 1/3 Batch 492/1093 Trining Loss: 0.2633772188706732 Step: 15744/35000\n",
      "Epoch 1/3 Batch 493/1093 Trining Loss: 0.26293802015014883 Step: 15776/35000\n",
      "Epoch 1/3 Batch 494/1093 Trining Loss: 0.26249862258673196 Step: 15808/35000\n",
      "Epoch 1/3 Batch 495/1093 Trining Loss: 0.26207565162519014 Step: 15840/35000\n",
      "Epoch 1/3 Batch 496/1093 Trining Loss: 0.2616388790699984 Step: 15872/35000\n",
      "Epoch 1/3 Batch 497/1093 Trining Loss: 0.2611838201326983 Step: 15904/35000\n",
      "Epoch 1/3 Batch 498/1093 Trining Loss: 0.2607512711266796 Step: 15936/35000\n",
      "Epoch 1/3 Batch 499/1093 Trining Loss: 0.26032863369954373 Step: 15968/35000\n",
      "Epoch 1/3 Batch 500/1093 Trining Loss: 0.2599168919324875 Step: 16000/35000\n",
      "Epoch 1/3 Batch 501/1093 Trining Loss: 0.2594825491948637 Step: 16032/35000\n",
      "Epoch 1/3 Batch 502/1093 Trining Loss: 0.25904490927093293 Step: 16064/35000\n",
      "Epoch 1/3 Batch 503/1093 Trining Loss: 0.2586341503170684 Step: 16096/35000\n",
      "Epoch 1/3 Batch 504/1093 Trining Loss: 0.25822094961675623 Step: 16128/35000\n",
      "Epoch 1/3 Batch 505/1093 Trining Loss: 0.25781386239398824 Step: 16160/35000\n",
      "Epoch 1/3 Batch 506/1093 Trining Loss: 0.2574124389103514 Step: 16192/35000\n",
      "Epoch 1/3 Batch 507/1093 Trining Loss: 0.2569786591172453 Step: 16224/35000\n",
      "Epoch 1/3 Batch 508/1093 Trining Loss: 0.2565696187185373 Step: 16256/35000\n",
      "Epoch 1/3 Batch 509/1093 Trining Loss: 0.25613958195349557 Step: 16288/35000\n",
      "Epoch 1/3 Batch 510/1093 Trining Loss: 0.2557415421467786 Step: 16320/35000\n",
      "Epoch 1/3 Batch 511/1093 Trining Loss: 0.2553516624243875 Step: 16352/35000\n",
      "Epoch 1/3 Batch 512/1093 Trining Loss: 0.2549431918596383 Step: 16384/35000\n",
      "Epoch 1/3 Batch 513/1093 Trining Loss: 0.254526606973326 Step: 16416/35000\n",
      "Epoch 1/3 Batch 514/1093 Trining Loss: 0.2541237282005034 Step: 16448/35000\n",
      "Epoch 1/3 Batch 515/1093 Trining Loss: 0.2537466931256276 Step: 16480/35000\n",
      "Epoch 1/3 Batch 516/1093 Trining Loss: 0.2533482963638019 Step: 16512/35000\n",
      "Epoch 1/3 Batch 517/1093 Trining Loss: 0.25292863423001144 Step: 16544/35000\n",
      "Epoch 1/3 Batch 518/1093 Trining Loss: 0.2525470857094959 Step: 16576/35000\n",
      "Epoch 1/3 Batch 519/1093 Trining Loss: 0.252168357817309 Step: 16608/35000\n",
      "Epoch 1/3 Batch 520/1093 Trining Loss: 0.2517871878682994 Step: 16640/35000\n",
      "Epoch 1/3 Batch 521/1093 Trining Loss: 0.2513881955874973 Step: 16672/35000\n",
      "Epoch 1/3 Batch 522/1093 Trining Loss: 0.2510044862626841 Step: 16704/35000\n",
      "Epoch 1/3 Batch 523/1093 Trining Loss: 0.25058393492710634 Step: 16736/35000\n",
      "Epoch 1/3 Batch 523/1093 Trining Loss: 0.25010572131083325\n",
      "Epoch 2/3 Batch 524/1093 Trining Loss: 8.038093712721162e-05 Step: 16768/35000\n",
      "Epoch 2/3 Batch 525/1093 Trining Loss: 0.00016302137147812616 Step: 16800/35000\n",
      "Epoch 2/3 Batch 526/1093 Trining Loss: 0.00023208567058178862 Step: 16832/35000\n",
      "Epoch 2/3 Batch 527/1093 Trining Loss: 0.00032680893786944296 Step: 16864/35000\n",
      "Epoch 2/3 Batch 528/1093 Trining Loss: 0.00040050279913526594 Step: 16896/35000\n",
      "Epoch 2/3 Batch 529/1093 Trining Loss: 0.00046281060925728885 Step: 16928/35000\n",
      "Epoch 2/3 Batch 530/1093 Trining Loss: 0.0005533609328404912 Step: 16960/35000\n",
      "Epoch 2/3 Batch 531/1093 Trining Loss: 0.0006386135720600516 Step: 16992/35000\n",
      "Epoch 2/3 Batch 532/1093 Trining Loss: 0.0007251318180023279 Step: 17024/35000\n",
      "Epoch 2/3 Batch 533/1093 Trining Loss: 0.0008270425781411927 Step: 17056/35000\n",
      "Epoch 2/3 Batch 534/1093 Trining Loss: 0.0009299109337593286 Step: 17088/35000\n",
      "Epoch 2/3 Batch 535/1093 Trining Loss: 0.00103503216371358 Step: 17120/35000\n",
      "Epoch 2/3 Batch 536/1093 Trining Loss: 0.0011445457157470396 Step: 17152/35000\n",
      "Epoch 2/3 Batch 537/1093 Trining Loss: 0.0012424159274753912 Step: 17184/35000\n",
      "Epoch 2/3 Batch 538/1093 Trining Loss: 0.0013302142652215567 Step: 17216/35000\n",
      "Epoch 2/3 Batch 539/1093 Trining Loss: 0.0014112710344548129 Step: 17248/35000\n",
      "Epoch 2/3 Batch 540/1093 Trining Loss: 0.0015036357359753715 Step: 17280/35000\n",
      "Epoch 2/3 Batch 541/1093 Trining Loss: 0.0016209171351575587 Step: 17312/35000\n",
      "Epoch 2/3 Batch 542/1093 Trining Loss: 0.0016924722182024888 Step: 17344/35000\n",
      "Epoch 2/3 Batch 543/1093 Trining Loss: 0.0017599002295216584 Step: 17376/35000\n",
      "Epoch 2/3 Batch 544/1093 Trining Loss: 0.0018367332009160344 Step: 17408/35000\n",
      "Epoch 2/3 Batch 545/1093 Trining Loss: 0.0019118121936233766 Step: 17440/35000\n",
      "Epoch 2/3 Batch 546/1093 Trining Loss: 0.0020088546742231417 Step: 17472/35000\n",
      "Epoch 2/3 Batch 547/1093 Trining Loss: 0.002099541249179317 Step: 17504/35000\n",
      "Epoch 2/3 Batch 548/1093 Trining Loss: 0.002192147885088938 Step: 17536/35000\n",
      "Epoch 2/3 Batch 549/1093 Trining Loss: 0.0022628979523325226 Step: 17568/35000\n",
      "Epoch 2/3 Batch 550/1093 Trining Loss: 0.002336762229149992 Step: 17600/35000\n",
      "Epoch 2/3 Batch 551/1093 Trining Loss: 0.002413191428634519 Step: 17632/35000\n",
      "Epoch 2/3 Batch 552/1093 Trining Loss: 0.0024868497098593607 Step: 17664/35000\n",
      "Epoch 2/3 Batch 553/1093 Trining Loss: 0.00256478691483708 Step: 17696/35000\n",
      "Epoch 2/3 Batch 554/1093 Trining Loss: 0.0026506928774101208 Step: 17728/35000\n",
      "Epoch 2/3 Batch 555/1093 Trining Loss: 0.002722260329100463 Step: 17760/35000\n",
      "Epoch 2/3 Batch 556/1093 Trining Loss: 0.002808112610908721 Step: 17792/35000\n",
      "Epoch 2/3 Batch 557/1093 Trining Loss: 0.0028742346210368446 Step: 17824/35000\n",
      "Epoch 2/3 Batch 558/1093 Trining Loss: 0.0029776686782478005 Step: 17856/35000\n",
      "Epoch 2/3 Batch 559/1093 Trining Loss: 0.003038184207485912 Step: 17888/35000\n",
      "Epoch 2/3 Batch 560/1093 Trining Loss: 0.0031359746120870114 Step: 17920/35000\n",
      "Epoch 2/3 Batch 561/1093 Trining Loss: 0.003198310101999106 Step: 17952/35000\n",
      "Epoch 2/3 Batch 562/1093 Trining Loss: 0.0032809458845344725 Step: 17984/35000\n",
      "Epoch 2/3 Batch 563/1093 Trining Loss: 0.0033459541442445073 Step: 18016/35000\n",
      "Epoch 2/3 Batch 564/1093 Trining Loss: 0.003422130672062965 Step: 18048/35000\n",
      "Epoch 2/3 Batch 565/1093 Trining Loss: 0.0035072006417059265 Step: 18080/35000\n",
      "Epoch 2/3 Batch 566/1093 Trining Loss: 0.0036096010199178656 Step: 18112/35000\n",
      "Epoch 2/3 Batch 567/1093 Trining Loss: 0.0036765673580518685 Step: 18144/35000\n",
      "Epoch 2/3 Batch 568/1093 Trining Loss: 0.0037499618008208107 Step: 18176/35000\n",
      "Epoch 2/3 Batch 569/1093 Trining Loss: 0.0038035547128356406 Step: 18208/35000\n",
      "Epoch 2/3 Batch 570/1093 Trining Loss: 0.0038837771797389314 Step: 18240/35000\n",
      "Epoch 2/3 Batch 571/1093 Trining Loss: 0.003955466636895298 Step: 18272/35000\n",
      "Epoch 2/3 Batch 572/1093 Trining Loss: 0.004007622215349774 Step: 18304/35000\n",
      "Epoch 2/3 Batch 573/1093 Trining Loss: 0.004064523795356301 Step: 18336/35000\n",
      "Epoch 2/3 Batch 574/1093 Trining Loss: 0.004149580348399873 Step: 18368/35000\n",
      "Epoch 2/3 Batch 575/1093 Trining Loss: 0.004222905810760415 Step: 18400/35000\n",
      "Epoch 2/3 Batch 576/1093 Trining Loss: 0.004296580885743929 Step: 18432/35000\n",
      "Epoch 2/3 Batch 577/1093 Trining Loss: 0.004361610071962793 Step: 18464/35000\n",
      "Epoch 2/3 Batch 578/1093 Trining Loss: 0.004413161562130526 Step: 18496/35000\n",
      "Epoch 2/3 Batch 579/1093 Trining Loss: 0.004455936750934326 Step: 18528/35000\n",
      "Epoch 2/3 Batch 580/1093 Trining Loss: 0.004514272514216859 Step: 18560/35000\n",
      "Epoch 2/3 Batch 581/1093 Trining Loss: 0.004566383748451527 Step: 18592/35000\n",
      "Epoch 2/3 Batch 582/1093 Trining Loss: 0.004633808940572222 Step: 18624/35000\n",
      "Epoch 2/3 Batch 583/1093 Trining Loss: 0.004708082611651813 Step: 18656/35000\n",
      "Epoch 2/3 Batch 584/1093 Trining Loss: 0.004766129196481141 Step: 18688/35000\n",
      "Epoch 2/3 Batch 585/1093 Trining Loss: 0.004826475797682746 Step: 18720/35000\n",
      "Epoch 2/3 Batch 586/1093 Trining Loss: 0.0048975345135091105 Step: 18752/35000\n",
      "Epoch 2/3 Batch 587/1093 Trining Loss: 0.004948877829430051 Step: 18784/35000\n",
      "Epoch 2/3 Batch 588/1093 Trining Loss: 0.005005346614309922 Step: 18816/35000\n",
      "Epoch 2/3 Batch 589/1093 Trining Loss: 0.0050622704067284875 Step: 18848/35000\n",
      "Epoch 2/3 Batch 590/1093 Trining Loss: 0.00512931295433792 Step: 18880/35000\n",
      "Epoch 2/3 Batch 591/1093 Trining Loss: 0.00517906533796876 Step: 18912/35000\n",
      "Epoch 2/3 Batch 592/1093 Trining Loss: 0.005235040897025248 Step: 18944/35000\n",
      "Epoch 2/3 Batch 593/1093 Trining Loss: 0.00528862669398069 Step: 18976/35000\n",
      "Epoch 2/3 Batch 594/1093 Trining Loss: 0.005342523228715767 Step: 19008/35000\n",
      "Epoch 2/3 Batch 595/1093 Trining Loss: 0.005381878033405592 Step: 19040/35000\n",
      "Epoch 2/3 Batch 596/1093 Trining Loss: 0.005426559322232368 Step: 19072/35000\n",
      "Epoch 2/3 Batch 597/1093 Trining Loss: 0.005460756638990575 Step: 19104/35000\n",
      "Epoch 2/3 Batch 598/1093 Trining Loss: 0.005509867136643882 Step: 19136/35000\n",
      "Epoch 2/3 Batch 599/1093 Trining Loss: 0.005562184431367804 Step: 19168/35000\n",
      "Epoch 2/3 Batch 600/1093 Trining Loss: 0.005604271205763022 Step: 19200/35000\n",
      "Epoch 2/3 Batch 601/1093 Trining Loss: 0.005641945930641027 Step: 19232/35000\n",
      "Epoch 2/3 Batch 602/1093 Trining Loss: 0.005684165383048628 Step: 19264/35000\n",
      "Epoch 2/3 Batch 603/1093 Trining Loss: 0.005740583727559442 Step: 19296/35000\n",
      "Epoch 2/3 Batch 604/1093 Trining Loss: 0.005787561444059903 Step: 19328/35000\n",
      "Epoch 2/3 Batch 605/1093 Trining Loss: 0.005832917920567773 Step: 19360/35000\n",
      "Epoch 2/3 Batch 606/1093 Trining Loss: 0.005881924360449558 Step: 19392/35000\n",
      "Epoch 2/3 Batch 607/1093 Trining Loss: 0.005925052365076404 Step: 19424/35000\n",
      "Epoch 2/3 Batch 608/1093 Trining Loss: 0.005994902157812919 Step: 19456/35000\n",
      "Epoch 2/3 Batch 609/1093 Trining Loss: 0.0060417887514643675 Step: 19488/35000\n",
      "Epoch 2/3 Batch 610/1093 Trining Loss: 0.006120028218529264 Step: 19520/35000\n",
      "Epoch 2/3 Batch 611/1093 Trining Loss: 0.006178111722006931 Step: 19552/35000\n",
      "Epoch 2/3 Batch 612/1093 Trining Loss: 0.006273821343247797 Step: 19584/35000\n",
      "Epoch 2/3 Batch 613/1093 Trining Loss: 0.006313792346303654 Step: 19616/35000\n",
      "Epoch 2/3 Batch 614/1093 Trining Loss: 0.006365615710713188 Step: 19648/35000\n",
      "Epoch 2/3 Batch 615/1093 Trining Loss: 0.006449443210915821 Step: 19680/35000\n",
      "Epoch 2/3 Batch 616/1093 Trining Loss: 0.006521714083221439 Step: 19712/35000\n",
      "Epoch 2/3 Batch 617/1093 Trining Loss: 0.00657633946819089 Step: 19744/35000\n",
      "Epoch 2/3 Batch 618/1093 Trining Loss: 0.006641585271335342 Step: 19776/35000\n",
      "Epoch 2/3 Batch 619/1093 Trining Loss: 0.006690188255371685 Step: 19808/35000\n",
      "Epoch 2/3 Batch 620/1093 Trining Loss: 0.006739734635958748 Step: 19840/35000\n",
      "Epoch 2/3 Batch 621/1093 Trining Loss: 0.006796926775896223 Step: 19872/35000\n",
      "Epoch 2/3 Batch 622/1093 Trining Loss: 0.006866565728829605 Step: 19904/35000\n",
      "Epoch 2/3 Batch 623/1093 Trining Loss: 0.006913726901213966 Step: 19936/35000\n",
      "Epoch 2/3 Batch 624/1093 Trining Loss: 0.006974566226395277 Step: 19968/35000\n",
      "Epoch 2/3 Batch 625/1093 Trining Loss: 0.007065025019645691 Step: 20000/35000\n",
      "Epoch 2/3 Batch 626/1093 Trining Loss: 0.007099702853126267 Step: 20032/35000\n",
      "Epoch 2/3 Batch 627/1093 Trining Loss: 0.007144695935114339 Step: 20064/35000\n",
      "Epoch 2/3 Batch 628/1093 Trining Loss: 0.007217925219873714 Step: 20096/35000\n",
      "Epoch 2/3 Batch 629/1093 Trining Loss: 0.007284639879990079 Step: 20128/35000\n",
      "Epoch 2/3 Batch 630/1093 Trining Loss: 0.007330279744097164 Step: 20160/35000\n",
      "Epoch 2/3 Batch 631/1093 Trining Loss: 0.007377023541511711 Step: 20192/35000\n",
      "Epoch 2/3 Batch 632/1093 Trining Loss: 0.007455020288264827 Step: 20224/35000\n",
      "Epoch 2/3 Batch 633/1093 Trining Loss: 0.007510548066141857 Step: 20256/35000\n",
      "Epoch 2/3 Batch 634/1093 Trining Loss: 0.0075436714178756204 Step: 20288/35000\n",
      "Epoch 2/3 Batch 635/1093 Trining Loss: 0.007588582216050681 Step: 20320/35000\n",
      "Epoch 2/3 Batch 636/1093 Trining Loss: 0.007643436257428718 Step: 20352/35000\n",
      "Epoch 2/3 Batch 637/1093 Trining Loss: 0.007693359040577707 Step: 20384/35000\n",
      "Epoch 2/3 Batch 638/1093 Trining Loss: 0.007738954980265011 Step: 20416/35000\n",
      "Epoch 2/3 Batch 639/1093 Trining Loss: 0.0077866217316782714 Step: 20448/35000\n",
      "Epoch 2/3 Batch 640/1093 Trining Loss: 0.007848830666625872 Step: 20480/35000\n",
      "Epoch 2/3 Batch 641/1093 Trining Loss: 0.007896788906474567 Step: 20512/35000\n",
      "Epoch 2/3 Batch 642/1093 Trining Loss: 0.007941046427946966 Step: 20544/35000\n",
      "Epoch 2/3 Batch 643/1093 Trining Loss: 0.007990172211063408 Step: 20576/35000\n",
      "Epoch 2/3 Batch 644/1093 Trining Loss: 0.008023048727917745 Step: 20608/35000\n",
      "Epoch 2/3 Batch 645/1093 Trining Loss: 0.008064283003178678 Step: 20640/35000\n",
      "Epoch 2/3 Batch 646/1093 Trining Loss: 0.008104608691190788 Step: 20672/35000\n",
      "Epoch 2/3 Batch 647/1093 Trining Loss: 0.00815063867562521 Step: 20704/35000\n",
      "Epoch 2/3 Batch 648/1093 Trining Loss: 0.00819440897738124 Step: 20736/35000\n",
      "Epoch 2/3 Batch 649/1093 Trining Loss: 0.008250665344378247 Step: 20768/35000\n",
      "Epoch 2/3 Batch 650/1093 Trining Loss: 0.008279601888587841 Step: 20800/35000\n",
      "Epoch 2/3 Batch 651/1093 Trining Loss: 0.008331212886078384 Step: 20832/35000\n",
      "Epoch 2/3 Batch 652/1093 Trining Loss: 0.008370577320570777 Step: 20864/35000\n",
      "Epoch 2/3 Batch 653/1093 Trining Loss: 0.008398698329081213 Step: 20896/35000\n",
      "Epoch 2/3 Batch 654/1093 Trining Loss: 0.008435418279642176 Step: 20928/35000\n",
      "Epoch 2/3 Batch 655/1093 Trining Loss: 0.00848371009164639 Step: 20960/35000\n",
      "Epoch 2/3 Batch 656/1093 Trining Loss: 0.008526975127141468 Step: 20992/35000\n",
      "Epoch 2/3 Batch 657/1093 Trining Loss: 0.008557248985340968 Step: 21024/35000\n",
      "Epoch 2/3 Batch 658/1093 Trining Loss: 0.008590467112835237 Step: 21056/35000\n",
      "Epoch 2/3 Batch 659/1093 Trining Loss: 0.008624424880910354 Step: 21088/35000\n",
      "Epoch 2/3 Batch 660/1093 Trining Loss: 0.008656532737906232 Step: 21120/35000\n",
      "Epoch 2/3 Batch 661/1093 Trining Loss: 0.008690956920813685 Step: 21152/35000\n",
      "Epoch 2/3 Batch 662/1093 Trining Loss: 0.008719467422910868 Step: 21184/35000\n",
      "Epoch 2/3 Batch 663/1093 Trining Loss: 0.008753952108110058 Step: 21216/35000\n",
      "Epoch 2/3 Batch 664/1093 Trining Loss: 0.008788843939350132 Step: 21248/35000\n",
      "Epoch 2/3 Batch 665/1093 Trining Loss: 0.008831651881337166 Step: 21280/35000\n",
      "Epoch 2/3 Batch 666/1093 Trining Loss: 0.008858331769995682 Step: 21312/35000\n",
      "Epoch 2/3 Batch 667/1093 Trining Loss: 0.008906816784432088 Step: 21344/35000\n",
      "Epoch 2/3 Batch 668/1093 Trining Loss: 0.00894994923041877 Step: 21376/35000\n",
      "Epoch 2/3 Batch 669/1093 Trining Loss: 0.008980987927638361 Step: 21408/35000\n",
      "Epoch 2/3 Batch 670/1093 Trining Loss: 0.00901376451296148 Step: 21440/35000\n",
      "Epoch 2/3 Batch 671/1093 Trining Loss: 0.009038128699946333 Step: 21472/35000\n",
      "Epoch 2/3 Batch 672/1093 Trining Loss: 0.009086314646992832 Step: 21504/35000\n",
      "Epoch 2/3 Batch 673/1093 Trining Loss: 0.00910805792646015 Step: 21536/35000\n",
      "Epoch 2/3 Batch 674/1093 Trining Loss: 0.00914400805185828 Step: 21568/35000\n",
      "Epoch 2/3 Batch 675/1093 Trining Loss: 0.00916834792605153 Step: 21600/35000\n",
      "Epoch 2/3 Batch 676/1093 Trining Loss: 0.009207601433057757 Step: 21632/35000\n",
      "Epoch 2/3 Batch 677/1093 Trining Loss: 0.009242087943779313 Step: 21664/35000\n",
      "Epoch 2/3 Batch 678/1093 Trining Loss: 0.009263618593722317 Step: 21696/35000\n",
      "Epoch 2/3 Batch 679/1093 Trining Loss: 0.009299193109784807 Step: 21728/35000\n",
      "Epoch 2/3 Batch 680/1093 Trining Loss: 0.00933185830602751 Step: 21760/35000\n",
      "Epoch 2/3 Batch 681/1093 Trining Loss: 0.00935766437855411 Step: 21792/35000\n",
      "Epoch 2/3 Batch 682/1093 Trining Loss: 0.009386055608473914 Step: 21824/35000\n",
      "Epoch 2/3 Batch 683/1093 Trining Loss: 0.009417733548435425 Step: 21856/35000\n",
      "Epoch 2/3 Batch 684/1093 Trining Loss: 0.009430361786388864 Step: 21888/35000\n",
      "Epoch 2/3 Batch 685/1093 Trining Loss: 0.009460661348200193 Step: 21920/35000\n",
      "Epoch 2/3 Batch 686/1093 Trining Loss: 0.009484150886818027 Step: 21952/35000\n",
      "Epoch 2/3 Batch 687/1093 Trining Loss: 0.009496886738653267 Step: 21984/35000\n",
      "Epoch 2/3 Batch 688/1093 Trining Loss: 0.009514960292510168 Step: 22016/35000\n",
      "Epoch 2/3 Batch 689/1093 Trining Loss: 0.009553566694000117 Step: 22048/35000\n",
      "Epoch 2/3 Batch 690/1093 Trining Loss: 0.009575754609228907 Step: 22080/35000\n",
      "Epoch 2/3 Batch 691/1093 Trining Loss: 0.009594026009529268 Step: 22112/35000\n",
      "Epoch 2/3 Batch 692/1093 Trining Loss: 0.00962595161416627 Step: 22144/35000\n",
      "Epoch 2/3 Batch 693/1093 Trining Loss: 0.009647273107782586 Step: 22176/35000\n",
      "Epoch 2/3 Batch 694/1093 Trining Loss: 0.009684119840563202 Step: 22208/35000\n",
      "Epoch 2/3 Batch 695/1093 Trining Loss: 0.009713432050758987 Step: 22240/35000\n",
      "Epoch 2/3 Batch 696/1093 Trining Loss: 0.009738402513401776 Step: 22272/35000\n",
      "Epoch 2/3 Batch 697/1093 Trining Loss: 0.009779301019040721 Step: 22304/35000\n",
      "Epoch 2/3 Batch 698/1093 Trining Loss: 0.009808348072826179 Step: 22336/35000\n",
      "Epoch 2/3 Batch 699/1093 Trining Loss: 0.009841777866596998 Step: 22368/35000\n",
      "Epoch 2/3 Batch 700/1093 Trining Loss: 0.009882363424237286 Step: 22400/35000\n",
      "Epoch 2/3 Batch 701/1093 Trining Loss: 0.009912960776014947 Step: 22432/35000\n",
      "Epoch 2/3 Batch 702/1093 Trining Loss: 0.00993695867080719 Step: 22464/35000\n",
      "Epoch 2/3 Batch 703/1093 Trining Loss: 0.009959530599341114 Step: 22496/35000\n",
      "Epoch 2/3 Batch 704/1093 Trining Loss: 0.009996681569398126 Step: 22528/35000\n",
      "Epoch 2/3 Batch 705/1093 Trining Loss: 0.010025618920195187 Step: 22560/35000\n",
      "Epoch 2/3 Batch 706/1093 Trining Loss: 0.010052474381840095 Step: 22592/35000\n",
      "Epoch 2/3 Batch 707/1093 Trining Loss: 0.010093656614886483 Step: 22624/35000\n",
      "Epoch 2/3 Batch 708/1093 Trining Loss: 0.010108078651046012 Step: 22656/35000\n",
      "Epoch 2/3 Batch 709/1093 Trining Loss: 0.01013377963792538 Step: 22688/35000\n",
      "Epoch 2/3 Batch 710/1093 Trining Loss: 0.010188826910731658 Step: 22720/35000\n",
      "Epoch 2/3 Batch 711/1093 Trining Loss: 0.010227982979136848 Step: 22752/35000\n",
      "Epoch 2/3 Batch 712/1093 Trining Loss: 0.010258625881495268 Step: 22784/35000\n",
      "Epoch 2/3 Batch 713/1093 Trining Loss: 0.010285254850001355 Step: 22816/35000\n",
      "Epoch 2/3 Batch 714/1093 Trining Loss: 0.010321188024153896 Step: 22848/35000\n",
      "Epoch 2/3 Batch 715/1093 Trining Loss: 0.010347798946020486 Step: 22880/35000\n",
      "Epoch 2/3 Batch 716/1093 Trining Loss: 0.010370704387028458 Step: 22912/35000\n",
      "Epoch 2/3 Batch 717/1093 Trining Loss: 0.010409212329605467 Step: 22944/35000\n",
      "Epoch 2/3 Batch 718/1093 Trining Loss: 0.010442689061164856 Step: 22976/35000\n",
      "Epoch 2/3 Batch 719/1093 Trining Loss: 0.010459167719591973 Step: 23008/35000\n",
      "Epoch 2/3 Batch 720/1093 Trining Loss: 0.010486013680282567 Step: 23040/35000\n",
      "Epoch 2/3 Batch 721/1093 Trining Loss: 0.010524633233325656 Step: 23072/35000\n",
      "Epoch 2/3 Batch 722/1093 Trining Loss: 0.010557593592027218 Step: 23104/35000\n",
      "Epoch 2/3 Batch 723/1093 Trining Loss: 0.010584215907944188 Step: 23136/35000\n",
      "Epoch 2/3 Batch 724/1093 Trining Loss: 0.010618911672754017 Step: 23168/35000\n",
      "Epoch 2/3 Batch 725/1093 Trining Loss: 0.010635183301979097 Step: 23200/35000\n",
      "Epoch 2/3 Batch 726/1093 Trining Loss: 0.010654925786923442 Step: 23232/35000\n",
      "Epoch 2/3 Batch 727/1093 Trining Loss: 0.01068475995176924 Step: 23264/35000\n",
      "Epoch 2/3 Batch 728/1093 Trining Loss: 0.010700978111004927 Step: 23296/35000\n",
      "Epoch 2/3 Batch 729/1093 Trining Loss: 0.010717473781076152 Step: 23328/35000\n",
      "Epoch 2/3 Batch 730/1093 Trining Loss: 0.010734866470200558 Step: 23360/35000\n",
      "Epoch 2/3 Batch 731/1093 Trining Loss: 0.010776519757554079 Step: 23392/35000\n",
      "Epoch 2/3 Batch 732/1093 Trining Loss: 0.010810753424738436 Step: 23424/35000\n",
      "Epoch 2/3 Batch 733/1093 Trining Loss: 0.010847133646578124 Step: 23456/35000\n",
      "Epoch 2/3 Batch 734/1093 Trining Loss: 0.010870356762360487 Step: 23488/35000\n",
      "Epoch 2/3 Batch 735/1093 Trining Loss: 0.010890677586501958 Step: 23520/35000\n",
      "Epoch 2/3 Batch 736/1093 Trining Loss: 0.010927395869815804 Step: 23552/35000\n",
      "Epoch 2/3 Batch 737/1093 Trining Loss: 0.010943704556415978 Step: 23584/35000\n",
      "Epoch 2/3 Batch 738/1093 Trining Loss: 0.01096202339724635 Step: 23616/35000\n",
      "Epoch 2/3 Batch 739/1093 Trining Loss: 0.010990016273297218 Step: 23648/35000\n",
      "Epoch 2/3 Batch 740/1093 Trining Loss: 0.011024651933159378 Step: 23680/35000\n",
      "Epoch 2/3 Batch 741/1093 Trining Loss: 0.01104725404880066 Step: 23712/35000\n",
      "Epoch 2/3 Batch 742/1093 Trining Loss: 0.011075381709217543 Step: 23744/35000\n",
      "Epoch 2/3 Batch 743/1093 Trining Loss: 0.011097336867335347 Step: 23776/35000\n",
      "Epoch 2/3 Batch 744/1093 Trining Loss: 0.011117265957798208 Step: 23808/35000\n",
      "Epoch 2/3 Batch 745/1093 Trining Loss: 0.011141173126513526 Step: 23840/35000\n",
      "Epoch 2/3 Batch 746/1093 Trining Loss: 0.011162644284640017 Step: 23872/35000\n",
      "Epoch 2/3 Batch 747/1093 Trining Loss: 0.011180421736445453 Step: 23904/35000\n",
      "Epoch 2/3 Batch 748/1093 Trining Loss: 0.011197344952805794 Step: 23936/35000\n",
      "Epoch 2/3 Batch 749/1093 Trining Loss: 0.011229631241077734 Step: 23968/35000\n",
      "Epoch 2/3 Batch 750/1093 Trining Loss: 0.01125041963160038 Step: 24000/35000\n",
      "Epoch 2/3 Batch 751/1093 Trining Loss: 0.011271901927223694 Step: 24032/35000\n",
      "Epoch 2/3 Batch 752/1093 Trining Loss: 0.011281019686027727 Step: 24064/35000\n",
      "Epoch 2/3 Batch 753/1093 Trining Loss: 0.011297819730275339 Step: 24096/35000\n",
      "Epoch 2/3 Batch 754/1093 Trining Loss: 0.011320607635422315 Step: 24128/35000\n",
      "Epoch 2/3 Batch 755/1093 Trining Loss: 0.011341727909860232 Step: 24160/35000\n",
      "Epoch 2/3 Batch 756/1093 Trining Loss: 0.011364895071854036 Step: 24192/35000\n",
      "Epoch 2/3 Batch 757/1093 Trining Loss: 0.011389758336988907 Step: 24224/35000\n",
      "Epoch 2/3 Batch 758/1093 Trining Loss: 0.011417545139848558 Step: 24256/35000\n",
      "Epoch 2/3 Batch 759/1093 Trining Loss: 0.011433784959148199 Step: 24288/35000\n",
      "Epoch 2/3 Batch 760/1093 Trining Loss: 0.011462793224736263 Step: 24320/35000\n",
      "Epoch 2/3 Batch 761/1093 Trining Loss: 0.01148293291692195 Step: 24352/35000\n",
      "Epoch 2/3 Batch 762/1093 Trining Loss: 0.011489024317366245 Step: 24384/35000\n",
      "Epoch 2/3 Batch 763/1093 Trining Loss: 0.011506406509782508 Step: 24416/35000\n",
      "Epoch 2/3 Batch 764/1093 Trining Loss: 0.011539337459308003 Step: 24448/35000\n",
      "Epoch 2/3 Batch 765/1093 Trining Loss: 0.011574109448911319 Step: 24480/35000\n",
      "Epoch 2/3 Batch 766/1093 Trining Loss: 0.011623928848624851 Step: 24512/35000\n",
      "Epoch 2/3 Batch 767/1093 Trining Loss: 0.01163234828086145 Step: 24544/35000\n",
      "Epoch 2/3 Batch 768/1093 Trining Loss: 0.011660480660793837 Step: 24576/35000\n",
      "Epoch 2/3 Batch 769/1093 Trining Loss: 0.011708139649002542 Step: 24608/35000\n",
      "Epoch 2/3 Batch 770/1093 Trining Loss: 0.011725247711814069 Step: 24640/35000\n",
      "Epoch 2/3 Batch 771/1093 Trining Loss: 0.011736220751445736 Step: 24672/35000\n",
      "Epoch 2/3 Batch 772/1093 Trining Loss: 0.011763887272404574 Step: 24704/35000\n",
      "Epoch 2/3 Batch 773/1093 Trining Loss: 0.011795072827155655 Step: 24736/35000\n",
      "Epoch 2/3 Batch 774/1093 Trining Loss: 0.011812131457242546 Step: 24768/35000\n",
      "Epoch 2/3 Batch 775/1093 Trining Loss: 0.011834397601985163 Step: 24800/35000\n",
      "Epoch 2/3 Batch 776/1093 Trining Loss: 0.011859977337022879 Step: 24832/35000\n",
      "Epoch 2/3 Batch 777/1093 Trining Loss: 0.011874354539134635 Step: 24864/35000\n",
      "Epoch 2/3 Batch 778/1093 Trining Loss: 0.01189243610814252 Step: 24896/35000\n",
      "Epoch 2/3 Batch 779/1093 Trining Loss: 0.01192184840597987 Step: 24928/35000\n",
      "Epoch 2/3 Batch 780/1093 Trining Loss: 0.011950445172782893 Step: 24960/35000\n",
      "Epoch 2/3 Batch 781/1093 Trining Loss: 0.011990882372829428 Step: 24992/35000\n",
      "Epoch 2/3 Batch 782/1093 Trining Loss: 0.01201439320879138 Step: 25024/35000\n",
      "Epoch 2/3 Batch 783/1093 Trining Loss: 0.012049190079172453 Step: 25056/35000\n",
      "Epoch 2/3 Batch 784/1093 Trining Loss: 0.012086879730471695 Step: 25088/35000\n",
      "Epoch 2/3 Batch 785/1093 Trining Loss: 0.012106304468622632 Step: 25120/35000\n",
      "Epoch 2/3 Batch 786/1093 Trining Loss: 0.012124294562042517 Step: 25152/35000\n",
      "Epoch 2/3 Batch 787/1093 Trining Loss: 0.012154418875710634 Step: 25184/35000\n",
      "Epoch 2/3 Batch 788/1093 Trining Loss: 0.01218879334439481 Step: 25216/35000\n",
      "Epoch 2/3 Batch 789/1093 Trining Loss: 0.012201273981458182 Step: 25248/35000\n",
      "Epoch 2/3 Batch 790/1093 Trining Loss: 0.012234483593249622 Step: 25280/35000\n",
      "Epoch 2/3 Batch 791/1093 Trining Loss: 0.01226107191233207 Step: 25312/35000\n",
      "Epoch 2/3 Batch 792/1093 Trining Loss: 0.012279991671969795 Step: 25344/35000\n",
      "Epoch 2/3 Batch 793/1093 Trining Loss: 0.01228966472334549 Step: 25376/35000\n",
      "Epoch 2/3 Batch 794/1093 Trining Loss: 0.012315554483336225 Step: 25408/35000\n",
      "Epoch 2/3 Batch 795/1093 Trining Loss: 0.012348542025066772 Step: 25440/35000\n",
      "Epoch 2/3 Batch 796/1093 Trining Loss: 0.012375738935067725 Step: 25472/35000\n",
      "Epoch 2/3 Batch 797/1093 Trining Loss: 0.012398611560202154 Step: 25504/35000\n",
      "Epoch 2/3 Batch 798/1093 Trining Loss: 0.012425963729526614 Step: 25536/35000\n",
      "Epoch 2/3 Batch 799/1093 Trining Loss: 0.01244238770341918 Step: 25568/35000\n",
      "Epoch 2/3 Batch 800/1093 Trining Loss: 0.012461163236293942 Step: 25600/35000\n",
      "Epoch 2/3 Batch 801/1093 Trining Loss: 0.012487661291132407 Step: 25632/35000\n",
      "Epoch 2/3 Batch 802/1093 Trining Loss: 0.01250322052089502 Step: 25664/35000\n",
      "Epoch 2/3 Batch 803/1093 Trining Loss: 0.012523622227691178 Step: 25696/35000\n",
      "Epoch 2/3 Batch 804/1093 Trining Loss: 0.012534904057410225 Step: 25728/35000\n",
      "Epoch 2/3 Batch 805/1093 Trining Loss: 0.012554661390481529 Step: 25760/35000\n",
      "Epoch 2/3 Batch 806/1093 Trining Loss: 0.012572488775268826 Step: 25792/35000\n",
      "Epoch 2/3 Batch 807/1093 Trining Loss: 0.01257773427595067 Step: 25824/35000\n",
      "Epoch 2/3 Batch 808/1093 Trining Loss: 0.012598741313184399 Step: 25856/35000\n",
      "Epoch 2/3 Batch 809/1093 Trining Loss: 0.012618189539451829 Step: 25888/35000\n",
      "Epoch 2/3 Batch 810/1093 Trining Loss: 0.012633767684944617 Step: 25920/35000\n",
      "Epoch 2/3 Batch 811/1093 Trining Loss: 0.012654581002698138 Step: 25952/35000\n",
      "Epoch 2/3 Batch 812/1093 Trining Loss: 0.012667641321224679 Step: 25984/35000\n",
      "Epoch 2/3 Batch 813/1093 Trining Loss: 0.012679688655399016 Step: 26016/35000\n",
      "Epoch 2/3 Batch 814/1093 Trining Loss: 0.012688425227688484 Step: 26048/35000\n",
      "Epoch 2/3 Batch 815/1093 Trining Loss: 0.01270451897446729 Step: 26080/35000\n",
      "Epoch 2/3 Batch 816/1093 Trining Loss: 0.012721651640045 Step: 26112/35000\n",
      "Epoch 2/3 Batch 817/1093 Trining Loss: 0.012738108245092769 Step: 26144/35000\n",
      "Epoch 2/3 Batch 818/1093 Trining Loss: 0.012751602096378366 Step: 26176/35000\n",
      "Epoch 2/3 Batch 819/1093 Trining Loss: 0.012767673723663457 Step: 26208/35000\n",
      "Epoch 2/3 Batch 820/1093 Trining Loss: 0.012778885049245707 Step: 26240/35000\n",
      "Epoch 2/3 Batch 821/1093 Trining Loss: 0.012796909431501515 Step: 26272/35000\n",
      "Epoch 2/3 Batch 822/1093 Trining Loss: 0.01280466556648776 Step: 26304/35000\n",
      "Epoch 2/3 Batch 823/1093 Trining Loss: 0.012816808013333729 Step: 26336/35000\n",
      "Epoch 2/3 Batch 824/1093 Trining Loss: 0.012831466378716445 Step: 26368/35000\n",
      "Epoch 2/3 Batch 825/1093 Trining Loss: 0.012841450757149493 Step: 26400/35000\n",
      "Epoch 2/3 Batch 826/1093 Trining Loss: 0.012862542184119363 Step: 26432/35000\n",
      "Epoch 2/3 Batch 827/1093 Trining Loss: 0.012867925501684179 Step: 26464/35000\n",
      "Epoch 2/3 Batch 828/1093 Trining Loss: 0.012878942260617652 Step: 26496/35000\n",
      "Epoch 2/3 Batch 829/1093 Trining Loss: 0.012896466274510967 Step: 26528/35000\n",
      "Epoch 2/3 Batch 830/1093 Trining Loss: 0.01291443066200219 Step: 26560/35000\n",
      "Epoch 2/3 Batch 831/1093 Trining Loss: 0.012919567086062563 Step: 26592/35000\n",
      "Epoch 2/3 Batch 832/1093 Trining Loss: 0.01292767525704291 Step: 26624/35000\n",
      "Epoch 2/3 Batch 833/1093 Trining Loss: 0.012946356277988881 Step: 26656/35000\n",
      "Epoch 2/3 Batch 834/1093 Trining Loss: 0.012961982957107559 Step: 26688/35000\n",
      "Epoch 2/3 Batch 835/1093 Trining Loss: 0.012974765265505471 Step: 26720/35000\n",
      "Epoch 2/3 Batch 836/1093 Trining Loss: 0.012983655149304267 Step: 26752/35000\n",
      "Epoch 2/3 Batch 837/1093 Trining Loss: 0.012997301900703705 Step: 26784/35000\n",
      "Epoch 2/3 Batch 838/1093 Trining Loss: 0.013005844737472 Step: 26816/35000\n",
      "Epoch 2/3 Batch 839/1093 Trining Loss: 0.01301447352720813 Step: 26848/35000\n",
      "Epoch 2/3 Batch 840/1093 Trining Loss: 0.013039840952981087 Step: 26880/35000\n",
      "Epoch 2/3 Batch 841/1093 Trining Loss: 0.013045097691284915 Step: 26912/35000\n",
      "Epoch 2/3 Batch 842/1093 Trining Loss: 0.013066828930020475 Step: 26944/35000\n",
      "Epoch 2/3 Batch 843/1093 Trining Loss: 0.013088429700207597 Step: 26976/35000\n",
      "Epoch 2/3 Batch 844/1093 Trining Loss: 0.013095198245498383 Step: 27008/35000\n",
      "Epoch 2/3 Batch 845/1093 Trining Loss: 0.01311771984504172 Step: 27040/35000\n",
      "Epoch 2/3 Batch 846/1093 Trining Loss: 0.013141915195535684 Step: 27072/35000\n",
      "Epoch 2/3 Batch 847/1093 Trining Loss: 0.013146586610617857 Step: 27104/35000\n",
      "Epoch 2/3 Batch 848/1093 Trining Loss: 0.013167407315329841 Step: 27136/35000\n",
      "Epoch 2/3 Batch 849/1093 Trining Loss: 0.013188465176977313 Step: 27168/35000\n",
      "Epoch 2/3 Batch 850/1093 Trining Loss: 0.01320313125191366 Step: 27200/35000\n",
      "Epoch 2/3 Batch 851/1093 Trining Loss: 0.013235970607112773 Step: 27232/35000\n",
      "Epoch 2/3 Batch 852/1093 Trining Loss: 0.013257737960141729 Step: 27264/35000\n",
      "Epoch 2/3 Batch 853/1093 Trining Loss: 0.013268694766346203 Step: 27296/35000\n",
      "Epoch 2/3 Batch 854/1093 Trining Loss: 0.013282629554388954 Step: 27328/35000\n",
      "Epoch 2/3 Batch 855/1093 Trining Loss: 0.013300241453692928 Step: 27360/35000\n",
      "Epoch 2/3 Batch 856/1093 Trining Loss: 0.01331156015256855 Step: 27392/35000\n",
      "Epoch 2/3 Batch 857/1093 Trining Loss: 0.013321131861213347 Step: 27424/35000\n",
      "Epoch 2/3 Batch 858/1093 Trining Loss: 0.013343296013772488 Step: 27456/35000\n",
      "Epoch 2/3 Batch 859/1093 Trining Loss: 0.01335280921800242 Step: 27488/35000\n",
      "Epoch 2/3 Batch 860/1093 Trining Loss: 0.013357769481317942 Step: 27520/35000\n",
      "Epoch 2/3 Batch 861/1093 Trining Loss: 0.013380072240768547 Step: 27552/35000\n",
      "Epoch 2/3 Batch 862/1093 Trining Loss: 0.01338802776980082 Step: 27584/35000\n",
      "Epoch 2/3 Batch 863/1093 Trining Loss: 0.013402351546225377 Step: 27616/35000\n",
      "Epoch 2/3 Batch 864/1093 Trining Loss: 0.013408091475462748 Step: 27648/35000\n",
      "Epoch 2/3 Batch 865/1093 Trining Loss: 0.01341853891580091 Step: 27680/35000\n",
      "Epoch 2/3 Batch 866/1093 Trining Loss: 0.013426136436246284 Step: 27712/35000\n",
      "Epoch 2/3 Batch 867/1093 Trining Loss: 0.013442098840882072 Step: 27744/35000\n",
      "Epoch 2/3 Batch 868/1093 Trining Loss: 0.013449399877974789 Step: 27776/35000\n",
      "Epoch 2/3 Batch 869/1093 Trining Loss: 0.013463439053338197 Step: 27808/35000\n",
      "Epoch 2/3 Batch 870/1093 Trining Loss: 0.013470562585982783 Step: 27840/35000\n",
      "Epoch 2/3 Batch 871/1093 Trining Loss: 0.01347332450547325 Step: 27872/35000\n",
      "Epoch 2/3 Batch 872/1093 Trining Loss: 0.013484691408539721 Step: 27904/35000\n",
      "Epoch 2/3 Batch 873/1093 Trining Loss: 0.013492889730057083 Step: 27936/35000\n",
      "Epoch 2/3 Batch 874/1093 Trining Loss: 0.013500708524026914 Step: 27968/35000\n",
      "Epoch 2/3 Batch 875/1093 Trining Loss: 0.013518566744668142 Step: 28000/35000\n",
      "Epoch 2/3 Batch 876/1093 Trining Loss: 0.013530579999463471 Step: 28032/35000\n",
      "Epoch 2/3 Batch 877/1093 Trining Loss: 0.013551019034438692 Step: 28064/35000\n",
      "Epoch 2/3 Batch 878/1093 Trining Loss: 0.013558554897988576 Step: 28096/35000\n",
      "Epoch 2/3 Batch 879/1093 Trining Loss: 0.013572243492588384 Step: 28128/35000\n",
      "Epoch 2/3 Batch 880/1093 Trining Loss: 0.013589209028181027 Step: 28160/35000\n",
      "Epoch 2/3 Batch 881/1093 Trining Loss: 0.013602543789280259 Step: 28192/35000\n",
      "Epoch 2/3 Batch 882/1093 Trining Loss: 0.01360594126651414 Step: 28224/35000\n",
      "Epoch 2/3 Batch 883/1093 Trining Loss: 0.01361755452130416 Step: 28256/35000\n",
      "Epoch 2/3 Batch 884/1093 Trining Loss: 0.013636409802303325 Step: 28288/35000\n",
      "Epoch 2/3 Batch 885/1093 Trining Loss: 0.013644795151332677 Step: 28320/35000\n",
      "Epoch 2/3 Batch 886/1093 Trining Loss: 0.01364961867147119 Step: 28352/35000\n",
      "Epoch 2/3 Batch 887/1093 Trining Loss: 0.013657668210658442 Step: 28384/35000\n",
      "Epoch 2/3 Batch 888/1093 Trining Loss: 0.013659626430085113 Step: 28416/35000\n",
      "Epoch 2/3 Batch 889/1093 Trining Loss: 0.013667593392699589 Step: 28448/35000\n",
      "Epoch 2/3 Batch 890/1093 Trining Loss: 0.01366961659821734 Step: 28480/35000\n",
      "Epoch 2/3 Batch 891/1093 Trining Loss: 0.013690772614787792 Step: 28512/35000\n",
      "Epoch 2/3 Batch 892/1093 Trining Loss: 0.01370135399579768 Step: 28544/35000\n",
      "Epoch 2/3 Batch 893/1093 Trining Loss: 0.013719846479895067 Step: 28576/35000\n",
      "Epoch 2/3 Batch 894/1093 Trining Loss: 0.013722446806503823 Step: 28608/35000\n",
      "Epoch 2/3 Batch 895/1093 Trining Loss: 0.013728052497367952 Step: 28640/35000\n",
      "Epoch 2/3 Batch 896/1093 Trining Loss: 0.013746027844068262 Step: 28672/35000\n",
      "Epoch 2/3 Batch 897/1093 Trining Loss: 0.01374998477411144 Step: 28704/35000\n",
      "Epoch 2/3 Batch 898/1093 Trining Loss: 0.013765310031675084 Step: 28736/35000\n",
      "Epoch 2/3 Batch 899/1093 Trining Loss: 0.013785141043663357 Step: 28768/35000\n",
      "Epoch 2/3 Batch 900/1093 Trining Loss: 0.013797537190839648 Step: 28800/35000\n",
      "Epoch 2/3 Batch 901/1093 Trining Loss: 0.01381361016592261 Step: 28832/35000\n",
      "Epoch 2/3 Batch 902/1093 Trining Loss: 0.013829413527816931 Step: 28864/35000\n",
      "Epoch 2/3 Batch 903/1093 Trining Loss: 0.01383596595690265 Step: 28896/35000\n",
      "Epoch 2/3 Batch 904/1093 Trining Loss: 0.013848087879153282 Step: 28928/35000\n",
      "Epoch 2/3 Batch 905/1093 Trining Loss: 0.013854399694963384 Step: 28960/35000\n",
      "Epoch 2/3 Batch 906/1093 Trining Loss: 0.01386558853406418 Step: 28992/35000\n",
      "Epoch 2/3 Batch 907/1093 Trining Loss: 0.013867991840900174 Step: 29024/35000\n",
      "Epoch 2/3 Batch 908/1093 Trining Loss: 0.013875140602359885 Step: 29056/35000\n",
      "Epoch 2/3 Batch 909/1093 Trining Loss: 0.013887791966095289 Step: 29088/35000\n",
      "Epoch 2/3 Batch 910/1093 Trining Loss: 0.01389772087478867 Step: 29120/35000\n",
      "Epoch 2/3 Batch 911/1093 Trining Loss: 0.013900373295628344 Step: 29152/35000\n",
      "Epoch 2/3 Batch 912/1093 Trining Loss: 0.013912548928872863 Step: 29184/35000\n",
      "Epoch 2/3 Batch 913/1093 Trining Loss: 0.013919130650115366 Step: 29216/35000\n",
      "Epoch 2/3 Batch 914/1093 Trining Loss: 0.013928427431259622 Step: 29248/35000\n",
      "Epoch 2/3 Batch 915/1093 Trining Loss: 0.013930293715106007 Step: 29280/35000\n",
      "Epoch 2/3 Batch 916/1093 Trining Loss: 0.013933807687720031 Step: 29312/35000\n",
      "Epoch 2/3 Batch 917/1093 Trining Loss: 0.013933510116363429 Step: 29344/35000\n",
      "Epoch 2/3 Batch 918/1093 Trining Loss: 0.013940272623498584 Step: 29376/35000\n",
      "Epoch 2/3 Batch 919/1093 Trining Loss: 0.013942388525806261 Step: 29408/35000\n",
      "Epoch 2/3 Batch 920/1093 Trining Loss: 0.013947490169464246 Step: 29440/35000\n",
      "Epoch 2/3 Batch 921/1093 Trining Loss: 0.013953048697431105 Step: 29472/35000\n",
      "Epoch 2/3 Batch 922/1093 Trining Loss: 0.013950168966253059 Step: 29504/35000\n",
      "Epoch 2/3 Batch 923/1093 Trining Loss: 0.01395566461916573 Step: 29536/35000\n",
      "Epoch 2/3 Batch 924/1093 Trining Loss: 0.013962296209624641 Step: 29568/35000\n",
      "Epoch 2/3 Batch 925/1093 Trining Loss: 0.013968712350202573 Step: 29600/35000\n",
      "Epoch 2/3 Batch 926/1093 Trining Loss: 0.013975732540410036 Step: 29632/35000\n",
      "Epoch 2/3 Batch 927/1093 Trining Loss: 0.013976058820232411 Step: 29664/35000\n",
      "Epoch 2/3 Batch 928/1093 Trining Loss: 0.0139834744974184 Step: 29696/35000\n",
      "Epoch 2/3 Batch 929/1093 Trining Loss: 0.013994688796735618 Step: 29728/35000\n",
      "Epoch 2/3 Batch 930/1093 Trining Loss: 0.014006318543506886 Step: 29760/35000\n",
      "Epoch 2/3 Batch 931/1093 Trining Loss: 0.014021348031127683 Step: 29792/35000\n",
      "Epoch 2/3 Batch 932/1093 Trining Loss: 0.014038963150432885 Step: 29824/35000\n",
      "Epoch 2/3 Batch 933/1093 Trining Loss: 0.014045480783481754 Step: 29856/35000\n",
      "Epoch 2/3 Batch 934/1093 Trining Loss: 0.014055564151019878 Step: 29888/35000\n",
      "Epoch 2/3 Batch 935/1093 Trining Loss: 0.01406815064881734 Step: 29920/35000\n",
      "Epoch 2/3 Batch 936/1093 Trining Loss: 0.014086260526982319 Step: 29952/35000\n",
      "Epoch 2/3 Batch 937/1093 Trining Loss: 0.014099314432141938 Step: 29984/35000\n",
      "Epoch 2/3 Batch 938/1093 Trining Loss: 0.014108063463471147 Step: 30016/35000\n",
      "Epoch 2/3 Batch 939/1093 Trining Loss: 0.0141177632329198 Step: 30048/35000\n",
      "Epoch 2/3 Batch 940/1093 Trining Loss: 0.014130680927848246 Step: 30080/35000\n",
      "Epoch 2/3 Batch 941/1093 Trining Loss: 0.01413852066161877 Step: 30112/35000\n",
      "Epoch 2/3 Batch 942/1093 Trining Loss: 0.014149812082789696 Step: 30144/35000\n",
      "Epoch 2/3 Batch 943/1093 Trining Loss: 0.014149633343879644 Step: 30176/35000\n",
      "Epoch 2/3 Batch 944/1093 Trining Loss: 0.014166796479326815 Step: 30208/35000\n",
      "Epoch 2/3 Batch 945/1093 Trining Loss: 0.01418552024378663 Step: 30240/35000\n",
      "Epoch 2/3 Batch 946/1093 Trining Loss: 0.0142015504885079 Step: 30272/35000\n",
      "Epoch 2/3 Batch 947/1093 Trining Loss: 0.01420260623880242 Step: 30304/35000\n",
      "Epoch 2/3 Batch 948/1093 Trining Loss: 0.01422079259486484 Step: 30336/35000\n",
      "Epoch 2/3 Batch 949/1093 Trining Loss: 0.014234058958359526 Step: 30368/35000\n",
      "Epoch 2/3 Batch 950/1093 Trining Loss: 0.014243898075073957 Step: 30400/35000\n",
      "Epoch 2/3 Batch 951/1093 Trining Loss: 0.014255024822767663 Step: 30432/35000\n",
      "Epoch 2/3 Batch 952/1093 Trining Loss: 0.014275114287763145 Step: 30464/35000\n",
      "Epoch 2/3 Batch 953/1093 Trining Loss: 0.01427861348728787 Step: 30496/35000\n",
      "Epoch 2/3 Batch 954/1093 Trining Loss: 0.01430321939238806 Step: 30528/35000\n",
      "Epoch 2/3 Batch 955/1093 Trining Loss: 0.014304198513870464 Step: 30560/35000\n",
      "Epoch 2/3 Batch 956/1093 Trining Loss: 0.014311078690366515 Step: 30592/35000\n",
      "Epoch 2/3 Batch 957/1093 Trining Loss: 0.01432071221932037 Step: 30624/35000\n",
      "Epoch 2/3 Batch 958/1093 Trining Loss: 0.01432653245494734 Step: 30656/35000\n",
      "Epoch 2/3 Batch 959/1093 Trining Loss: 0.014330340562945736 Step: 30688/35000\n",
      "Epoch 2/3 Batch 960/1093 Trining Loss: 0.014336732152150944 Step: 30720/35000\n",
      "Epoch 2/3 Batch 961/1093 Trining Loss: 0.014340004341969505 Step: 30752/35000\n",
      "Epoch 2/3 Batch 962/1093 Trining Loss: 0.014337566797379521 Step: 30784/35000\n",
      "Epoch 2/3 Batch 963/1093 Trining Loss: 0.014344159492435608 Step: 30816/35000\n",
      "Epoch 2/3 Batch 964/1093 Trining Loss: 0.014346530162478137 Step: 30848/35000\n",
      "Epoch 2/3 Batch 965/1093 Trining Loss: 0.014347425582842814 Step: 30880/35000\n",
      "Epoch 2/3 Batch 966/1093 Trining Loss: 0.014353385111259193 Step: 30912/35000\n",
      "Epoch 2/3 Batch 967/1093 Trining Loss: 0.0143570804452252 Step: 30944/35000\n",
      "Epoch 2/3 Batch 968/1093 Trining Loss: 0.01436003163545431 Step: 30976/35000\n",
      "Epoch 2/3 Batch 969/1093 Trining Loss: 0.014364182273400765 Step: 31008/35000\n",
      "Epoch 2/3 Batch 970/1093 Trining Loss: 0.014375842569069456 Step: 31040/35000\n",
      "Epoch 2/3 Batch 971/1093 Trining Loss: 0.014376134002613849 Step: 31072/35000\n",
      "Epoch 2/3 Batch 972/1093 Trining Loss: 0.014373545627849399 Step: 31104/35000\n",
      "Epoch 2/3 Batch 973/1093 Trining Loss: 0.014379032590796929 Step: 31136/35000\n",
      "Epoch 2/3 Batch 974/1093 Trining Loss: 0.014392611858467486 Step: 31168/35000\n",
      "Epoch 2/3 Batch 975/1093 Trining Loss: 0.014401314590030755 Step: 31200/35000\n",
      "Epoch 2/3 Batch 976/1093 Trining Loss: 0.014407669011785909 Step: 31232/35000\n",
      "Epoch 2/3 Batch 977/1093 Trining Loss: 0.014414321300416361 Step: 31264/35000\n",
      "Epoch 2/3 Batch 978/1093 Trining Loss: 0.014415547982094417 Step: 31296/35000\n",
      "Epoch 2/3 Batch 979/1093 Trining Loss: 0.01442423676054038 Step: 31328/35000\n",
      "Epoch 2/3 Batch 980/1093 Trining Loss: 0.01442466772218444 Step: 31360/35000\n",
      "Epoch 2/3 Batch 981/1093 Trining Loss: 0.014428735358467767 Step: 31392/35000\n",
      "Epoch 2/3 Batch 982/1093 Trining Loss: 0.014438583574384876 Step: 31424/35000\n",
      "Epoch 2/3 Batch 983/1093 Trining Loss: 0.014442009561423247 Step: 31456/35000\n",
      "Epoch 2/3 Batch 984/1093 Trining Loss: 0.014444624880192483 Step: 31488/35000\n",
      "Epoch 2/3 Batch 985/1093 Trining Loss: 0.014443821919524124 Step: 31520/35000\n",
      "Epoch 2/3 Batch 986/1093 Trining Loss: 0.014445608950259719 Step: 31552/35000\n",
      "Epoch 2/3 Batch 987/1093 Trining Loss: 0.014450351327370848 Step: 31584/35000\n",
      "Epoch 2/3 Batch 988/1093 Trining Loss: 0.014456249403989749 Step: 31616/35000\n",
      "Epoch 2/3 Batch 989/1093 Trining Loss: 0.014459587241870448 Step: 31648/35000\n",
      "Epoch 2/3 Batch 990/1093 Trining Loss: 0.014466814694907328 Step: 31680/35000\n",
      "Epoch 2/3 Batch 991/1093 Trining Loss: 0.01447764560993757 Step: 31712/35000\n",
      "Epoch 2/3 Batch 992/1093 Trining Loss: 0.014478757151312405 Step: 31744/35000\n",
      "Epoch 2/3 Batch 993/1093 Trining Loss: 0.01448982644147143 Step: 31776/35000\n",
      "Epoch 2/3 Batch 994/1093 Trining Loss: 0.014485077235390482 Step: 31808/35000\n",
      "Epoch 2/3 Batch 995/1093 Trining Loss: 0.01449027282359013 Step: 31840/35000\n",
      "Epoch 2/3 Batch 996/1093 Trining Loss: 0.014498706095399388 Step: 31872/35000\n",
      "Epoch 2/3 Batch 997/1093 Trining Loss: 0.014499546685363488 Step: 31904/35000\n",
      "Epoch 2/3 Batch 998/1093 Trining Loss: 0.01450843937351613 Step: 31936/35000\n",
      "Epoch 2/3 Batch 999/1093 Trining Loss: 0.01451322592578612 Step: 31968/35000\n",
      "Epoch 2/3 Batch 1000/1093 Trining Loss: 0.014513738459907471 Step: 32000/35000\n",
      "Epoch 2/3 Batch 1001/1093 Trining Loss: 0.014535305173983881 Step: 32032/35000\n",
      "Epoch 2/3 Batch 1002/1093 Trining Loss: 0.014539901926526112 Step: 32064/35000\n",
      "Epoch 2/3 Batch 1003/1093 Trining Loss: 0.014550028334474814 Step: 32096/35000\n",
      "Epoch 2/3 Batch 1004/1093 Trining Loss: 0.014568342657507475 Step: 32128/35000\n",
      "Epoch 2/3 Batch 1005/1093 Trining Loss: 0.014578997559697177 Step: 32160/35000\n",
      "Epoch 2/3 Batch 1006/1093 Trining Loss: 0.01459890005325021 Step: 32192/35000\n",
      "Epoch 2/3 Batch 1007/1093 Trining Loss: 0.014603108235162266 Step: 32224/35000\n",
      "Epoch 2/3 Batch 1008/1093 Trining Loss: 0.01461011033952384 Step: 32256/35000\n",
      "Epoch 2/3 Batch 1009/1093 Trining Loss: 0.014622118164359696 Step: 32288/35000\n",
      "Epoch 2/3 Batch 1010/1093 Trining Loss: 0.014626699926718922 Step: 32320/35000\n",
      "Epoch 2/3 Batch 1011/1093 Trining Loss: 0.014634462039361308 Step: 32352/35000\n",
      "Epoch 2/3 Batch 1012/1093 Trining Loss: 0.014648717492193042 Step: 32384/35000\n",
      "Epoch 2/3 Batch 1013/1093 Trining Loss: 0.014655506937982475 Step: 32416/35000\n",
      "Epoch 2/3 Batch 1014/1093 Trining Loss: 0.014662827459702184 Step: 32448/35000\n",
      "Epoch 2/3 Batch 1015/1093 Trining Loss: 0.014667711246373324 Step: 32480/35000\n",
      "Epoch 2/3 Batch 1016/1093 Trining Loss: 0.014671025574977297 Step: 32512/35000\n",
      "Epoch 2/3 Batch 1017/1093 Trining Loss: 0.014672148356446767 Step: 32544/35000\n",
      "Epoch 2/3 Batch 1018/1093 Trining Loss: 0.014680774824799922 Step: 32576/35000\n",
      "Epoch 2/3 Batch 1019/1093 Trining Loss: 0.014694893585519831 Step: 32608/35000\n",
      "Epoch 2/3 Batch 1020/1093 Trining Loss: 0.01470166517001595 Step: 32640/35000\n",
      "Epoch 2/3 Batch 1021/1093 Trining Loss: 0.014714787290937812 Step: 32672/35000\n",
      "Epoch 2/3 Batch 1022/1093 Trining Loss: 0.014716927480652432 Step: 32704/35000\n",
      "Epoch 2/3 Batch 1023/1093 Trining Loss: 0.014725117687452749 Step: 32736/35000\n",
      "Epoch 2/3 Batch 1024/1093 Trining Loss: 0.01472970757913572 Step: 32768/35000\n",
      "Epoch 2/3 Batch 1025/1093 Trining Loss: 0.014736221337282076 Step: 32800/35000\n",
      "Epoch 2/3 Batch 1026/1093 Trining Loss: 0.014736921965828997 Step: 32832/35000\n",
      "Epoch 2/3 Batch 1027/1093 Trining Loss: 0.014742041367605808 Step: 32864/35000\n",
      "Epoch 2/3 Batch 1028/1093 Trining Loss: 0.014747715522517026 Step: 32896/35000\n",
      "Epoch 2/3 Batch 1029/1093 Trining Loss: 0.01475047305465266 Step: 32928/35000\n",
      "Epoch 2/3 Batch 1030/1093 Trining Loss: 0.014753804661478232 Step: 32960/35000\n",
      "Epoch 2/3 Batch 1031/1093 Trining Loss: 0.014753875674031292 Step: 32992/35000\n",
      "Epoch 2/3 Batch 1032/1093 Trining Loss: 0.01475964127426115 Step: 33024/35000\n",
      "Epoch 2/3 Batch 1033/1093 Trining Loss: 0.014762361300433017 Step: 33056/35000\n",
      "Epoch 2/3 Batch 1034/1093 Trining Loss: 0.01476960921231333 Step: 33088/35000\n",
      "Epoch 2/3 Batch 1035/1093 Trining Loss: 0.014770202389985755 Step: 33120/35000\n",
      "Epoch 2/3 Batch 1036/1093 Trining Loss: 0.014775881524877372 Step: 33152/35000\n",
      "Epoch 2/3 Batch 1037/1093 Trining Loss: 0.014779320249842298 Step: 33184/35000\n",
      "Epoch 2/3 Batch 1038/1093 Trining Loss: 0.014788437163661843 Step: 33216/35000\n",
      "Epoch 2/3 Batch 1039/1093 Trining Loss: 0.014786686560029703 Step: 33248/35000\n",
      "Epoch 2/3 Batch 1040/1093 Trining Loss: 0.014803307716591427 Step: 33280/35000\n",
      "Epoch 2/3 Batch 1041/1093 Trining Loss: 0.014812090605010675 Step: 33312/35000\n",
      "Epoch 2/3 Batch 1042/1093 Trining Loss: 0.014810360311181955 Step: 33344/35000\n",
      "Epoch 2/3 Batch 1043/1093 Trining Loss: 0.014815517988730997 Step: 33376/35000\n",
      "Epoch 2/3 Batch 1044/1093 Trining Loss: 0.014817651244365678 Step: 33408/35000\n",
      "Epoch 2/3 Batch 1045/1093 Trining Loss: 0.014822060321613647 Step: 33440/35000\n",
      "Epoch 2/3 Batch 1046/1093 Trining Loss: 0.014819322394324649 Step: 33472/35000\n",
      "Epoch 2/3 Batch 1046/1093 Trining Loss: 0.014805168313718798\n",
      "Epoch 3/3 Batch 1047/1093 Trining Loss: 3.095015655729809e-05 Step: 33504/35000\n",
      "Epoch 3/3 Batch 1048/1093 Trining Loss: 5.71165649746438e-05 Step: 33536/35000\n",
      "Epoch 3/3 Batch 1049/1093 Trining Loss: 7.283712805624345e-05 Step: 33568/35000\n",
      "Epoch 3/3 Batch 1050/1093 Trining Loss: 8.944752315680185e-05 Step: 33600/35000\n",
      "Epoch 3/3 Batch 1051/1093 Trining Loss: 0.0001028846376898967 Step: 33632/35000\n",
      "Epoch 3/3 Batch 1052/1093 Trining Loss: 0.00012991908218839095 Step: 33664/35000\n",
      "Epoch 3/3 Batch 1053/1093 Trining Loss: 0.0001515471257078342 Step: 33696/35000\n",
      "Epoch 3/3 Batch 1054/1093 Trining Loss: 0.00016625297204564824 Step: 33728/35000\n",
      "Epoch 3/3 Batch 1055/1093 Trining Loss: 0.0001817295062951567 Step: 33760/35000\n",
      "Epoch 3/3 Batch 1056/1093 Trining Loss: 0.00019306002121249384 Step: 33792/35000\n",
      "Epoch 3/3 Batch 1057/1093 Trining Loss: 0.00020592976819685076 Step: 33824/35000\n",
      "Epoch 3/3 Batch 1058/1093 Trining Loss: 0.00021971428118970084 Step: 33856/35000\n",
      "Epoch 3/3 Batch 1059/1093 Trining Loss: 0.00024209405199692314 Step: 33888/35000\n",
      "Epoch 3/3 Batch 1060/1093 Trining Loss: 0.00025891892805273803 Step: 33920/35000\n",
      "Epoch 3/3 Batch 1061/1093 Trining Loss: 0.00027558163710774173 Step: 33952/35000\n",
      "Epoch 3/3 Batch 1062/1093 Trining Loss: 0.0002880776897353417 Step: 33984/35000\n",
      "Epoch 3/3 Batch 1063/1093 Trining Loss: 0.0003063171797082431 Step: 34016/35000\n",
      "Epoch 3/3 Batch 1064/1093 Trining Loss: 0.0003166432944091415 Step: 34048/35000\n",
      "Epoch 3/3 Batch 1065/1093 Trining Loss: 0.0003325827629633353 Step: 34080/35000\n",
      "Epoch 3/3 Batch 1066/1093 Trining Loss: 0.00034909788169679677 Step: 34112/35000\n",
      "Epoch 3/3 Batch 1067/1093 Trining Loss: 0.0003609788449587616 Step: 34144/35000\n",
      "Epoch 3/3 Batch 1068/1093 Trining Loss: 0.0003802631746224138 Step: 34176/35000\n",
      "Epoch 3/3 Batch 1069/1093 Trining Loss: 0.00040039933879900693 Step: 34208/35000\n",
      "Epoch 3/3 Batch 1070/1093 Trining Loss: 0.00040682713614306716 Step: 34240/35000\n",
      "Epoch 3/3 Batch 1071/1093 Trining Loss: 0.0004250320582454946 Step: 34272/35000\n",
      "Epoch 3/3 Batch 1072/1093 Trining Loss: 0.00044646278916816437 Step: 34304/35000\n",
      "Epoch 3/3 Batch 1073/1093 Trining Loss: 0.000460082041021551 Step: 34336/35000\n",
      "Epoch 3/3 Batch 1074/1093 Trining Loss: 0.0004771037688010565 Step: 34368/35000\n",
      "Epoch 3/3 Batch 1075/1093 Trining Loss: 0.0004923059453451356 Step: 34400/35000\n",
      "Epoch 3/3 Batch 1076/1093 Trining Loss: 0.0005056130739628272 Step: 34432/35000\n",
      "Epoch 3/3 Batch 1077/1093 Trining Loss: 0.0005242793449825895 Step: 34464/35000\n",
      "Epoch 3/3 Batch 1078/1093 Trining Loss: 0.0005412353626029058 Step: 34496/35000\n",
      "Epoch 3/3 Batch 1079/1093 Trining Loss: 0.0005505145212378184 Step: 34528/35000\n",
      "Epoch 3/3 Batch 1080/1093 Trining Loss: 0.0005655438406392932 Step: 34560/35000\n",
      "Epoch 3/3 Batch 1081/1093 Trining Loss: 0.0005803986751473578 Step: 34592/35000\n",
      "Epoch 3/3 Batch 1082/1093 Trining Loss: 0.00059405128452617 Step: 34624/35000\n",
      "Epoch 3/3 Batch 1083/1093 Trining Loss: 0.0006116083990834096 Step: 34656/35000\n",
      "Epoch 3/3 Batch 1084/1093 Trining Loss: 0.0006225392423446438 Step: 34688/35000\n",
      "Epoch 3/3 Batch 1085/1093 Trining Loss: 0.0006346766403468523 Step: 34720/35000\n",
      "Epoch 3/3 Batch 1086/1093 Trining Loss: 0.000650203472919227 Step: 34752/35000\n",
      "Epoch 3/3 Batch 1087/1093 Trining Loss: 0.0006665806967179053 Step: 34784/35000\n",
      "Epoch 3/3 Batch 1088/1093 Trining Loss: 0.0006758000595522497 Step: 34816/35000\n",
      "Epoch 3/3 Batch 1089/1093 Trining Loss: 0.0007000022898774502 Step: 34848/35000\n",
      "Epoch 3/3 Batch 1090/1093 Trining Loss: 0.0007163851154506753 Step: 34880/35000\n",
      "Epoch 3/3 Batch 1091/1093 Trining Loss: 0.0007391615124550792 Step: 34912/35000\n",
      "Epoch 3/3 Batch 1092/1093 Trining Loss: 0.0007606491995736575 Step: 34944/35000\n",
      "Epoch 3/3 Batch 1093/1093 Trining Loss: 0.0007719690225548081 Step: 34976/35000\n",
      "Epoch 3/3 Batch 1094/1093 Trining Loss: 0.0007916632350159628 Step: 35008/35000\n",
      "Epoch 3/3 Batch 1094/1093 Trining Loss: 0.0007909402548926606\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "_train(training_config)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "                   max_len=148, device=\"cpu\"):\n",
    "\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tgt = [model.bart.config.decoder_start_token_id] + ([tokenizer.pad_token_type_id] * (max_len))\n",
    "        # tgt = torch.tensor([tgt], device=device)\n",
    "        # tgt = tgt.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "        # tgt_msks = [1] + ([tokenizer.pad_token_type_id])*max_len\n",
    "        # tgt_msks = torch.tensor([tgt_msks], device=device)\n",
    "        # tgt_msks = tgt_msks.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "        strt_t = model.bart.config.decoder_start_token_id\n",
    "\n",
    "        tgt = torch.full((src_sequences.size(0),1), strt_t, device=device)    \n",
    "\n",
    "        # Generate tokens one by one\n",
    "        for i in range(1,max_len):\n",
    "            #tgt_msks[:,i] = 1\n",
    "\n",
    "            # if i > 1:\n",
    "            #     tgt = torch.full((src_sequences.size(0), i-1), tokenizer.pad_token_id, device=device)\n",
    "            #     tgt = torch.cat((tgt_start, tgt), dim=1)\n",
    "            # else:\n",
    "            #     tgt = tgt_start\n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "            tgt_msks = torch.full(tgt.shape, 0, device=device)\n",
    "\n",
    "            m = {\n",
    "                \"input_ids\": src_sequences,\n",
    "                \"attention_mask\" : src_msks,\n",
    "                \"decoder_input_ids\" : tgt,\n",
    "                \"decoder_attention_mask\" : tgt_msks,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "            out = model(m)\n",
    "\n",
    "            # Get next token prediction\n",
    "            next_token = out.argmax(dim=-1)\n",
    "            next_token = next_token[:, -1]\n",
    "            # store new prediction for active sequences\n",
    "            tgt = torch.cat((tgt, next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return tgt\n",
    "\n",
    "# def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "#                    max_len=148, device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "#     Generates predictions for a batch of source sequences using the given model.\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The model used for generating predictions.\n",
    "#         src_sequence (torch.Tensor): The source sequences to be translated.\n",
    "#         vocab (Vocabulary): The vocabulary object containing all tokens.\n",
    "#         max_len (int, optional): The maximum length of the generated sequences. Defaults to 128.\n",
    "#         device (str, optional): The device to run the model on (\"cpu\" or \"cuda\"). Defaults to \"cpu\".\n",
    "#     Returns:\n",
    "#         torch.Tensor: The generated target sequences.\n",
    "#     \"\"\"\n",
    "\n",
    "#     eos = tokenizer.convert_tokens_to_ids(['<s\\>'])[0]\n",
    "#     bos = tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Initialize target sequence with SOS token\n",
    "#         #tgt = [vocab.sos_idx] + [vocab.pad_idx] * (max_len)\n",
    "#         tgt = [bos] + ([tokenizer.pad_token_type_id] * (max_len))\n",
    "#         tgt = torch.tensor([tgt], device=device)\n",
    "#         tgt = tgt.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         tgt_msks = [1] + ([tokenizer.pad_token_type_id])*max_len\n",
    "#         tgt_msks = torch.tensor([tgt_msks], device=device)\n",
    "#         tgt_msks = tgt_msks.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         # holds indx of sequences containing EOS token\n",
    "#         finished = torch.tensor([False]*src_sequences.size(0), device=device)\n",
    "        \n",
    "#         # Generate tokens one by one\n",
    "#         for i in range(1,max_len):\n",
    "#             tgt_msks[:,i] = 1\n",
    "#             # indx of batch dim where EOS token has not been generated\n",
    "#             active_indxs = torch.where(finished == False)[0]\n",
    "#             # feed only unfinished sequences to decoder\n",
    "#             # remove padding\n",
    "#             m = {\n",
    "#                 \"input_ids\": src_sequences[active_indxs],\n",
    "#                 \"attention_mask\" : src_msks[active_indxs],\n",
    "#                 \"decoder_input_ids\" : tgt[active_indxs],\n",
    "#                 \"decoder_attention_mask\" : tgt_msks[active_indxs],\n",
    "#                 \"head_mask\" : None,\n",
    "#                 \"decoder_head_mask\" : None,\n",
    "#                 \"cross_attn_head_mask\" : None,\n",
    "#                 \"encoder_outputs\" : None,\n",
    "#                 \"past_key_values\" : None,\n",
    "#                 \"inputs_embeds\" : None,\n",
    "#                 \"decoder_inputs_embeds\" : None,\n",
    "#                 \"use_cache\" : None,\n",
    "#                 \"output_attentions\" : None,\n",
    "#                 \"output_hidden_states\" : None,\n",
    "#                 \"return_dict\" : None,\n",
    "#             }\n",
    "#             out = model(m)\n",
    "\n",
    "#             # Get next token prediction\n",
    "#             next_token = out.argmax(dim=-1)\n",
    "#             next_token = next_token[:, -1]\n",
    "#             # store new prediction for active sequences\n",
    "#             tgt[active_indxs,i] = next_token\n",
    "            \n",
    "#             # update finished sequences if any EOS token is generated\n",
    "#             new_finished = torch.where(next_token == eos, True, False)\n",
    "#             finished[active_indxs] = torch.logical_or(finished[active_indxs], new_finished)\n",
    "\n",
    "#             # early stopping if all sequences produced EOS token\n",
    "#             if finished.all():\n",
    "#                 break\n",
    "#     return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 120])\n",
      "I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s>II_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = test_dataset[2]\n",
    "inputs = inputs.repeat(2,1).to('cuda')\n",
    "inputs_msks = inputs_msks.repeat(2,1).to('cuda')\n",
    "print(inputs.shape)\n",
    "pred = predict_batch(model, inputs, inputs_msks, tokenizer, device='cuda')\n",
    "print(tokenizer.decode(targets, skip_special_tokens=True))\n",
    "tokenizer.decode(pred[0].tolist(), skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'II I_ I_ I_ I<s>_</s></s></s></s>TTURNURNURNRRRRURN<s><s>URNURNURNURNURN<s><s>URNURNURNURNURNURN__URNUMPUMPOOKURN_______URNOOK I_________OOKT_________TT__<s><s></s>___<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {\n",
    "                \"input_ids\": inputs,\n",
    "                \"attention_mask\" : inputs_msks,\n",
    "                \"decoder_input_ids\" : None,\n",
    "                \"decoder_attention_mask\" : None,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "out = model(m)\n",
    "#PRED\n",
    "tokenizer.decode(out.argmax(dim=-1)[0].tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10457])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.randint(0, len(train_dataset), (1,))\n",
    "\n",
    "rand.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   100,  1215,   565, 28267,  1215,  3850, 11615,    38,  1215,\n",
       "          574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,\n",
       "           38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850,\n",
       "        11615,    38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,\n",
       "          500,  8167,    38,  1215,   565, 28267,  1215,   500,  8167,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,   500,  8167,\n",
       "           38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,   574,\n",
       "        23775,    38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,\n",
       "          565, 28267,  1215,   500,  8167,    38,  1215,   574, 23775,     2,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0\n",
      "</s> 2\n",
      "<unk> 3\n",
      "<pad> 1\n",
      "<mask> 50264\n"
     ]
    }
   ],
   "source": [
    "special = tokenizer.all_special_tokens\n",
    "for s in special:\n",
    "    print(s, tokenizer.convert_tokens_to_ids(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"tbag/bart_1_1_5k.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1,3,2}\n",
    "sorted(a)\n",
    "a.add(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  6, 11, 17, 19, 22, 24, 25, 27, 31, 33, 34, 36, 39, 42,\n",
       "        47, 48, 49, 50, 55, 60, 65, 66, 67, 68, 69, 73, 77, 78, 79, 83, 84, 85,\n",
       "        87, 93, 95, 96, 97])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 100, (5,10))\n",
    "\n",
    "torch.sort(torch.unique(a.flatten())).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
