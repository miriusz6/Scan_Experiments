{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models for all Experiments\n",
    "##### numbers of epoches are based on analysis of k-fold results\n",
    "##### all hyperparams are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "#from bart import SimpleBart\n",
    "from experiments.experiment_type import ExperimentType\n",
    "from dataset.scan_dataset import ScanDatasetHF\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BartModel, BartForConditionalGeneration\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id = 1, decoder_start_token_id = 2):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "class SimpleBart(nn.Module):\n",
    "    def __init__(self, out_vocab_size, input_length = 120):\n",
    "        super().__init__()\n",
    "        #self.bart = AutoAdapterModel.from_pretrained('facebook/bart-base')\n",
    "        #self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.bart = BartModel.from_pretrained('facebook/bart-base')\n",
    "        #self.up = nn.Linear(768, vocab_size)\n",
    "        #self.post = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "        self.up = nn.Linear(768, out_vocab_size)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def adaptable(self):\n",
    "        return self.bart\n",
    "\n",
    "    #def forward(self, in_ids, in_mask, tgt_ids, tgt_mask):\n",
    "    def forward(self, kwargs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the model.\n",
    "        NOT autoregressive\n",
    "        Args:\n",
    "            in_ids (torch.Tensor): Input IDs for the encoder.\n",
    "            in_mask (torch.Tensor): Attention mask for the encoder input.\n",
    "            tgt_ids (torch.Tensor): Input IDs for the decoder.\n",
    "            tgt_mask (torch.Tensor): Attention mask for the decoder input.\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model after passing through the encoder and decoder.\n",
    "        \"\"\"\n",
    "\n",
    "        #x = self.bart( **kwargs).logits\n",
    "\n",
    "        x = self.bart( **kwargs).last_hidden_state\n",
    "\n",
    "        x = self.up(x)\n",
    "        #x = self.post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR, BATCH_SIZE\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "batch_size = 32\n",
    "lr = 0.001#0.0001\n",
    "w_decay = 0.00001\n",
    "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA\n",
    "e_type = ExperimentType.E_1_1\n",
    "data_paths = e_type.get_data_paths()\n",
    "\n",
    "train_dataset = ScanDatasetHF(data_paths[\"train\"], tokenizer)\n",
    "test_dataset = ScanDatasetHF(data_paths[\"test\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "\n",
    "output_vocab = torch.concatenate([train_dataset.output_vocab,test_dataset.output_vocab])\n",
    "output_vocab = torch.sort(torch.unique(output_vocab))\n",
    "_outvoc2voc = dict(zip(output_vocab.indices.numpy(),output_vocab.values.numpy()))\n",
    "_voc2outvoc = dict(zip(output_vocab.values.numpy(),output_vocab.indices.numpy()))\n",
    "output_vocab =output_vocab.values\n",
    "\n",
    "\n",
    "def outvoc2voc(in_tensor):\n",
    "    for j in range(in_tensor.shape[0]):\n",
    "    return torch.tensor([_outvoc2voc[i] for i in in_tensor.numpy()])\n",
    "def voc2outvoc(in_tensor):\n",
    "    return torch.tensor([_voc2outvoc[i] for i in in_tensor.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,     8,    71,   198,   235,   314,   356,   422,\n",
       "         1004,  1656,  2330,  2463,  2962,  3704,  5483, 10097, 10161, 13724,\n",
       "        15922, 43750])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.bias size: 384\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.bias size: 384\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "up.weight size: 16896\n",
      "up.bias size: 22\n",
      "prcnt of trainable: 0.008660926675186508\n",
      "All in millions: 140.627926\n",
      "Trainable in millions: 1.20751\n"
     ]
    }
   ],
   "source": [
    "#MODEL INITIALIZATION\n",
    "import adapters\n",
    "\n",
    "model = SimpleBart(out_vocab_size = output_vocab.size(0))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for name, param in model.bart.named_parameters():\n",
    "    if \"bart\"in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adapters.init(model.adaptable)\n",
    "\n",
    "from adapters import SeqBnInvConfig\n",
    "config = SeqBnInvConfig()\n",
    "model.adaptable.add_adapter(\"lang_adapter\", config=config)\n",
    "model.adaptable.set_active_adapters(\"lang_adapter\")\n",
    "model.adaptable.train_adapter(\"lang_adapter\")\n",
    "\n",
    "# from adapters import PrefixTuningConfig\n",
    "# config = PrefixTuningConfig(flat=False, prefix_length=30)\n",
    "# model.adaptable.add_adapter(\"prefix_tuning\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"prefix_tuning\")\n",
    "# model.adaptable.train_adapter(\"prefix_tuning\")\n",
    "\n",
    "\n",
    "\n",
    "# from adapters import IA3Config\n",
    "# config = IA3Config()\n",
    "# model.adaptable.add_adapter(\"ia3_adapter\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"ia3_adapter\")\n",
    "# model.adaptable.train_adapter(\"ia3_adapter\")\n",
    "\n",
    "#model.load_state_dict(torch.load(\"tbag/bart_1_1_5k.pth\"))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "all_cnt = 0\n",
    "trainable_cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_cnt += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_cnt += param.numel()\n",
    "        print(name, 'size:', param.numel())\n",
    "\n",
    "#model.bart.lm_head.weight.requires_grad = True\n",
    "\n",
    "print('prcnt of trainable:', trainable_cnt/(all_cnt-trainable_cnt))\n",
    "print(\"All in millions:\", all_cnt/1000000)\n",
    "print(\"Trainable in millions:\", trainable_cnt/1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from math import e\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"name\": e_type.name,\n",
    "    \"tokenizer\" : tokenizer,\n",
    "    \"model\" : model,\n",
    "    \"evaluator\" : None,\n",
    "    \"optimizer\" : None,\n",
    "    \"grad_clip\" : 5.0,\n",
    "    \"lr\" : lr,\n",
    "    \"scheduler\" : None,\n",
    "    \"criterion\" : criterion,\n",
    "    \"train_dataset\" : train_dataset,\n",
    "    \"test_dataset\" : test_dataset,\n",
    "    \"train_loader\" : train_loader,\n",
    "    \"test_loader\" : test_loader,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"max_steps\" : 35 * 1000,#0,#//batch_size,\n",
    "    \"max_epochs\" : None,\n",
    "    \"evaluation_interval\" : 50,\n",
    "    \"model_save_interval\" : 50,\n",
    "    \"detailed_logging\" : True,\n",
    "    \"use_tensorboard\" : False,\n",
    "    \"tensorboard_dir\" : None,\n",
    "    \"model_save_dir\" : None,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "from os import name\n",
    "from turtle import mode\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from math import ceil\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from dataset import vocab\n",
    "\n",
    "def _train(config):\n",
    "    name = config[\"name\"]\n",
    "    tokenizer = config[\"tokenizer\"]\n",
    "    model = config[\"model\"]\n",
    "    evaluator = config[\"evaluator\"]\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    scheduler = config[\"scheduler\"]\n",
    "    grad_clip = config[\"grad_clip\"]\n",
    "    lr = config[\"lr\"]\n",
    "    criterion = config[\"criterion\"]\n",
    "    train_dataset = config[\"train_dataset\"]\n",
    "    test_dataset = config[\"test_dataset\"]\n",
    "    train_loader = config[\"train_loader\"]\n",
    "    test_loader = config[\"test_loader\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    max_steps = config[\"max_steps\"]\n",
    "    epochs = config[\"max_epochs\"]\n",
    "    evaluation_interval = config[\"evaluation_interval\"]\n",
    "    model_save_interval = config[\"model_save_interval\"]\n",
    "    detailed_logging = config[\"detailed_logging\"]\n",
    "    use_tensorboard = config[\"use_tensorboard\"]\n",
    "    tensorboard_dir = config[\"tensorboard_dir\"]\n",
    "    model_save_dir = config[\"model_save_dir\"]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=w_decay,\n",
    "        )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "    r_indx = name.find(\"_rep\")\n",
    "    org_name_base,org_rep_info = name[:r_indx], name[r_indx:]\n",
    "\n",
    "\n",
    "    name = org_name_base + org_rep_info\n",
    "\n",
    "    if use_tensorboard:\n",
    "        writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    # if model_weights_path is not None:\n",
    "    #     _load_weights()\n",
    "\n",
    "    grad_clip = grad_clip\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    max_epoch = epochs if epochs is not None else ceil(max_steps/len(train_dataset))\n",
    "    max_steps = max_steps if max_steps is not None else (max_epoch*len(train_loader)*batch_size)+1\n",
    "    max_batches = max_steps//batch_size\n",
    "    batch_num = 0\n",
    "    step_num = 0\n",
    "    batch_step = 0\n",
    "\n",
    "    eval_interval = evaluation_interval\n",
    "    model_sv_interval = model_save_interval\n",
    "\n",
    "    \n",
    "\n",
    "    if detailed_logging:\n",
    "        print(\"Training started for experiment: \", name)\n",
    "\n",
    "    \n",
    "    t ,_,_,_,_,_ = train_dataset[0]\n",
    "    \n",
    "    tgt_empty = torch.full((batch_size,t.shape[0]), tokenizer.pad_token_id, device=device)\n",
    "    tgt_empty_msks = torch.full(tgt_empty.shape, 0, device=device)\n",
    "    tgt_empty_msks[:,0] = 1\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    epoch_progress = tqdm(range(max_epoch), desc=\"EPOCH\")\n",
    "    for epoch in epoch_progress:\n",
    "        if max_steps is not None and step_num >= max_steps:\n",
    "                print(\"Early Stopping: Max steps reached\")\n",
    "                break\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        #batch_bar = tqdm(train_loader, desc=\"BATCH\")\n",
    "        #for batch in batch_bar:\n",
    "        batch_step = 0\n",
    "        for batch in train_loader:\n",
    "            if max_steps is not None and step_num >= max_steps:\n",
    "                break\n",
    "\n",
    "            inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            inputs_msks = inputs_msks.to(device)\n",
    "            dec_in = dec_in.to(device)\n",
    "            dec_in_msks = dec_in_msks.to(device)\n",
    "            dec_in_msks[:,0] = 1\n",
    "            targets = targets.to(device)\n",
    "            targets_msks = targets_msks.to(device)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            #out = model(inputs,inputs_msks,dec_in, dec_in_msks)\n",
    "\n",
    "            if True: #batch_num%2: #step_num < max_steps//2:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(dec_in), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : dec_in_msks, #WRONG!!!!\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            else:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(tgt_empty), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : tgt_empty_msks, # None, #targets_msks,\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            out = model(m)\n",
    "\n",
    "            targets_cmpr = voc2outvoc(targets)\n",
    "\n",
    "            loss = criterion(out.permute(0, 2, 1), targets_cmpr)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_num += 1\n",
    "            batch_step += 1\n",
    "            step_num += batch_size\n",
    "\n",
    "            if detailed_logging or batch_num == len(train_loader):\n",
    "                print(f\"Epoch {epoch+1}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num)} Step: {step_num}/{max_steps}\" )\n",
    "\n",
    "            \n",
    "\n",
    "        epoch += 1\n",
    "        if detailed_logging or epoch == max_epoch:\n",
    "            print(f\"Epoch {epoch}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num + 1)}\")\n",
    "\n",
    "        # EVALUATION \n",
    "        if epoch % eval_interval == 0 or epoch == max_epoch or max_steps is not None and step_num >= max_steps:\n",
    "            pass\n",
    "            # model.eval()\n",
    "            # with torch.no_grad():\n",
    "            #     total_loss = 0\n",
    "            #     for batch in test_loader:\n",
    "            #         inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            #         inputs = inputs.to(device)\n",
    "            #         inputs_msks = inputs_msks.to(device)\n",
    "            #         targets = targets.to(device)\n",
    "            #         targets_msks = targets_msks.to(device)\n",
    "            #         dec_in = dec_in.to(device)\n",
    "            #         dec_in_msks = dec_in_msks.to(device)\n",
    "\n",
    "            #         #out = model(inputs,inputs_msks,None, None)\n",
    "            #         m = {\n",
    "            #             \"input_ids\": inputs,\n",
    "            #             \"attention_mask\" : inputs_msks,\n",
    "            #             \"decoder_input_ids\" : None,\n",
    "            #             \"decoder_attention_mask\" : None,\n",
    "            #             \"head_mask\" : None,\n",
    "            #             \"decoder_head_mask\" : None,\n",
    "            #             \"cross_attn_head_mask\" : None,\n",
    "            #             \"encoder_outputs\" : None,\n",
    "            #             \"past_key_values\" : None,\n",
    "            #             \"inputs_embeds\" : None,\n",
    "            #             \"decoder_inputs_embeds\" : None,\n",
    "            #             \"use_cache\" : None,\n",
    "            #             \"output_attentions\" : None,\n",
    "            #             \"output_hidden_states\" : None,\n",
    "            #             \"return_dict\" : None,\n",
    "            #         }\n",
    "            #         out = model(m)\n",
    "            #         loss = criterion(out.permute(0, 2, 1), targets)\n",
    "            #         total_loss += loss.item()\n",
    "            #     print(f\"Epoch {epoch}/{max_epoch} Validation Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "        #     result = evaluate_model_batchwise(model,test_dataset, test_loader, test_dataset.vocab, device)\n",
    "        #     #result = EvaluationResult(result, name+fold_info+f\"_epoch_{epoch}\", e_type)\n",
    "        #     result = EvaluationResult(result, name+f\"_epoch_{epoch}\", e_type)\n",
    "        #     if detailed_logging or epoch == max_epoch:\n",
    "        #         result.print()\n",
    "        #     result_container.append_results(result)\n",
    "        \n",
    "        if epoch % model_sv_interval == 0 or epoch == max_epoch and not model_save_dir is None:\n",
    "            buff = name\n",
    "            name = org_name_base + f\"_epoch_{epoch}\" + org_rep_info\n",
    "            torch.save(model.state_dict(),model_save_dir+'/'+name)\n",
    "            name = buff\n",
    "\n",
    "\n",
    "        if use_tensorboard:\n",
    "            writer.add_scalar(tag = 'TrainLoss',\n",
    "                                scalar_value = total_loss,\n",
    "                                global_step = epoch)\n",
    "        \n",
    "    if use_tensorboard:\n",
    "        writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for experiment:  E_1_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15016065d13f4319b5143e7db3d11293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCH:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 149\u001b[0m, in \u001b[0;36m_train\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    130\u001b[0m     m \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs,\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m : inputs_msks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m     }\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m model(m)\n\u001b[1;32m--> 149\u001b[0m targets_cmpr \u001b[38;5;241m=\u001b[39m \u001b[43mvoc2outvoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), targets_cmpr)\n\u001b[0;32m    152\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36mvoc2outvoc\u001b[1;34m(in_tensor)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoc2outvoc\u001b[39m(in_tensor):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([_voc2outvoc[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m in_tensor\u001b[38;5;241m.\u001b[39mnumpy()])\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoc2outvoc\u001b[39m(in_tensor):\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43m_voc2outvoc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m in_tensor\u001b[38;5;241m.\u001b[39mnumpy()])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "_train(training_config)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "                   max_len=148, device=\"cpu\"):\n",
    "\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # tgt = [model.bart.config.decoder_start_token_id] + ([tokenizer.pad_token_type_id] * (max_len))\n",
    "        # tgt = torch.tensor([tgt], device=device)\n",
    "        # tgt = tgt.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "        # tgt_msks = [1] + ([tokenizer.pad_token_type_id])*max_len\n",
    "        # tgt_msks = torch.tensor([tgt_msks], device=device)\n",
    "        # tgt_msks = tgt_msks.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "        strt_t = model.bart.config.decoder_start_token_id\n",
    "\n",
    "        tgt = torch.full((src_sequences.size(0),1), strt_t, device=device)    \n",
    "\n",
    "        # Generate tokens one by one\n",
    "        for i in range(1,max_len):\n",
    "            #tgt_msks[:,i] = 1\n",
    "\n",
    "            # if i > 1:\n",
    "            #     tgt = torch.full((src_sequences.size(0), i-1), tokenizer.pad_token_id, device=device)\n",
    "            #     tgt = torch.cat((tgt_start, tgt), dim=1)\n",
    "            # else:\n",
    "            #     tgt = tgt_start\n",
    "                \n",
    "                \n",
    "\n",
    "            \n",
    "            tgt_msks = torch.full(tgt.shape, 0, device=device)\n",
    "\n",
    "            m = {\n",
    "                \"input_ids\": src_sequences,\n",
    "                \"attention_mask\" : src_msks,\n",
    "                \"decoder_input_ids\" : tgt,\n",
    "                \"decoder_attention_mask\" : tgt_msks,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "            out = model(m)\n",
    "\n",
    "            # Get next token prediction\n",
    "            next_token = out.argmax(dim=-1)\n",
    "            next_token = next_token[:, -1]\n",
    "            # store new prediction for active sequences\n",
    "            tgt = torch.cat((tgt, next_token.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return tgt\n",
    "\n",
    "# def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "#                    max_len=148, device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "#     Generates predictions for a batch of source sequences using the given model.\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The model used for generating predictions.\n",
    "#         src_sequence (torch.Tensor): The source sequences to be translated.\n",
    "#         vocab (Vocabulary): The vocabulary object containing all tokens.\n",
    "#         max_len (int, optional): The maximum length of the generated sequences. Defaults to 128.\n",
    "#         device (str, optional): The device to run the model on (\"cpu\" or \"cuda\"). Defaults to \"cpu\".\n",
    "#     Returns:\n",
    "#         torch.Tensor: The generated target sequences.\n",
    "#     \"\"\"\n",
    "\n",
    "#     eos = tokenizer.convert_tokens_to_ids(['<s\\>'])[0]\n",
    "#     bos = tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Initialize target sequence with SOS token\n",
    "#         #tgt = [vocab.sos_idx] + [vocab.pad_idx] * (max_len)\n",
    "#         tgt = [bos] + ([tokenizer.pad_token_type_id] * (max_len))\n",
    "#         tgt = torch.tensor([tgt], device=device)\n",
    "#         tgt = tgt.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         tgt_msks = [1] + ([tokenizer.pad_token_type_id])*max_len\n",
    "#         tgt_msks = torch.tensor([tgt_msks], device=device)\n",
    "#         tgt_msks = tgt_msks.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         # holds indx of sequences containing EOS token\n",
    "#         finished = torch.tensor([False]*src_sequences.size(0), device=device)\n",
    "        \n",
    "#         # Generate tokens one by one\n",
    "#         for i in range(1,max_len):\n",
    "#             tgt_msks[:,i] = 1\n",
    "#             # indx of batch dim where EOS token has not been generated\n",
    "#             active_indxs = torch.where(finished == False)[0]\n",
    "#             # feed only unfinished sequences to decoder\n",
    "#             # remove padding\n",
    "#             m = {\n",
    "#                 \"input_ids\": src_sequences[active_indxs],\n",
    "#                 \"attention_mask\" : src_msks[active_indxs],\n",
    "#                 \"decoder_input_ids\" : tgt[active_indxs],\n",
    "#                 \"decoder_attention_mask\" : tgt_msks[active_indxs],\n",
    "#                 \"head_mask\" : None,\n",
    "#                 \"decoder_head_mask\" : None,\n",
    "#                 \"cross_attn_head_mask\" : None,\n",
    "#                 \"encoder_outputs\" : None,\n",
    "#                 \"past_key_values\" : None,\n",
    "#                 \"inputs_embeds\" : None,\n",
    "#                 \"decoder_inputs_embeds\" : None,\n",
    "#                 \"use_cache\" : None,\n",
    "#                 \"output_attentions\" : None,\n",
    "#                 \"output_hidden_states\" : None,\n",
    "#                 \"return_dict\" : None,\n",
    "#             }\n",
    "#             out = model(m)\n",
    "\n",
    "#             # Get next token prediction\n",
    "#             next_token = out.argmax(dim=-1)\n",
    "#             next_token = next_token[:, -1]\n",
    "#             # store new prediction for active sequences\n",
    "#             tgt[active_indxs,i] = next_token\n",
    "            \n",
    "#             # update finished sequences if any EOS token is generated\n",
    "#             new_finished = torch.where(next_token == eos, True, False)\n",
    "#             finished[active_indxs] = torch.logical_or(finished[active_indxs], new_finished)\n",
    "\n",
    "#             # early stopping if all sequences produced EOS token\n",
    "#             if finished.all():\n",
    "#                 break\n",
    "#     return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 120])\n",
      "I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s>II_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = test_dataset[2]\n",
    "inputs = inputs.repeat(2,1).to(device)\n",
    "inputs_msks = inputs_msks.repeat(2,1).to(device)\n",
    "print(inputs.shape)\n",
    "pred = predict_batch(model, inputs, inputs_msks, tokenizer, device=device)\n",
    "print(tokenizer.decode(targets, skip_special_tokens=True))\n",
    "tokenizer.decode(pred[0].tolist(), skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'II I_ I_ I_ I<s>_</s></s></s></s>TTURNURNURNRRRRURN<s><s>URNURNURNURNURN<s><s>URNURNURNURNURNURN__URNUMPUMPOOKURN_______URNOOK I_________OOKT_________TT__<s><s></s>___<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {\n",
    "                \"input_ids\": inputs,\n",
    "                \"attention_mask\" : inputs_msks,\n",
    "                \"decoder_input_ids\" : None,\n",
    "                \"decoder_attention_mask\" : None,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "out = model(m)\n",
    "#PRED\n",
    "tokenizer.decode(out.argmax(dim=-1)[0].tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10457])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.randint(0, len(train_dataset), (1,))\n",
    "\n",
    "rand.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   100,  1215,   565, 28267,  1215,  3850, 11615,    38,  1215,\n",
       "          574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,\n",
       "           38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850,\n",
       "        11615,    38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,\n",
       "          500,  8167,    38,  1215,   565, 28267,  1215,   500,  8167,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,   500,  8167,\n",
       "           38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,   574,\n",
       "        23775,    38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,\n",
       "          565, 28267,  1215,   500,  8167,    38,  1215,   574, 23775,     2,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0\n",
      "</s> 2\n",
      "<unk> 3\n",
      "<pad> 1\n",
      "<mask> 50264\n"
     ]
    }
   ],
   "source": [
    "special = tokenizer.all_special_tokens\n",
    "for s in special:\n",
    "    print(s, tokenizer.convert_tokens_to_ids(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"tbag/bart_1_1_5k.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1,3,2}\n",
    "sorted(a)\n",
    "a.add(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  6,  8, 10, 13, 15, 16, 19, 21, 24, 27, 29, 30, 32, 33,\n",
       "        34, 36, 37, 42, 47, 52, 53, 54, 55, 57, 61, 63, 66, 68, 76, 77, 78, 81,\n",
       "        84, 89, 90, 93])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 100, (5,10))\n",
    "\n",
    "a = torch.sort(torch.unique(a.flatten())).values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.full((5,100), 1)\n",
    "\n",
    "#a = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "a.shape\n",
    "\n",
    "# b.shape\n",
    "#b[:,a].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 8,\n",
       " 4: 71,\n",
       " 5: 198,\n",
       " 6: 235,\n",
       " 7: 314,\n",
       " 8: 356,\n",
       " 9: 422,\n",
       " 10: 1004,\n",
       " 11: 1656,\n",
       " 12: 2330,\n",
       " 13: 2463,\n",
       " 14: 2962,\n",
       " 15: 3704,\n",
       " 16: 5483,\n",
       " 17: 10097,\n",
       " 18: 10161,\n",
       " 19: 13724,\n",
       " 20: 15922,\n",
       " 21: 43750}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab = torch.concatenate([train_dataset.output_vocab,test_dataset.output_vocab])\n",
    "output_vocab = torch.sort(torch.unique(output_vocab))\n",
    "output_vocab\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
