{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models for all Experiments\n",
    "##### numbers of epoches are based on analysis of k-fold results\n",
    "##### all hyperparams are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "#from bart import SimpleBart\n",
    "from experiments.experiment_type import ExperimentType\n",
    "from dataset.scan_dataset import ScanDatasetHF\n",
    "\n",
    "device = 'cuda'#'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BartModel, BartForConditionalGeneration\n",
    "from adapters import AutoAdapterModel\n",
    "\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id = 1, decoder_start_token_id = 2):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "class SimpleBart(nn.Module):\n",
    "    def __init__(self, out_vocab_size, input_length = 120):\n",
    "        super().__init__()\n",
    "        #self.bart = AutoAdapterModel.from_pretrained('facebook/bart-base')\n",
    "        #self.bart = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "        self.bart = BartModel.from_pretrained('facebook/bart-base')\n",
    "        #self.up = nn.Linear(768, vocab_size)\n",
    "        #self.post = nn.Linear(vocab_size, vocab_size)\n",
    "\n",
    "        self.up = nn.Linear(768, out_vocab_size, device=device)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def adaptable(self):\n",
    "        return self.bart\n",
    "\n",
    "    #def forward(self, in_ids, in_mask, tgt_ids, tgt_mask):\n",
    "    def forward(self, kwargs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the model.\n",
    "        NOT autoregressive\n",
    "        Args:\n",
    "            in_ids (torch.Tensor): Input IDs for the encoder.\n",
    "            in_mask (torch.Tensor): Attention mask for the encoder input.\n",
    "            tgt_ids (torch.Tensor): Input IDs for the decoder.\n",
    "            tgt_mask (torch.Tensor): Attention mask for the decoder input.\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model after passing through the encoder and decoder.\n",
    "        \"\"\"\n",
    "\n",
    "        #x = self.bart( **kwargs).logits\n",
    "\n",
    "        x = self.bart( **kwargs).last_hidden_state\n",
    "\n",
    "        x = self.up(x)\n",
    "        #x = self.post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR, BATCH_SIZE\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "batch_size = 32\n",
    "lr = 0.001#0.0001\n",
    "w_decay = 0.00001\n",
    "criterion = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA\n",
    "e_type = ExperimentType.E_1_1\n",
    "data_paths = e_type.get_data_paths()\n",
    "\n",
    "train_dataset = ScanDatasetHF(data_paths[\"train\"], tokenizer)\n",
    "test_dataset = ScanDatasetHF(data_paths[\"test\"], tokenizer)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True\n",
    "                          )\n",
    "\n",
    "output_vocab = torch.concatenate([train_dataset.output_vocab,test_dataset.output_vocab])\n",
    "output_vocab = torch.sort(torch.unique(output_vocab))\n",
    "_outvoc2voc = dict(zip(output_vocab.indices.numpy(),output_vocab.values.numpy()))\n",
    "_voc2outvoc = dict(zip(output_vocab.values.numpy(),output_vocab.indices.numpy()))\n",
    "output_vocab =output_vocab.values\n",
    "\n",
    "\n",
    "def outvoc2voc(in_tensor):\n",
    "    out_tensor = torch.zeros(in_tensor.shape, dtype=torch.int64, device=device)    \n",
    "    for j in range(in_tensor.shape[0]):\n",
    "        out_tensor[j] =  torch.tensor([_outvoc2voc[i] for i in in_tensor[j].cpu().numpy()], dtype=torch.int64, device=device)\n",
    "    return out_tensor\n",
    "def voc2outvoc(in_tensor):\n",
    "    out_tensor = torch.zeros(in_tensor.shape, dtype=torch.int64, device=device)  \n",
    "    #print(in_tensor)\n",
    "    for j in range(in_tensor.shape[0]):\n",
    "        out_tensor[j] = torch.tensor([_voc2outvoc[i] for i in in_tensor[j].cpu().numpy()], dtype=torch.int64, device=device)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.encoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.F.2.bias size: 384\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.0.bias size: 192\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.weight size: 73728\n",
      "bart.encoder.invertible_adapters.lang_adapter.G.2.bias size: 384\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.0.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.1.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.2.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.3.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.4.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_down.0.bias size: 48\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.weight size: 36864\n",
      "bart.decoder.layers.5.output_adapters.adapters.lang_adapter.adapter_up.bias size: 768\n",
      "up.weight size: 14592\n",
      "up.bias size: 19\n",
      "prcnt of trainable: 0.00864437960076091\n",
      "All in millions: 140.625619\n",
      "Trainable in millions: 1.205203\n"
     ]
    }
   ],
   "source": [
    "#MODEL INITIALIZATION\n",
    "import adapters\n",
    "\n",
    "model = SimpleBart(out_vocab_size = output_vocab.size(0))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for name, param in model.bart.named_parameters():\n",
    "    if \"bart\"in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adapters.init(model.adaptable)\n",
    "\n",
    "from adapters import SeqBnInvConfig\n",
    "config = SeqBnInvConfig()\n",
    "model.adaptable.add_adapter(\"lang_adapter\", config=config)\n",
    "model.adaptable.set_active_adapters(\"lang_adapter\")\n",
    "model.adaptable.train_adapter(\"lang_adapter\")\n",
    "\n",
    "# from adapters import PrefixTuningConfig\n",
    "# config = PrefixTuningConfig(flat=False, prefix_length=30)\n",
    "# model.adaptable.add_adapter(\"prefix_tuning\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"prefix_tuning\")\n",
    "# model.adaptable.train_adapter(\"prefix_tuning\")\n",
    "\n",
    "\n",
    "\n",
    "# from adapters import IA3Config\n",
    "# config = IA3Config()\n",
    "# model.adaptable.add_adapter(\"ia3_adapter\", config=config)\n",
    "# model.adaptable.set_active_adapters(\"ia3_adapter\")\n",
    "# model.adaptable.train_adapter(\"ia3_adapter\")\n",
    "\n",
    "#model.load_state_dict(torch.load(\"tbag/bart_1_1_5k.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "all_cnt = 0\n",
    "trainable_cnt = 0\n",
    "for name, param in model.named_parameters():\n",
    "    all_cnt += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_cnt += param.numel()\n",
    "        print(name, 'size:', param.numel())\n",
    "\n",
    "#model.bart.lm_head.weight.requires_grad = True\n",
    "\n",
    "print('prcnt of trainable:', trainable_cnt/(all_cnt-trainable_cnt))\n",
    "print(\"All in millions:\", all_cnt/1000000)\n",
    "print(\"Trainable in millions:\", trainable_cnt/1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "from math import e\n",
    "\n",
    "\n",
    "training_config = {\n",
    "    \"name\": e_type.name,\n",
    "    \"tokenizer\" : tokenizer,\n",
    "    \"model\" : model,\n",
    "    \"evaluator\" : None,\n",
    "    \"optimizer\" : None,\n",
    "    \"grad_clip\" : 5.0,\n",
    "    \"lr\" : lr,\n",
    "    \"scheduler\" : None,\n",
    "    \"criterion\" : criterion,\n",
    "    \"train_dataset\" : train_dataset,\n",
    "    \"test_dataset\" : test_dataset,\n",
    "    \"train_loader\" : train_loader,\n",
    "    \"test_loader\" : test_loader,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"max_steps\" : 10 * 1000,#0,#//batch_size,\n",
    "    \"max_epochs\" : None,\n",
    "    \"evaluation_interval\" : 50,\n",
    "    \"model_save_interval\" : 50,\n",
    "    \"detailed_logging\" : True,\n",
    "    \"use_tensorboard\" : False,\n",
    "    \"tensorboard_dir\" : None,\n",
    "    \"model_save_dir\" : None,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "from os import name\n",
    "from turtle import mode\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from math import ceil\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from dataset import vocab\n",
    "\n",
    "def _train(config):\n",
    "    name = config[\"name\"]\n",
    "    tokenizer = config[\"tokenizer\"]\n",
    "    model = config[\"model\"]\n",
    "    evaluator = config[\"evaluator\"]\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    scheduler = config[\"scheduler\"]\n",
    "    grad_clip = config[\"grad_clip\"]\n",
    "    lr = config[\"lr\"]\n",
    "    criterion = config[\"criterion\"]\n",
    "    train_dataset = config[\"train_dataset\"]\n",
    "    test_dataset = config[\"test_dataset\"]\n",
    "    train_loader = config[\"train_loader\"]\n",
    "    test_loader = config[\"test_loader\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    max_steps = config[\"max_steps\"]\n",
    "    epochs = config[\"max_epochs\"]\n",
    "    evaluation_interval = config[\"evaluation_interval\"]\n",
    "    model_save_interval = config[\"model_save_interval\"]\n",
    "    detailed_logging = config[\"detailed_logging\"]\n",
    "    use_tensorboard = config[\"use_tensorboard\"]\n",
    "    tensorboard_dir = config[\"tensorboard_dir\"]\n",
    "    model_save_dir = config[\"model_save_dir\"]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=w_decay,\n",
    "        )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10000, gamma=0.1)\n",
    "\n",
    "    r_indx = name.find(\"_rep\")\n",
    "    org_name_base,org_rep_info = name[:r_indx], name[r_indx:]\n",
    "\n",
    "\n",
    "    name = org_name_base + org_rep_info\n",
    "\n",
    "    if use_tensorboard:\n",
    "        writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    # if model_weights_path is not None:\n",
    "    #     _load_weights()\n",
    "\n",
    "    grad_clip = grad_clip\n",
    "    batch_size = batch_size\n",
    "    \n",
    "    max_epoch = epochs if epochs is not None else ceil(max_steps/len(train_dataset))\n",
    "    max_steps = max_steps if max_steps is not None else (max_epoch*len(train_loader)*batch_size)+1\n",
    "    max_batches = max_steps//batch_size\n",
    "    batch_num = 0\n",
    "    step_num = 0\n",
    "    batch_step = 0\n",
    "\n",
    "    eval_interval = evaluation_interval\n",
    "    model_sv_interval = model_save_interval\n",
    "\n",
    "    \n",
    "\n",
    "    if detailed_logging:\n",
    "        print(\"Training started for experiment: \", name)\n",
    "\n",
    "    \n",
    "    t ,_,_,_,_,_ = train_dataset[0]\n",
    "    \n",
    "    tgt_empty = torch.full((batch_size,t.shape[0]), tokenizer.pad_token_id, device=device)\n",
    "    tgt_empty_msks = torch.full(tgt_empty.shape, 0, device=device)\n",
    "    tgt_empty_msks[:,0] = 1\n",
    "\n",
    "    # TRAINING LOOP\n",
    "    epoch_progress = tqdm(range(max_epoch), desc=\"EPOCH\")\n",
    "    for epoch in epoch_progress:\n",
    "        if max_steps is not None and step_num >= max_steps:\n",
    "                print(\"Early Stopping: Max steps reached\")\n",
    "                break\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        #batch_bar = tqdm(train_loader, desc=\"BATCH\")\n",
    "        #for batch in batch_bar:\n",
    "        batch_step = 0\n",
    "        for batch in train_loader:\n",
    "            if max_steps is not None and step_num >= max_steps:\n",
    "                break\n",
    "\n",
    "            inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            inputs_msks = inputs_msks.to(device)\n",
    "            dec_in = dec_in.to(device)\n",
    "            dec_in_msks = dec_in_msks.to(device)\n",
    "            dec_in_msks[:,0] = 1\n",
    "            targets = targets.to(device)\n",
    "            targets_msks = targets_msks.to(device)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            #out = model(inputs,inputs_msks,dec_in, dec_in_msks)\n",
    "\n",
    "            if True: #batch_num%2: #step_num < max_steps//2:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(voc2outvoc(dec_in)), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : shift_tokens_right(dec_in_msks), #WRONG!!!!\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            else:\n",
    "                m = {\n",
    "                    \"input_ids\": inputs,\n",
    "                    \"attention_mask\" : inputs_msks,\n",
    "                    \"decoder_input_ids\" : shift_tokens_right(tgt_empty), #None, #targets,\n",
    "                    \"decoder_attention_mask\" : tgt_empty_msks, # None, #targets_msks,\n",
    "                    \"head_mask\" : None,\n",
    "                    \"decoder_head_mask\" : None,\n",
    "                    \"cross_attn_head_mask\" : None,\n",
    "                    \"encoder_outputs\" : None,\n",
    "                    \"past_key_values\" : None,\n",
    "                    \"inputs_embeds\" : None,\n",
    "                    \"decoder_inputs_embeds\" : None,\n",
    "                    \"use_cache\" : None,\n",
    "                    \"output_attentions\" : None,\n",
    "                    \"output_hidden_states\" : None,\n",
    "                    \"return_dict\" : None,\n",
    "                }\n",
    "            out = model(m)\n",
    "\n",
    "            targets_cmpr = voc2outvoc(targets)\n",
    "\n",
    "            loss = criterion(out.permute(0, 2, 1), targets_cmpr)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_num += 1\n",
    "            batch_step += 1\n",
    "            step_num += batch_size\n",
    "\n",
    "            if detailed_logging or batch_num == len(train_loader):\n",
    "                print(f\"Epoch {epoch+1}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num)} Step: {step_num}/{max_steps}\" )\n",
    "\n",
    "            \n",
    "\n",
    "        epoch += 1\n",
    "        if detailed_logging or epoch == max_epoch:\n",
    "            print(f\"Epoch {epoch}/{max_epoch} Batch {batch_num}/{max_batches} Trining Loss: {total_loss / (batch_num + 1)}\")\n",
    "\n",
    "        # EVALUATION \n",
    "        if epoch % eval_interval == 0 or epoch == max_epoch or max_steps is not None and step_num >= max_steps:\n",
    "            pass\n",
    "            # model.eval()\n",
    "            # with torch.no_grad():\n",
    "            #     total_loss = 0\n",
    "            #     for batch in test_loader:\n",
    "            #         inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = batch\n",
    "            #         inputs = inputs.to(device)\n",
    "            #         inputs_msks = inputs_msks.to(device)\n",
    "            #         targets = targets.to(device)\n",
    "            #         targets_msks = targets_msks.to(device)\n",
    "            #         dec_in = dec_in.to(device)\n",
    "            #         dec_in_msks = dec_in_msks.to(device)\n",
    "\n",
    "            #         #out = model(inputs,inputs_msks,None, None)\n",
    "            #         m = {\n",
    "            #             \"input_ids\": inputs,\n",
    "            #             \"attention_mask\" : inputs_msks,\n",
    "            #             \"decoder_input_ids\" : None,\n",
    "            #             \"decoder_attention_mask\" : None,\n",
    "            #             \"head_mask\" : None,\n",
    "            #             \"decoder_head_mask\" : None,\n",
    "            #             \"cross_attn_head_mask\" : None,\n",
    "            #             \"encoder_outputs\" : None,\n",
    "            #             \"past_key_values\" : None,\n",
    "            #             \"inputs_embeds\" : None,\n",
    "            #             \"decoder_inputs_embeds\" : None,\n",
    "            #             \"use_cache\" : None,\n",
    "            #             \"output_attentions\" : None,\n",
    "            #             \"output_hidden_states\" : None,\n",
    "            #             \"return_dict\" : None,\n",
    "            #         }\n",
    "            #         out = model(m)\n",
    "            #         loss = criterion(out.permute(0, 2, 1), targets)\n",
    "            #         total_loss += loss.item()\n",
    "            #     print(f\"Epoch {epoch}/{max_epoch} Validation Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "        #     result = evaluate_model_batchwise(model,test_dataset, test_loader, test_dataset.vocab, device)\n",
    "        #     #result = EvaluationResult(result, name+fold_info+f\"_epoch_{epoch}\", e_type)\n",
    "        #     result = EvaluationResult(result, name+f\"_epoch_{epoch}\", e_type)\n",
    "        #     if detailed_logging or epoch == max_epoch:\n",
    "        #         result.print()\n",
    "        #     result_container.append_results(result)\n",
    "        \n",
    "        if epoch % model_sv_interval == 0 or epoch == max_epoch and not model_save_dir is None:\n",
    "            buff = name\n",
    "            name = org_name_base + f\"_epoch_{epoch}\" + org_rep_info\n",
    "            torch.save(model.state_dict(),model_save_dir+'/'+name)\n",
    "            name = buff\n",
    "\n",
    "\n",
    "        if use_tensorboard:\n",
    "            writer.add_scalar(tag = 'TrainLoss',\n",
    "                                scalar_value = total_loss,\n",
    "                                global_step = epoch)\n",
    "        }\n",
    "    if use_tensorboard:\n",
    "        writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for experiment:  E_1_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d7e55ca6a940c5be07243e54104630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EPOCH:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Batch 1/312 Trining Loss: 3.4110898971557617 Step: 32/10000\n",
      "Epoch 1/1 Batch 2/312 Trining Loss: 3.123060464859009 Step: 64/10000\n",
      "Epoch 1/1 Batch 3/312 Trining Loss: 3.0406734148661294 Step: 96/10000\n",
      "Epoch 1/1 Batch 4/312 Trining Loss: 2.942711591720581 Step: 128/10000\n",
      "Epoch 1/1 Batch 5/312 Trining Loss: 2.8641037940979004 Step: 160/10000\n",
      "Epoch 1/1 Batch 6/312 Trining Loss: 2.7966291507085166 Step: 192/10000\n",
      "Epoch 1/1 Batch 7/312 Trining Loss: 2.7455385412488664 Step: 224/10000\n",
      "Epoch 1/1 Batch 8/312 Trining Loss: 2.7055512964725494 Step: 256/10000\n",
      "Epoch 1/1 Batch 9/312 Trining Loss: 2.6761597792307534 Step: 288/10000\n",
      "Epoch 1/1 Batch 10/312 Trining Loss: 2.6482025623321532 Step: 320/10000\n",
      "Epoch 1/1 Batch 11/312 Trining Loss: 2.6202081116763027 Step: 352/10000\n",
      "Epoch 1/1 Batch 12/312 Trining Loss: 2.593984007835388 Step: 384/10000\n",
      "Epoch 1/1 Batch 13/312 Trining Loss: 2.568935430966891 Step: 416/10000\n",
      "Epoch 1/1 Batch 14/312 Trining Loss: 2.540286728313991 Step: 448/10000\n",
      "Epoch 1/1 Batch 15/312 Trining Loss: 2.511341667175293 Step: 480/10000\n",
      "Epoch 1/1 Batch 16/312 Trining Loss: 2.4800631254911423 Step: 512/10000\n",
      "Epoch 1/1 Batch 17/312 Trining Loss: 2.4480085863786587 Step: 544/10000\n",
      "Epoch 1/1 Batch 18/312 Trining Loss: 2.4124989377127752 Step: 576/10000\n",
      "Epoch 1/1 Batch 19/312 Trining Loss: 2.3751670059404875 Step: 608/10000\n",
      "Epoch 1/1 Batch 20/312 Trining Loss: 2.3345030307769776 Step: 640/10000\n",
      "Epoch 1/1 Batch 21/312 Trining Loss: 2.2856290567488897 Step: 672/10000\n",
      "Epoch 1/1 Batch 22/312 Trining Loss: 2.234858361157504 Step: 704/10000\n",
      "Epoch 1/1 Batch 23/312 Trining Loss: 2.184592174447101 Step: 736/10000\n",
      "Epoch 1/1 Batch 24/312 Trining Loss: 2.1323677798112235 Step: 768/10000\n",
      "Epoch 1/1 Batch 25/312 Trining Loss: 2.08055367231369 Step: 800/10000\n",
      "Epoch 1/1 Batch 26/312 Trining Loss: 2.0305617657991557 Step: 832/10000\n",
      "Epoch 1/1 Batch 27/312 Trining Loss: 1.979356211644632 Step: 864/10000\n",
      "Epoch 1/1 Batch 28/312 Trining Loss: 1.9305750642504012 Step: 896/10000\n",
      "Epoch 1/1 Batch 29/312 Trining Loss: 1.88457099322615 Step: 928/10000\n",
      "Epoch 1/1 Batch 30/312 Trining Loss: 1.8399049798647562 Step: 960/10000\n",
      "Epoch 1/1 Batch 31/312 Trining Loss: 1.796714230891197 Step: 992/10000\n",
      "Epoch 1/1 Batch 32/312 Trining Loss: 1.7552388310432434 Step: 1024/10000\n",
      "Epoch 1/1 Batch 33/312 Trining Loss: 1.7166812537294445 Step: 1056/10000\n",
      "Epoch 1/1 Batch 34/312 Trining Loss: 1.6786499645780115 Step: 1088/10000\n",
      "Epoch 1/1 Batch 35/312 Trining Loss: 1.6420929534094675 Step: 1120/10000\n",
      "Epoch 1/1 Batch 36/312 Trining Loss: 1.6075932458043098 Step: 1152/10000\n",
      "Epoch 1/1 Batch 37/312 Trining Loss: 1.574181168466001 Step: 1184/10000\n",
      "Epoch 1/1 Batch 38/312 Trining Loss: 1.5424257799198753 Step: 1216/10000\n",
      "Epoch 1/1 Batch 39/312 Trining Loss: 1.5113939054501362 Step: 1248/10000\n",
      "Epoch 1/1 Batch 40/312 Trining Loss: 1.482433807849884 Step: 1280/10000\n",
      "Epoch 1/1 Batch 41/312 Trining Loss: 1.4552060830883864 Step: 1312/10000\n",
      "Epoch 1/1 Batch 42/312 Trining Loss: 1.428723543172791 Step: 1344/10000\n",
      "Epoch 1/1 Batch 43/312 Trining Loss: 1.4034833838773328 Step: 1376/10000\n",
      "Epoch 1/1 Batch 44/312 Trining Loss: 1.379044620828195 Step: 1408/10000\n",
      "Epoch 1/1 Batch 45/312 Trining Loss: 1.3550532791349623 Step: 1440/10000\n",
      "Epoch 1/1 Batch 46/312 Trining Loss: 1.332367590588072 Step: 1472/10000\n",
      "Epoch 1/1 Batch 47/312 Trining Loss: 1.310518502554995 Step: 1504/10000\n",
      "Epoch 1/1 Batch 48/312 Trining Loss: 1.2893709267179172 Step: 1536/10000\n",
      "Epoch 1/1 Batch 49/312 Trining Loss: 1.2689220662019691 Step: 1568/10000\n",
      "Epoch 1/1 Batch 50/312 Trining Loss: 1.2499052518606186 Step: 1600/10000\n",
      "Epoch 1/1 Batch 51/312 Trining Loss: 1.2310375518658583 Step: 1632/10000\n",
      "Epoch 1/1 Batch 52/312 Trining Loss: 1.2128783945853894 Step: 1664/10000\n",
      "Epoch 1/1 Batch 53/312 Trining Loss: 1.1953844534900953 Step: 1696/10000\n",
      "Epoch 1/1 Batch 54/312 Trining Loss: 1.1779661669775292 Step: 1728/10000\n",
      "Epoch 1/1 Batch 55/312 Trining Loss: 1.1618408246473833 Step: 1760/10000\n",
      "Epoch 1/1 Batch 56/312 Trining Loss: 1.1457408490989889 Step: 1792/10000\n",
      "Epoch 1/1 Batch 57/312 Trining Loss: 1.1301961504576499 Step: 1824/10000\n",
      "Epoch 1/1 Batch 58/312 Trining Loss: 1.1150719564536522 Step: 1856/10000\n",
      "Epoch 1/1 Batch 59/312 Trining Loss: 1.1003517796427518 Step: 1888/10000\n",
      "Epoch 1/1 Batch 60/312 Trining Loss: 1.0861947293082872 Step: 1920/10000\n",
      "Epoch 1/1 Batch 61/312 Trining Loss: 1.072387099754615 Step: 1952/10000\n",
      "Epoch 1/1 Batch 62/312 Trining Loss: 1.0590507138159968 Step: 1984/10000\n",
      "Epoch 1/1 Batch 63/312 Trining Loss: 1.0460131104503358 Step: 2016/10000\n",
      "Epoch 1/1 Batch 64/312 Trining Loss: 1.0334800742566586 Step: 2048/10000\n",
      "Epoch 1/1 Batch 65/312 Trining Loss: 1.0212037574786406 Step: 2080/10000\n",
      "Epoch 1/1 Batch 66/312 Trining Loss: 1.0092982792041518 Step: 2112/10000\n",
      "Epoch 1/1 Batch 67/312 Trining Loss: 0.9977720533289126 Step: 2144/10000\n",
      "Epoch 1/1 Batch 68/312 Trining Loss: 0.9868194888181546 Step: 2176/10000\n",
      "Epoch 1/1 Batch 69/312 Trining Loss: 0.9763113849836847 Step: 2208/10000\n",
      "Epoch 1/1 Batch 70/312 Trining Loss: 0.9658812427095005 Step: 2240/10000\n",
      "Epoch 1/1 Batch 71/312 Trining Loss: 0.9556502091213012 Step: 2272/10000\n",
      "Epoch 1/1 Batch 72/312 Trining Loss: 0.9458247683942318 Step: 2304/10000\n",
      "Epoch 1/1 Batch 73/312 Trining Loss: 0.9364172399860539 Step: 2336/10000\n",
      "Epoch 1/1 Batch 74/312 Trining Loss: 0.9266697504633182 Step: 2368/10000\n",
      "Epoch 1/1 Batch 75/312 Trining Loss: 0.9173241577545802 Step: 2400/10000\n",
      "Epoch 1/1 Batch 76/312 Trining Loss: 0.9081058378674483 Step: 2432/10000\n",
      "Epoch 1/1 Batch 77/312 Trining Loss: 0.899187714054987 Step: 2464/10000\n",
      "Epoch 1/1 Batch 78/312 Trining Loss: 0.8905584909594976 Step: 2496/10000\n",
      "Epoch 1/1 Batch 79/312 Trining Loss: 0.8822511545842207 Step: 2528/10000\n",
      "Epoch 1/1 Batch 80/312 Trining Loss: 0.8740998653694987 Step: 2560/10000\n",
      "Epoch 1/1 Batch 81/312 Trining Loss: 0.865658630191544 Step: 2592/10000\n",
      "Epoch 1/1 Batch 82/312 Trining Loss: 0.8577251123582444 Step: 2624/10000\n",
      "Epoch 1/1 Batch 83/312 Trining Loss: 0.8496628970984953 Step: 2656/10000\n",
      "Epoch 1/1 Batch 84/312 Trining Loss: 0.8420509507968312 Step: 2688/10000\n",
      "Epoch 1/1 Batch 85/312 Trining Loss: 0.8345931414295645 Step: 2720/10000\n",
      "Epoch 1/1 Batch 86/312 Trining Loss: 0.8273530084379884 Step: 2752/10000\n",
      "Epoch 1/1 Batch 87/312 Trining Loss: 0.8202451613100096 Step: 2784/10000\n",
      "Epoch 1/1 Batch 88/312 Trining Loss: 0.8134050987322222 Step: 2816/10000\n",
      "Epoch 1/1 Batch 89/312 Trining Loss: 0.8063865806279558 Step: 2848/10000\n",
      "Epoch 1/1 Batch 90/312 Trining Loss: 0.7998055668340789 Step: 2880/10000\n",
      "Epoch 1/1 Batch 91/312 Trining Loss: 0.7933325913253721 Step: 2912/10000\n",
      "Epoch 1/1 Batch 92/312 Trining Loss: 0.7867633165872615 Step: 2944/10000\n",
      "Epoch 1/1 Batch 93/312 Trining Loss: 0.7804334471302647 Step: 2976/10000\n",
      "Epoch 1/1 Batch 94/312 Trining Loss: 0.7742612499822962 Step: 3008/10000\n",
      "Epoch 1/1 Batch 95/312 Trining Loss: 0.7680707106464788 Step: 3040/10000\n",
      "Epoch 1/1 Batch 96/312 Trining Loss: 0.762092482453833 Step: 3072/10000\n",
      "Epoch 1/1 Batch 97/312 Trining Loss: 0.7561420257558528 Step: 3104/10000\n",
      "Epoch 1/1 Batch 98/312 Trining Loss: 0.7502328277546533 Step: 3136/10000\n",
      "Epoch 1/1 Batch 99/312 Trining Loss: 0.7446808580196265 Step: 3168/10000\n",
      "Epoch 1/1 Batch 100/312 Trining Loss: 0.7390395648777485 Step: 3200/10000\n",
      "Epoch 1/1 Batch 101/312 Trining Loss: 0.733594266494902 Step: 3232/10000\n",
      "Epoch 1/1 Batch 102/312 Trining Loss: 0.728131961588766 Step: 3264/10000\n",
      "Epoch 1/1 Batch 103/312 Trining Loss: 0.7229526290326442 Step: 3296/10000\n",
      "Epoch 1/1 Batch 104/312 Trining Loss: 0.7178386828074088 Step: 3328/10000\n",
      "Epoch 1/1 Batch 105/312 Trining Loss: 0.7126568335862387 Step: 3360/10000\n",
      "Epoch 1/1 Batch 106/312 Trining Loss: 0.7076313409884021 Step: 3392/10000\n",
      "Epoch 1/1 Batch 107/312 Trining Loss: 0.7026378238034026 Step: 3424/10000\n",
      "Epoch 1/1 Batch 108/312 Trining Loss: 0.6978520437799118 Step: 3456/10000\n",
      "Epoch 1/1 Batch 109/312 Trining Loss: 0.6930469329204034 Step: 3488/10000\n",
      "Epoch 1/1 Batch 110/312 Trining Loss: 0.6885464076291431 Step: 3520/10000\n",
      "Epoch 1/1 Batch 111/312 Trining Loss: 0.6839686362324534 Step: 3552/10000\n",
      "Epoch 1/1 Batch 112/312 Trining Loss: 0.679632638315005 Step: 3584/10000\n",
      "Epoch 1/1 Batch 113/312 Trining Loss: 0.6749793360982321 Step: 3616/10000\n",
      "Epoch 1/1 Batch 114/312 Trining Loss: 0.6705587380810788 Step: 3648/10000\n",
      "Epoch 1/1 Batch 115/312 Trining Loss: 0.6663084297076516 Step: 3680/10000\n",
      "Epoch 1/1 Batch 116/312 Trining Loss: 0.662162454092297 Step: 3712/10000\n",
      "Epoch 1/1 Batch 117/312 Trining Loss: 0.6578358590093434 Step: 3744/10000\n",
      "Epoch 1/1 Batch 118/312 Trining Loss: 0.6538599399944484 Step: 3776/10000\n",
      "Epoch 1/1 Batch 119/312 Trining Loss: 0.6499067057831949 Step: 3808/10000\n",
      "Epoch 1/1 Batch 120/312 Trining Loss: 0.6456897267450888 Step: 3840/10000\n",
      "Epoch 1/1 Batch 121/312 Trining Loss: 0.6419080048799515 Step: 3872/10000\n",
      "Epoch 1/1 Batch 122/312 Trining Loss: 0.6379723710114839 Step: 3904/10000\n",
      "Epoch 1/1 Batch 123/312 Trining Loss: 0.633971903382278 Step: 3936/10000\n",
      "Epoch 1/1 Batch 124/312 Trining Loss: 0.6301587940223755 Step: 3968/10000\n",
      "Epoch 1/1 Batch 125/312 Trining Loss: 0.6265775727033616 Step: 4000/10000\n",
      "Epoch 1/1 Batch 126/312 Trining Loss: 0.6228173352659695 Step: 4032/10000\n",
      "Epoch 1/1 Batch 127/312 Trining Loss: 0.6192653872600691 Step: 4064/10000\n",
      "Epoch 1/1 Batch 128/312 Trining Loss: 0.61565609741956 Step: 4096/10000\n",
      "Epoch 1/1 Batch 129/312 Trining Loss: 0.6121535198402035 Step: 4128/10000\n",
      "Epoch 1/1 Batch 130/312 Trining Loss: 0.6084901863565811 Step: 4160/10000\n",
      "Epoch 1/1 Batch 131/312 Trining Loss: 0.6050216162022743 Step: 4192/10000\n",
      "Epoch 1/1 Batch 132/312 Trining Loss: 0.601557249824206 Step: 4224/10000\n",
      "Epoch 1/1 Batch 133/312 Trining Loss: 0.5982107252330708 Step: 4256/10000\n",
      "Epoch 1/1 Batch 134/312 Trining Loss: 0.5948561202011892 Step: 4288/10000\n",
      "Epoch 1/1 Batch 135/312 Trining Loss: 0.5914633295050373 Step: 4320/10000\n",
      "Epoch 1/1 Batch 136/312 Trining Loss: 0.5883462242782116 Step: 4352/10000\n",
      "Epoch 1/1 Batch 137/312 Trining Loss: 0.5852714965160746 Step: 4384/10000\n",
      "Epoch 1/1 Batch 138/312 Trining Loss: 0.5821463434376578 Step: 4416/10000\n",
      "Epoch 1/1 Batch 139/312 Trining Loss: 0.5791999786663399 Step: 4448/10000\n",
      "Epoch 1/1 Batch 140/312 Trining Loss: 0.576209196554763 Step: 4480/10000\n",
      "Epoch 1/1 Batch 141/312 Trining Loss: 0.5731395127925467 Step: 4512/10000\n",
      "Epoch 1/1 Batch 142/312 Trining Loss: 0.5701416727732604 Step: 4544/10000\n",
      "Epoch 1/1 Batch 143/312 Trining Loss: 0.56718152698937 Step: 4576/10000\n",
      "Epoch 1/1 Batch 144/312 Trining Loss: 0.5642070804412166 Step: 4608/10000\n",
      "Epoch 1/1 Batch 145/312 Trining Loss: 0.5615267038345337 Step: 4640/10000\n",
      "Epoch 1/1 Batch 146/312 Trining Loss: 0.5588914260064086 Step: 4672/10000\n",
      "Epoch 1/1 Batch 147/312 Trining Loss: 0.5561750078079651 Step: 4704/10000\n",
      "Epoch 1/1 Batch 148/312 Trining Loss: 0.5535349456241002 Step: 4736/10000\n",
      "Epoch 1/1 Batch 149/312 Trining Loss: 0.5507633558055699 Step: 4768/10000\n",
      "Epoch 1/1 Batch 150/312 Trining Loss: 0.5480056526263555 Step: 4800/10000\n",
      "Epoch 1/1 Batch 151/312 Trining Loss: 0.5453826780153426 Step: 4832/10000\n",
      "Epoch 1/1 Batch 152/312 Trining Loss: 0.54288036082136 Step: 4864/10000\n",
      "Epoch 1/1 Batch 153/312 Trining Loss: 0.5403684570508844 Step: 4896/10000\n",
      "Epoch 1/1 Batch 154/312 Trining Loss: 0.5377102092682541 Step: 4928/10000\n",
      "Epoch 1/1 Batch 155/312 Trining Loss: 0.5351993106065258 Step: 4960/10000\n",
      "Epoch 1/1 Batch 156/312 Trining Loss: 0.5329067547542926 Step: 4992/10000\n",
      "Epoch 1/1 Batch 157/312 Trining Loss: 0.5304829545651272 Step: 5024/10000\n",
      "Epoch 1/1 Batch 158/312 Trining Loss: 0.5280719782355465 Step: 5056/10000\n",
      "Epoch 1/1 Batch 159/312 Trining Loss: 0.5256812311006043 Step: 5088/10000\n",
      "Epoch 1/1 Batch 160/312 Trining Loss: 0.52314183996059 Step: 5120/10000\n",
      "Epoch 1/1 Batch 161/312 Trining Loss: 0.5206833178674952 Step: 5152/10000\n",
      "Epoch 1/1 Batch 162/312 Trining Loss: 0.5182560558580322 Step: 5184/10000\n",
      "Epoch 1/1 Batch 163/312 Trining Loss: 0.5158880997929105 Step: 5216/10000\n",
      "Epoch 1/1 Batch 164/312 Trining Loss: 0.5135562608427391 Step: 5248/10000\n",
      "Epoch 1/1 Batch 165/312 Trining Loss: 0.5112163638075192 Step: 5280/10000\n",
      "Epoch 1/1 Batch 166/312 Trining Loss: 0.5090823898383652 Step: 5312/10000\n",
      "Epoch 1/1 Batch 167/312 Trining Loss: 0.5067929563497355 Step: 5344/10000\n",
      "Epoch 1/1 Batch 168/312 Trining Loss: 0.5045802014480744 Step: 5376/10000\n",
      "Epoch 1/1 Batch 169/312 Trining Loss: 0.5023678217149345 Step: 5408/10000\n",
      "Epoch 1/1 Batch 170/312 Trining Loss: 0.5002784473492818 Step: 5440/10000\n",
      "Epoch 1/1 Batch 171/312 Trining Loss: 0.4980929232044527 Step: 5472/10000\n",
      "Epoch 1/1 Batch 172/312 Trining Loss: 0.4960878865230222 Step: 5504/10000\n",
      "Epoch 1/1 Batch 173/312 Trining Loss: 0.49398190907143447 Step: 5536/10000\n",
      "Epoch 1/1 Batch 174/312 Trining Loss: 0.4918987396034016 Step: 5568/10000\n",
      "Epoch 1/1 Batch 175/312 Trining Loss: 0.48986582411187035 Step: 5600/10000\n",
      "Epoch 1/1 Batch 176/312 Trining Loss: 0.487742561169646 Step: 5632/10000\n",
      "Epoch 1/1 Batch 177/312 Trining Loss: 0.48571862852842795 Step: 5664/10000\n",
      "Epoch 1/1 Batch 178/312 Trining Loss: 0.483633348278785 Step: 5696/10000\n",
      "Epoch 1/1 Batch 179/312 Trining Loss: 0.481656785201094 Step: 5728/10000\n",
      "Epoch 1/1 Batch 180/312 Trining Loss: 0.479691821253962 Step: 5760/10000\n",
      "Epoch 1/1 Batch 181/312 Trining Loss: 0.4777550952539918 Step: 5792/10000\n",
      "Epoch 1/1 Batch 182/312 Trining Loss: 0.4756696505860968 Step: 5824/10000\n",
      "Epoch 1/1 Batch 183/312 Trining Loss: 0.4737072875265215 Step: 5856/10000\n",
      "Epoch 1/1 Batch 184/312 Trining Loss: 0.4716834647014089 Step: 5888/10000\n",
      "Epoch 1/1 Batch 185/312 Trining Loss: 0.4697293767655218 Step: 5920/10000\n",
      "Epoch 1/1 Batch 186/312 Trining Loss: 0.4679776920025708 Step: 5952/10000\n",
      "Epoch 1/1 Batch 187/312 Trining Loss: 0.46602647165882394 Step: 5984/10000\n",
      "Epoch 1/1 Batch 188/312 Trining Loss: 0.4641788325648993 Step: 6016/10000\n",
      "Epoch 1/1 Batch 189/312 Trining Loss: 0.4623974285349644 Step: 6048/10000\n",
      "Epoch 1/1 Batch 190/312 Trining Loss: 0.4605120056553891 Step: 6080/10000\n",
      "Epoch 1/1 Batch 191/312 Trining Loss: 0.45867126220495913 Step: 6112/10000\n",
      "Epoch 1/1 Batch 192/312 Trining Loss: 0.45689611001095426 Step: 6144/10000\n",
      "Epoch 1/1 Batch 193/312 Trining Loss: 0.4550651959650257 Step: 6176/10000\n",
      "Epoch 1/1 Batch 194/312 Trining Loss: 0.4532945318166743 Step: 6208/10000\n",
      "Epoch 1/1 Batch 195/312 Trining Loss: 0.45141659646462173 Step: 6240/10000\n",
      "Epoch 1/1 Batch 196/312 Trining Loss: 0.4497204210837277 Step: 6272/10000\n",
      "Epoch 1/1 Batch 197/312 Trining Loss: 0.4480000083443477 Step: 6304/10000\n",
      "Epoch 1/1 Batch 198/312 Trining Loss: 0.4462631891923721 Step: 6336/10000\n",
      "Epoch 1/1 Batch 199/312 Trining Loss: 0.44457186994390874 Step: 6368/10000\n",
      "Epoch 1/1 Batch 200/312 Trining Loss: 0.4428500712662935 Step: 6400/10000\n",
      "Epoch 1/1 Batch 201/312 Trining Loss: 0.44118283103354533 Step: 6432/10000\n",
      "Epoch 1/1 Batch 202/312 Trining Loss: 0.4394682336709287 Step: 6464/10000\n",
      "Epoch 1/1 Batch 203/312 Trining Loss: 0.4377895675856492 Step: 6496/10000\n",
      "Epoch 1/1 Batch 204/312 Trining Loss: 0.4361455846650928 Step: 6528/10000\n",
      "Epoch 1/1 Batch 205/312 Trining Loss: 0.4345142621456123 Step: 6560/10000\n",
      "Epoch 1/1 Batch 206/312 Trining Loss: 0.43286654906510147 Step: 6592/10000\n",
      "Epoch 1/1 Batch 207/312 Trining Loss: 0.43126739885496057 Step: 6624/10000\n",
      "Epoch 1/1 Batch 208/312 Trining Loss: 0.4296381064117528 Step: 6656/10000\n",
      "Epoch 1/1 Batch 209/312 Trining Loss: 0.42810990329849663 Step: 6688/10000\n",
      "Epoch 1/1 Batch 210/312 Trining Loss: 0.4265932438274225 Step: 6720/10000\n",
      "Epoch 1/1 Batch 211/312 Trining Loss: 0.42505693068436534 Step: 6752/10000\n",
      "Epoch 1/1 Batch 212/312 Trining Loss: 0.4235098079057797 Step: 6784/10000\n",
      "Epoch 1/1 Batch 213/312 Trining Loss: 0.4219353898240367 Step: 6816/10000\n",
      "Epoch 1/1 Batch 214/312 Trining Loss: 0.42042931973516384 Step: 6848/10000\n",
      "Epoch 1/1 Batch 215/312 Trining Loss: 0.41890673044809074 Step: 6880/10000\n",
      "Epoch 1/1 Batch 216/312 Trining Loss: 0.41752479246093166 Step: 6912/10000\n",
      "Epoch 1/1 Batch 217/312 Trining Loss: 0.4160442110923578 Step: 6944/10000\n",
      "Epoch 1/1 Batch 218/312 Trining Loss: 0.4146062318201459 Step: 6976/10000\n",
      "Epoch 1/1 Batch 219/312 Trining Loss: 0.413179620998363 Step: 7008/10000\n",
      "Epoch 1/1 Batch 220/312 Trining Loss: 0.4117380016906695 Step: 7040/10000\n",
      "Epoch 1/1 Batch 221/312 Trining Loss: 0.4102680849341246 Step: 7072/10000\n",
      "Epoch 1/1 Batch 222/312 Trining Loss: 0.40884648562148884 Step: 7104/10000\n",
      "Epoch 1/1 Batch 223/312 Trining Loss: 0.4074000989508736 Step: 7136/10000\n",
      "Epoch 1/1 Batch 224/312 Trining Loss: 0.4060486757329532 Step: 7168/10000\n",
      "Epoch 1/1 Batch 225/312 Trining Loss: 0.40459508779976106 Step: 7200/10000\n",
      "Epoch 1/1 Batch 226/312 Trining Loss: 0.4032078561727452 Step: 7232/10000\n",
      "Epoch 1/1 Batch 227/312 Trining Loss: 0.4018694285451053 Step: 7264/10000\n",
      "Epoch 1/1 Batch 228/312 Trining Loss: 0.4004978945380763 Step: 7296/10000\n",
      "Epoch 1/1 Batch 229/312 Trining Loss: 0.39907418116601795 Step: 7328/10000\n",
      "Epoch 1/1 Batch 230/312 Trining Loss: 0.39780034759770266 Step: 7360/10000\n",
      "Epoch 1/1 Batch 231/312 Trining Loss: 0.39646144446743514 Step: 7392/10000\n",
      "Epoch 1/1 Batch 232/312 Trining Loss: 0.3950899075716734 Step: 7424/10000\n",
      "Epoch 1/1 Batch 233/312 Trining Loss: 0.3938412167163878 Step: 7456/10000\n",
      "Epoch 1/1 Batch 234/312 Trining Loss: 0.39261905565603167 Step: 7488/10000\n",
      "Epoch 1/1 Batch 235/312 Trining Loss: 0.3913293725315561 Step: 7520/10000\n",
      "Epoch 1/1 Batch 236/312 Trining Loss: 0.3900124540205224 Step: 7552/10000\n",
      "Epoch 1/1 Batch 237/312 Trining Loss: 0.38866637876642407 Step: 7584/10000\n",
      "Epoch 1/1 Batch 238/312 Trining Loss: 0.38743526125405015 Step: 7616/10000\n",
      "Epoch 1/1 Batch 239/312 Trining Loss: 0.38608036776342153 Step: 7648/10000\n",
      "Epoch 1/1 Batch 240/312 Trining Loss: 0.3848147688123087 Step: 7680/10000\n",
      "Epoch 1/1 Batch 241/312 Trining Loss: 0.38352161265631435 Step: 7712/10000\n",
      "Epoch 1/1 Batch 242/312 Trining Loss: 0.38227449311328326 Step: 7744/10000\n",
      "Epoch 1/1 Batch 243/312 Trining Loss: 0.3810405197702808 Step: 7776/10000\n",
      "Epoch 1/1 Batch 244/312 Trining Loss: 0.3798272180386254 Step: 7808/10000\n",
      "Epoch 1/1 Batch 245/312 Trining Loss: 0.3786090296750166 Step: 7840/10000\n",
      "Epoch 1/1 Batch 246/312 Trining Loss: 0.3773743392127316 Step: 7872/10000\n",
      "Epoch 1/1 Batch 247/312 Trining Loss: 0.3761801209587317 Step: 7904/10000\n",
      "Epoch 1/1 Batch 248/312 Trining Loss: 0.37498645421357885 Step: 7936/10000\n",
      "Epoch 1/1 Batch 249/312 Trining Loss: 0.3738090091321363 Step: 7968/10000\n",
      "Epoch 1/1 Batch 250/312 Trining Loss: 0.37263926774263384 Step: 8000/10000\n",
      "Epoch 1/1 Batch 251/312 Trining Loss: 0.37147186562953244 Step: 8032/10000\n",
      "Epoch 1/1 Batch 252/312 Trining Loss: 0.3703169062556255 Step: 8064/10000\n",
      "Epoch 1/1 Batch 253/312 Trining Loss: 0.3691613217999813 Step: 8096/10000\n",
      "Epoch 1/1 Batch 254/312 Trining Loss: 0.3679944760217441 Step: 8128/10000\n",
      "Epoch 1/1 Batch 255/312 Trining Loss: 0.3668499250622357 Step: 8160/10000\n",
      "Epoch 1/1 Batch 256/312 Trining Loss: 0.3657650204550009 Step: 8192/10000\n",
      "Epoch 1/1 Batch 257/312 Trining Loss: 0.3645798639187785 Step: 8224/10000\n",
      "Epoch 1/1 Batch 258/312 Trining Loss: 0.36349633424145766 Step: 8256/10000\n",
      "Epoch 1/1 Batch 259/312 Trining Loss: 0.36237583388520483 Step: 8288/10000\n",
      "Epoch 1/1 Batch 260/312 Trining Loss: 0.36121881320499455 Step: 8320/10000\n",
      "Epoch 1/1 Batch 261/312 Trining Loss: 0.3601073281289974 Step: 8352/10000\n",
      "Epoch 1/1 Batch 262/312 Trining Loss: 0.35903704732315234 Step: 8384/10000\n",
      "Epoch 1/1 Batch 263/312 Trining Loss: 0.3579400445798504 Step: 8416/10000\n",
      "Epoch 1/1 Batch 264/312 Trining Loss: 0.3568376262926243 Step: 8448/10000\n",
      "Epoch 1/1 Batch 265/312 Trining Loss: 0.3557246783572548 Step: 8480/10000\n",
      "Epoch 1/1 Batch 266/312 Trining Loss: 0.354674811608025 Step: 8512/10000\n",
      "Epoch 1/1 Batch 267/312 Trining Loss: 0.3536441222195991 Step: 8544/10000\n",
      "Epoch 1/1 Batch 268/312 Trining Loss: 0.35262564379277067 Step: 8576/10000\n",
      "Epoch 1/1 Batch 269/312 Trining Loss: 0.3515998263424436 Step: 8608/10000\n",
      "Epoch 1/1 Batch 270/312 Trining Loss: 0.3505663241363234 Step: 8640/10000\n",
      "Epoch 1/1 Batch 271/312 Trining Loss: 0.34947579538250323 Step: 8672/10000\n",
      "Epoch 1/1 Batch 272/312 Trining Loss: 0.34848086508538795 Step: 8704/10000\n",
      "Epoch 1/1 Batch 273/312 Trining Loss: 0.34741806492700683 Step: 8736/10000\n",
      "Epoch 1/1 Batch 274/312 Trining Loss: 0.3463659824762684 Step: 8768/10000\n",
      "Epoch 1/1 Batch 275/312 Trining Loss: 0.34533543341539125 Step: 8800/10000\n",
      "Epoch 1/1 Batch 276/312 Trining Loss: 0.344311548302463 Step: 8832/10000\n",
      "Epoch 1/1 Batch 277/312 Trining Loss: 0.3433759055816525 Step: 8864/10000\n",
      "Epoch 1/1 Batch 278/312 Trining Loss: 0.34234080820096485 Step: 8896/10000\n",
      "Epoch 1/1 Batch 279/312 Trining Loss: 0.3413226983266278 Step: 8928/10000\n",
      "Epoch 1/1 Batch 280/312 Trining Loss: 0.3403598017325359 Step: 8960/10000\n",
      "Epoch 1/1 Batch 281/312 Trining Loss: 0.3393349517733389 Step: 8992/10000\n",
      "Epoch 1/1 Batch 282/312 Trining Loss: 0.3383534929140451 Step: 9024/10000\n",
      "Epoch 1/1 Batch 283/312 Trining Loss: 0.3374000410335435 Step: 9056/10000\n",
      "Epoch 1/1 Batch 284/312 Trining Loss: 0.3364543164096458 Step: 9088/10000\n",
      "Epoch 1/1 Batch 285/312 Trining Loss: 0.3355213664994951 Step: 9120/10000\n",
      "Epoch 1/1 Batch 286/312 Trining Loss: 0.33455762523714894 Step: 9152/10000\n",
      "Epoch 1/1 Batch 287/312 Trining Loss: 0.33360503507511957 Step: 9184/10000\n",
      "Epoch 1/1 Batch 288/312 Trining Loss: 0.33262017824583584 Step: 9216/10000\n",
      "Epoch 1/1 Batch 289/312 Trining Loss: 0.33166966066537845 Step: 9248/10000\n",
      "Epoch 1/1 Batch 290/312 Trining Loss: 0.33072624522550353 Step: 9280/10000\n",
      "Epoch 1/1 Batch 291/312 Trining Loss: 0.3297986664850892 Step: 9312/10000\n",
      "Epoch 1/1 Batch 292/312 Trining Loss: 0.3288692094844906 Step: 9344/10000\n",
      "Epoch 1/1 Batch 293/312 Trining Loss: 0.3279324154474426 Step: 9376/10000\n",
      "Epoch 1/1 Batch 294/312 Trining Loss: 0.32699842662328765 Step: 9408/10000\n",
      "Epoch 1/1 Batch 295/312 Trining Loss: 0.32611904639308736 Step: 9440/10000\n",
      "Epoch 1/1 Batch 296/312 Trining Loss: 0.32518789826615435 Step: 9472/10000\n",
      "Epoch 1/1 Batch 297/312 Trining Loss: 0.32427957337914093 Step: 9504/10000\n",
      "Epoch 1/1 Batch 298/312 Trining Loss: 0.3234476593366805 Step: 9536/10000\n",
      "Epoch 1/1 Batch 299/312 Trining Loss: 0.32253634957067145 Step: 9568/10000\n",
      "Epoch 1/1 Batch 300/312 Trining Loss: 0.32160668587932983 Step: 9600/10000\n",
      "Epoch 1/1 Batch 301/312 Trining Loss: 0.32076339843215734 Step: 9632/10000\n",
      "Epoch 1/1 Batch 302/312 Trining Loss: 0.3199387686265423 Step: 9664/10000\n",
      "Epoch 1/1 Batch 303/312 Trining Loss: 0.31906583235159564 Step: 9696/10000\n",
      "Epoch 1/1 Batch 304/312 Trining Loss: 0.31821872214296537 Step: 9728/10000\n",
      "Epoch 1/1 Batch 305/312 Trining Loss: 0.3173732740224385 Step: 9760/10000\n",
      "Epoch 1/1 Batch 306/312 Trining Loss: 0.31649071887573776 Step: 9792/10000\n",
      "Epoch 1/1 Batch 307/312 Trining Loss: 0.31563297933398315 Step: 9824/10000\n",
      "Epoch 1/1 Batch 308/312 Trining Loss: 0.31477589712949927 Step: 9856/10000\n",
      "Epoch 1/1 Batch 309/312 Trining Loss: 0.31391464410090525 Step: 9888/10000\n",
      "Epoch 1/1 Batch 310/312 Trining Loss: 0.31311768360676306 Step: 9920/10000\n",
      "Epoch 1/1 Batch 311/312 Trining Loss: 0.3122958581523305 Step: 9952/10000\n",
      "Epoch 1/1 Batch 312/312 Trining Loss: 0.31147147747329795 Step: 9984/10000\n",
      "Epoch 1/1 Batch 313/312 Trining Loss: 0.310622012117705 Step: 10016/10000\n",
      "Epoch 1/1 Batch 313/312 Trining Loss: 0.30963277004089707\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "_train(training_config)\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "                   max_len=148, device=\"cpu\"):\n",
    "\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        strt_t = model.bart.config.decoder_start_token_id\n",
    "\n",
    "        tgt = torch.full((src_sequences.size(0),1), strt_t, device=device)    \n",
    "\n",
    "        # Generate tokens one by one\n",
    "        for i in range(1,max_len):\n",
    "            tgt_msks = torch.full(tgt.shape, 0, device=device)\n",
    "\n",
    "            m = {\n",
    "                \"input_ids\": src_sequences,\n",
    "                \"attention_mask\" : src_msks,\n",
    "                \"decoder_input_ids\" : tgt,\n",
    "                \"decoder_attention_mask\" : tgt_msks,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "            out = model(m)\n",
    "\n",
    "            # Get next token prediction\n",
    "            next_token = out.argmax(dim=-1)\n",
    "            next_token = next_token[:, -1]\n",
    "            next_token = next_token.unsqueeze(1)\n",
    "            next_token = outvoc2voc(next_token)\n",
    "            # store new prediction for active sequences\n",
    "            tgt = torch.cat((tgt, next_token), dim=1)\n",
    "\n",
    "    return tgt\n",
    "\n",
    "# def predict_batch(model, src_sequences, src_msks, tokenizer:PreTrainedTokenizer,\n",
    "#                    max_len=148, device=\"cpu\"):\n",
    "#     \"\"\"\n",
    "#     Generates predictions for a batch of source sequences using the given model.\n",
    "#     Args:\n",
    "#         model (torch.nn.Module): The model used for generating predictions.\n",
    "#         src_sequence (torch.Tensor): The source sequences to be translated.\n",
    "#         vocab (Vocabulary): The vocabulary object containing all tokens.\n",
    "#         max_len (int, optional): The maximum length of the generated sequences. Defaults to 128.\n",
    "#         device (str, optional): The device to run the model on (\"cpu\" or \"cuda\"). Defaults to \"cpu\".\n",
    "#     Returns:\n",
    "#         torch.Tensor: The generated target sequences.\n",
    "#     \"\"\"\n",
    "\n",
    "#     eos = tokenizer.convert_tokens_to_ids(['<s\\>'])[0]\n",
    "#     bos = tokenizer.convert_tokens_to_ids(['<s>'])[0]\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Initialize target sequence with SOS token\n",
    "#         #tgt = [vocab.sos_idx] + [vocab.pad_idx] * (max_len)\n",
    "#         tgt = [bos] + ([tokenizer.pad_token_type_id] * (max_len))\n",
    "#         tgt = torch.tensor([tgt], device=device)\n",
    "#         tgt = tgt.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         tgt_msks = [1] + ([tokenizer.pad_token_type_id])*max_len\n",
    "#         tgt_msks = torch.tensor([tgt_msks], device=device)\n",
    "#         tgt_msks = tgt_msks.repeat(src_sequences.size(0), 1)\n",
    "\n",
    "#         # holds indx of sequences containing EOS token\n",
    "#         finished = torch.tensor([False]*src_sequences.size(0), device=device)\n",
    "        \n",
    "#         # Generate tokens one by one\n",
    "#         for i in range(1,max_len):\n",
    "#             tgt_msks[:,i] = 1\n",
    "#             # indx of batch dim where EOS token has not been generated\n",
    "#             active_indxs = torch.where(finished == False)[0]\n",
    "#             # feed only unfinished sequences to decoder\n",
    "#             # remove padding\n",
    "#             m = {\n",
    "#                 \"input_ids\": src_sequences[active_indxs],\n",
    "#                 \"attention_mask\" : src_msks[active_indxs],\n",
    "#                 \"decoder_input_ids\" : tgt[active_indxs],\n",
    "#                 \"decoder_attention_mask\" : tgt_msks[active_indxs],\n",
    "#                 \"head_mask\" : None,\n",
    "#                 \"decoder_head_mask\" : None,\n",
    "#                 \"cross_attn_head_mask\" : None,\n",
    "#                 \"encoder_outputs\" : None,\n",
    "#                 \"past_key_values\" : None,\n",
    "#                 \"inputs_embeds\" : None,\n",
    "#                 \"decoder_inputs_embeds\" : None,\n",
    "#                 \"use_cache\" : None,\n",
    "#                 \"output_attentions\" : None,\n",
    "#                 \"output_hidden_states\" : None,\n",
    "#                 \"return_dict\" : None,\n",
    "#             }\n",
    "#             out = model(m)\n",
    "\n",
    "#             # Get next token prediction\n",
    "#             next_token = out.argmax(dim=-1)\n",
    "#             next_token = next_token[:, -1]\n",
    "#             # store new prediction for active sequences\n",
    "#             tgt[active_indxs,i] = next_token\n",
    "            \n",
    "#             # update finished sequences if any EOS token is generated\n",
    "#             new_finished = torch.where(next_token == eos, True, False)\n",
    "#             finished[active_indxs] = torch.logical_or(finished[active_indxs], new_finished)\n",
    "\n",
    "#             # early stopping if all sequences produced EOS token\n",
    "#             if finished.all():\n",
    "#                 break\n",
    "#     return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 120])\n",
      "I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK I_TURN_LEFT I_TURN_LEFT I_TURN_LEFT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s>II_______________________ IT IT_ IT IT_ IT IT_ IT_ IT IT_ IT_ IT_ IT_ I_______________________</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs,inputs_msks, dec_in, dec_in_msks, targets, targets_msks = test_dataset[2]\n",
    "inputs = inputs.repeat(2,1).to(device)\n",
    "inputs_msks = inputs_msks.repeat(2,1).to(device)\n",
    "print(inputs.shape)\n",
    "pred = predict_batch(model, inputs, inputs_msks, tokenizer, device=device)\n",
    "print(tokenizer.decode(targets, skip_special_tokens=True))\n",
    "tokenizer.decode(pred[0].tolist(), skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2.],\n",
       "        [2., 0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "         2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'II I_ I_ I_ I<s>_</s></s></s></s>TTURNURNURNRRRRURN<s><s>URNURNURNURNURN<s><s>URNURNURNURNURNURN__URNUMPUMPOOKURN_______URNOOK I_________OOKT_________TT__<s><s></s>___<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {\n",
    "                \"input_ids\": inputs,\n",
    "                \"attention_mask\" : inputs_msks,\n",
    "                \"decoder_input_ids\" : None,\n",
    "                \"decoder_attention_mask\" : None,\n",
    "                \"head_mask\" : None,\n",
    "                \"decoder_head_mask\" : None,\n",
    "                \"cross_attn_head_mask\" : None,\n",
    "                \"encoder_outputs\" : None,\n",
    "                \"past_key_values\" : None,\n",
    "                \"inputs_embeds\" : None,\n",
    "                \"decoder_inputs_embeds\" : None,\n",
    "                \"use_cache\" : None,\n",
    "                \"output_attentions\" : None,\n",
    "                \"output_hidden_states\" : None,\n",
    "                \"return_dict\" : None,\n",
    "            }\n",
    "out = model(m)\n",
    "#PRED\n",
    "tokenizer.decode(out.argmax(dim=-1)[0].tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10457])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.randint(0, len(train_dataset), (1,))\n",
    "\n",
    "rand.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        [    0, 13724,  5483,   235, 10161,  2463,    71,   356,   198,   314,\n",
       "             2,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   100,  1215,   565, 28267,  1215,  3850, 11615,    38,  1215,\n",
       "          574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850, 11615,\n",
       "           38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,  3850,\n",
       "        11615,    38,  1215,   574, 23775,    38,  1215,   565, 28267,  1215,\n",
       "          500,  8167,    38,  1215,   565, 28267,  1215,   500,  8167,    38,\n",
       "         1215,   574, 23775,    38,  1215,   565, 28267,  1215,   500,  8167,\n",
       "           38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,   574,\n",
       "        23775,    38,  1215,   565, 28267,  1215,   500,  8167,    38,  1215,\n",
       "          565, 28267,  1215,   500,  8167,    38,  1215,   574, 23775,     2,\n",
       "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 0\n",
      "</s> 2\n",
      "<unk> 3\n",
      "<pad> 1\n",
      "<mask> 50264\n"
     ]
    }
   ],
   "source": [
    "special = tokenizer.all_special_tokens\n",
    "for s in special:\n",
    "    print(s, tokenizer.convert_tokens_to_ids(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),\"tbag/bart_1_1_5k.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1,3,2}\n",
    "sorted(a)\n",
    "a.add(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  4,  5,  6,  8, 10, 13, 15, 16, 19, 21, 24, 27, 29, 30, 32, 33,\n",
       "        34, 36, 37, 42, 47, 52, 53, 54, 55, 57, 61, 63, 66, 68, 76, 77, 78, 81,\n",
       "        84, 89, 90, 93])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 100, (5,10))\n",
    "\n",
    "a = torch.sort(torch.unique(a.flatten())).values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.full((5,100), 1)\n",
    "\n",
    "#a = torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "a.shape\n",
    "\n",
    "# b.shape\n",
    "#b[:,a].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 8,\n",
       " 4: 71,\n",
       " 5: 198,\n",
       " 6: 235,\n",
       " 7: 314,\n",
       " 8: 356,\n",
       " 9: 422,\n",
       " 10: 1004,\n",
       " 11: 1656,\n",
       " 12: 2330,\n",
       " 13: 2463,\n",
       " 14: 2962,\n",
       " 15: 3704,\n",
       " 16: 5483,\n",
       " 17: 10097,\n",
       " 18: 10161,\n",
       " 19: 13724,\n",
       " 20: 15922,\n",
       " 21: 43750}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab = torch.concatenate([train_dataset.output_vocab,test_dataset.output_vocab])\n",
    "output_vocab = torch.sort(torch.unique(output_vocab))\n",
    "output_vocab\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
